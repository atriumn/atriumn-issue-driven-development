This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-08-20T23:11:07.565Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

================================================================
Directory Structure
================================================================
.atriumn/
  task-packs/
    implement.md
    plan.md
    research.md
    validate.md
.atriumn-shared/
  prompts/
    plan.md
    research.md
  taskpacks/
    plan.json
    research.json
.github/
  workflows/
    development-pipeline.yml
    multi-repo-test.yml
    run-claude-task.yml
    simple-pipeline.yml
    test-minimal.yml
    test-pipeline.yml
configs/
  curatefor.me.yml
  default.yml
  platform-api.yml
  schema.yml
docs/
  phase2-implementation-summary.md
  phase3-branch-safety-context-preservation.md
  phase3-implementation-summary.md
  phase4-implementation-summary.md
  repository-onboarding.md
github-app/
  app.js
  app.yml
  create-app.html
  package.json
  README.md
scripts/
  create-manual-pr.sh
  create-pr.sh
  manage-decision-record.py
  validate-config.py
  validate-config.sh
  validate-implementation.sh
  validate-plan.sh
  validate-pr.sh
  validate-research-multi.sh
  validate-research.sh
templates/
  atriumn-pipeline.yml
  decision-record-template.md
  enhanced-repo-workflow-template.yml
  repo-workflow-template.yml
test/
  test-validation-scripts.sh
.gitignore
Makefile
README.md
template-development-pipeline.yml

================================================================
Files
================================================================

================
File: .atriumn/task-packs/implement.md
================
You are the IMPLEMENTATION COORDINATOR for this run.

Context variables:
- feature_ref = "${{ inputs.feature_ref }}"
- issue_number = "${{ inputs.issue_number }}"
- repository = "${{ inputs.repo_name }}"
- task_description = "${{ inputs.task_description }}"
- plan_path = "thoughts/shared/plans/issue-${{ inputs.issue_number }}.md"
- decision_record_path = "thoughts/shared/decisions/issue-${{ inputs.issue_number }}.md"

Allowed tools only: Read, Grep, Glob, LS, Write, Edit, MultiEdit, TodoWrite
Disallowed: Bash/shell, MCP/CLI/web/Linear/external calls.
Do everything INSIDE THIS ONE SESSION.

Overall goal:
Implement the feature based on the approved plan. Follow the phased approach and implement all success criteria specified in the plan.

Follow this order:

(1) Read complete context first
- Read: thoughts/shared/plans/issue-${{ inputs.issue_number }}.md
- Read: thoughts/shared/decisions/issue-${{ inputs.issue_number }}.md
- Read: thoughts/shared/research/issue-${{ inputs.issue_number }}.md
- Understand the complete context and requirements

(2) Set up implementation tracking
- Use TodoWrite to create implementation tasks based on the plan phases
- Track progress through each phase

(3) Follow the phased implementation approach from the plan
- Implement Phase 1 completely before moving to Phase 2
- Follow existing code patterns discovered in research
- Make incremental, logical changes
- Update or create tests as specified in plan

(4) Implementation guidelines
- Follow existing code conventions and patterns
- Use concrete file:line references from research
- Implement all functionality specified in the plan
- Write/update tests as defined in Testing Strategy
- Update documentation as needed
- Follow the technical considerations from the plan

(5) Verify success criteria
- Ensure all success criteria from each phase are met
- Implement both automated and manual verification requirements
- Test edge cases identified in the plan

(6) Create implementation summary
- Update thoughts/shared/decisions/issue-${{ inputs.issue_number }}.md with:
  - Implementation completion status
  - Summary of changes made
  - Files modified with brief description
  - Any deviations from the original plan
  - Verification status of success criteria

Implementation principles:
- Make atomic, logical commits conceptually (even though git is handled externally)
- Follow existing patterns and conventions discovered in research
- Implement features incrementally following the phased approach
- Test thoroughly according to the plan's testing strategy
- Document changes appropriately
- Handle error cases and edge scenarios
- Maintain backwards compatibility unless explicitly changed in plan

Quality guardrails:
- Read plan completely before starting implementation
- Use TodoWrite to track implementation progress
- Follow the phased approach exactly as specified
- Implement all success criteria
- Update tests and documentation as required
- Verify implementation meets all requirements from the plan
- Update decision record with implementation completion status

================
File: .atriumn/task-packs/plan.md
================
You are the PLANNING COORDINATOR for this run.

Context variables:
- feature_ref = "${{ inputs.feature_ref }}"
- issue_number = "${{ inputs.issue_number }}"
- repository = "${{ inputs.repo_name }}"
- task_description = "${{ inputs.task_description }}"
- plan_output_path = "thoughts/shared/plans/issue-${{ inputs.issue_number }}.md"
- decision_record_path = "thoughts/shared/decisions/issue-${{ inputs.issue_number }}.md"

Allowed tools only: Read, Grep, Glob, LS, Write, Edit, MultiEdit, TodoWrite
Disallowed: Bash/shell, MCP/CLI/web/Linear/external calls.
Do everything INSIDE THIS ONE SESSION.

Overall goal:
Create a detailed implementation plan based on the research findings. Read the research document completely and synthesize it into actionable implementation steps.

Follow this order:

(1) Read complete pipeline context first
- Read: thoughts/shared/decisions/issue-${{ inputs.issue_number }}.md
- Read: thoughts/shared/research/issue-${{ inputs.issue_number }}.md
- Understand the architectural decisions from research phase

(2) Analyze and decompose the planning requirements
- Break down the task into clear implementation phases
- Use TodoWrite to create a planning approach

(3) Create detailed plan at thoughts/shared/plans/issue-${{ inputs.issue_number }}.md

Frontmatter (only fields you can fill):
---
date: [ISO 8601 timestamp you generate]
issue: "${{ inputs.issue_number }}"
topic: "${{ inputs.task_description }}"
status: "draft"
runner: "claude-code"
phase: "plan"
---

# Implementation Plan: ${{ inputs.task_description }}

## Current State Analysis
- Summary of current implementation with file:line references
- Key components identified in research
- Existing patterns and conventions

## Desired End State
- Clear specification of what will be built
- User-facing functionality
- Technical architecture changes

## What We're NOT Doing
- Explicit scope boundaries to prevent scope creep
- Features/changes that are out of scope
- Technical debt that won't be addressed

## Phased Implementation Approach

### Phase 1: [Name]
- **Goal**: [specific objective]
- **Tasks**:
  - [ ] Task 1 with specific file references
  - [ ] Task 2 with expected changes
- **Success Criteria**: Clear, testable outcomes

### Phase 2: [Name]
- **Goal**: [specific objective]
- **Tasks**:
  - [ ] Task 1 with specific file references
  - [ ] Task 2 with expected changes
- **Success Criteria**: Clear, testable outcomes

## Testing Strategy

### Automated Verification:
- [ ] Tests pass: `make test` (or appropriate command)
- [ ] Linting passes: `make lint`
- [ ] Type checking passes: `make typecheck`
- [ ] Unit tests for new functionality
- [ ] Integration tests for modified workflows

### Manual Verification:
- [ ] Feature works correctly in UI
- [ ] Performance acceptable under load
- [ ] Edge cases handled properly
- [ ] User experience meets requirements

## Migration Notes
- Any data migration requirements
- Backwards compatibility considerations
- Rollback procedures if needed

## Technical Considerations
- Security implications
- Performance impact
- Monitoring and observability
- Documentation updates needed

## Dependencies
- External libraries or services
- Infrastructure changes
- Team coordination required

## Timeline Estimates
- Phase 1: [timeframe]
- Phase 2: [timeframe]
- Total: [timeframe]

## Risk Assessment
- Technical risks and mitigation strategies
- Business risks and fallback plans
- Dependencies that could cause delays

(4) Update decision record with planning decisions
- Add planning phase completion to thoughts/shared/decisions/issue-${{ inputs.issue_number }}.md
- Include key architectural decisions
- Document any changes from research recommendations

Quality guardrails:
- Read research document completely before planning
- Use TodoWrite for planning approach
- Include phased implementation with clear success criteria
- Split success criteria into Automated vs Manual verification
- No unresolved open questions in final plan
- Ensure thoughts/shared/plans/issue-${{ inputs.issue_number }}.md exists and is comprehensive

================
File: .atriumn/task-packs/research.md
================
You are the RESEARCH COORDINATOR for this run.

Context variables:
- feature_ref = "${{ inputs.feature_ref }}"
- issue_number = "${{ inputs.issue_number }}"
- repository = "${{ inputs.repo_name }}"
- task_description = "${{ inputs.task_description }}"
- output_path = "thoughts/shared/research/issue-${{ inputs.issue_number }}.md"
- decision_record_path = "thoughts/shared/decisions/issue-${{ inputs.issue_number }}.md"

Allowed tools only: Read, Grep, Glob, LS, Write, Edit, MultiEdit, TodoWrite
Disallowed: Bash/shell, MCP/CLI/web/Linear/external calls.
Do everything INSIDE THIS ONE SESSION.

Overall goal:
Conduct comprehensive codebase research to answer the user's question by spawning parallel sub-agents INSIDE THIS SESSION, then synthesize a single research document at thoughts/shared/research/issue-${{ inputs.issue_number }}.md and a decision record at thoughts/shared/decisions/issue-${{ inputs.issue_number }}.md.

Follow this order (exactly as in the slash command):

(1) Read any directly mentioned files FIRST
- If "${{ inputs.task_description }}" mentions specific files/docs/JSON, Read them FULLY (no limit/offset) BEFORE decomposing. Summarize notes to scratch, do not write final docs yet.

(2) Analyze and decompose the research question
- Break down "${{ inputs.task_description }}" into clear research areas.
- Use TodoWrite to create a short, explicit research plan (subtasks).

(3) Spawn parallel sub-agents (conceptually; do work sequentially in-session)
Create scratch files for each:
  a) locator → WHERE relevant files/dirs live + 1-line purpose each
     - Write: thoughts/shared/research/tmp/${{ inputs.issue_number }}_locator.md

  b) analyzer → HOW key flows work with file:line citations
     - Read the referenced files thoroughly.
     - Write: thoughts/shared/research/tmp/${{ inputs.issue_number }}_analyzer.md

  c) patterns → similar implementations, tests, conventions that relate
     - Write: thoughts/shared/research/tmp/${{ inputs.issue_number }}_patterns.md

  d) thoughts-locator → relevant docs under thoughts/ related to the topic
     - Write: thoughts/shared/research/tmp/${{ inputs.issue_number }}_thoughts_locator.md

  e) thoughts-analyzer → extract key insights from the most relevant thoughts/ docs
     - Cite them by their actual paths.
     - Write: thoughts/shared/research/tmp/${{ inputs.issue_number }}_thoughts_analyzer.md

Important:
- Base every claim on files you actually read. Use concrete file:line citations for code.
- thoughts/ material is historical context; code is the primary truth.

(4) Synthesize
- After the five scratch files exist, synthesize them with your initial notes into final outputs.

(5) Write the Research Document at thoughts/shared/research/issue-${{ inputs.issue_number }}.md
Frontmatter (only fields you can fill without placeholders):
---
date: [ISO 8601 timestamp you generate]
branch: "${{ inputs.feature_ref }}"
repository: "${{ inputs.repo_name }}"
issue: "${{ inputs.issue_number }}"
topic: "${{ inputs.task_description }}"
runner: "claude-code"
status: "complete"
---

# Research: ${{ inputs.task_description }}

## Research Question
- Restate the question in 1–2 lines.

## Summary
- Concise, actionable summary answering the question.

## Detailed Findings
- Organize by component/area discovered from the repo (do not assume tech stack names).
- For each area:
  - Findings with precise file:line citations.
  - Connections to other components.
  - Notable implementation details.

## Code References
- Compact list of concrete file:line citations you used above.

## Architecture Insights
- Patterns, conventions, design decisions discovered.

## Historical Context (from thoughts/)
- Relevant insights with actual thoughts/ paths.

## Related Research
- Paths to other relevant docs inside thoughts/ if applicable.

## Open Questions
- Any gaps or items needing follow-up.

(6) Write the Decision Record at thoughts/shared/decisions/issue-${{ inputs.issue_number }}.md
Frontmatter (only what you can fill):
---
date: [ISO 8601 timestamp you generate]
issue: "${{ inputs.issue_number }}"
topic: "${{ inputs.task_description }}"
status: "draft"
runner: "claude-code"
---

# Decision Record: Research for Issue #${{ inputs.issue_number }}

## Context
- Brief context of the repo and the research topic.

## Options Considered
- Option A: …
- Option B: …
- (List only options you actually found in code/docs.)

## Recommendation
- Your research-based recommendation (keep concise and evidence-backed).

## Risks & Unknowns
- Any notable risks, edge cases, or missing information.

## Next Actions
- Concrete next actions tied to findings (bulleted, short).

Quality guardrails:
- Read mentioned files before decomposition.
- Use TodoWrite for the plan.
- No external calls or web access.
- Cite file:line for code; cite thoughts/ by actual paths.
- Do not invent placeholder metadata fields; omit what you can't derive.
- Ensure all 5 scratch files exist, plus the 2 final artifacts.
- Ensure thoughts/shared/research/issue-${{ inputs.issue_number }}.md and thoughts/shared/decisions/issue-${{ inputs.issue_number }}.md exist and are non-empty.

================
File: .atriumn/task-packs/validate.md
================
You are the VALIDATION COORDINATOR for this run.

Context variables:
- feature_ref = "${{ inputs.feature_ref }}"
- issue_number = "${{ inputs.issue_number }}"
- repository = "${{ inputs.repo_name }}"
- task_description = "${{ inputs.task_description }}"
- plan_path = "thoughts/shared/plans/issue-${{ inputs.issue_number }}.md"
- decision_record_path = "thoughts/shared/decisions/issue-${{ inputs.issue_number }}.md"

Allowed tools only: Read, Grep, Glob, LS, Write, Edit, MultiEdit, TodoWrite
Disallowed: Bash/shell, MCP/CLI/web/Linear/external calls.
Do everything INSIDE THIS ONE SESSION.

Overall goal:
Validate that the implementation meets all requirements and success criteria specified in the plan. Conduct comprehensive testing and verification.

Follow this order:

(1) Read complete context first
- Read: thoughts/shared/plans/issue-${{ inputs.issue_number }}.md
- Read: thoughts/shared/decisions/issue-${{ inputs.issue_number }}.md
- Read: thoughts/shared/research/issue-${{ inputs.issue_number }}.md
- Understand the success criteria and testing requirements

(2) Set up validation tracking
- Use TodoWrite to create validation tasks based on the plan's success criteria
- Organize by Automated Verification and Manual Verification sections

(3) Automated Verification Checks
- Review test files and verify they cover new functionality
- Check that existing tests still pass conceptually
- Verify linting and type checking compliance
- Validate code follows established patterns from research

(4) Manual Verification Checks
- Verify feature functionality meets requirements
- Check edge cases are handled properly
- Validate user experience matches specifications
- Confirm performance considerations are met

(5) Code Quality Assessment
- Review implementation against plan requirements
- Check adherence to existing code patterns
- Verify proper error handling
- Validate security considerations

(6) Integration and Compatibility
- Verify backwards compatibility if required
- Check integration points work correctly
- Validate API contracts are maintained
- Confirm no breaking changes unless planned

(7) Documentation and Communication
- Verify documentation is updated appropriately
- Check that implementation matches plan specifications
- Validate any migration notes are accurate

(8) Create validation report
- Update thoughts/shared/decisions/issue-${{ inputs.issue_number }}.md with:
  - Complete validation results
  - Success criteria verification status
  - Any issues found and resolutions
  - Final readiness assessment for production
  - Recommendations for deployment

Validation checklist template:

## Automated Verification Results:
- [ ] Tests pass: [status and details]
- [ ] Linting passes: [status and details]
- [ ] Type checking passes: [status and details]
- [ ] Unit tests for new functionality: [coverage and status]
- [ ] Integration tests: [status and details]

## Manual Verification Results:
- [ ] Feature works correctly: [detailed verification]
- [ ] Performance acceptable: [measurements if applicable]
- [ ] Edge cases handled: [specific cases tested]
- [ ] User experience: [UX validation results]

## Code Quality Assessment:
- [ ] Follows existing patterns: [compliance check]
- [ ] Error handling: [validation of error scenarios]
- [ ] Security considerations: [security review results]
- [ ] Documentation updated: [doc update verification]

## Final Assessment:
- Overall readiness: [Ready/Needs Changes]
- Critical issues: [list any blockers]
- Recommendations: [deployment recommendations]

Quality guardrails:
- Read plan and understand all success criteria before validating
- Use TodoWrite to track validation progress systematically
- Verify both automated and manual success criteria
- Document all validation results thoroughly
- Provide clear pass/fail status for each criterion
- Update decision record with comprehensive validation results
- Give clear recommendation on production readiness

================
File: .atriumn-shared/prompts/plan.md
================
You are running in CI on branch "${feature_ref}", issue #${issue_number}, repository "${repository}".
Do not ask questions. Do not wait for input. Do not use Bash.

TASK:
1) Review the research document at thoughts/shared/research/issue-${issue_number}.md to understand the context and findings.
2) Analyze the specific requirements for: "${task_description}".
3) Create a detailed implementation plan that covers technical approach, file changes, testing strategy, and risks.
4) Write it to path EXACTLY: ${output_path}
5) The file MUST be created in this run (non-empty), with this minimal structure:

---
date: [ISO timestamp]
branch: "${feature_ref}"
repository: "${repository}"
issue: "${issue_number}"
task_pack_id: "plan"
task_pack_version: 1
runner: "claude-code"
status: complete
---

# Implementation Plan for Issue #${issue_number}

## Overview
- Brief summary of the feature/fix to be implemented
- Key objectives and success criteria

## Technical Approach
- Architecture decisions and design patterns
- Integration points with existing systems
- Data flow and API changes

## Implementation Steps
1. **Phase 1**: [Description]
   - Specific files to modify/create
   - Code changes required
   
2. **Phase 2**: [Description]
   - Dependencies and prerequisites
   - Implementation details

3. **Phase 3**: [Description]
   - Testing and validation
   - Deployment considerations

## File Changes
- List specific files that will be modified or created
- Brief description of changes for each file
- Dependencies between changes

## Testing Strategy
- Unit tests to be written/modified
- Integration tests required
- Manual testing scenarios
- Acceptance criteria validation

## Risks and Mitigations
- Technical risks identified
- Potential breaking changes
- Rollback strategy
- Performance considerations

## Dependencies
- External libraries or services
- Team coordination required
- Infrastructure changes needed

## Acceptance Criteria
- Specific, testable requirements
- Definition of done
- Success metrics

RULES:
- Only use Read/Grep/Glob/LS/Edit/Write/TodoWrite. Never call Bash.
- Always reference the research document for context and findings.
- Be specific about file paths and implementation details.
- End by ensuring the file at ${output_path} exists and is non-empty.

================
File: .atriumn-shared/prompts/research.md
================
You are the RESEARCH COORDINATOR for this run.

Context variables (rendered by the workflow):
- feature_ref = "${feature_ref}"
- issue_number = "${issue_number}"
- repository = "${repository}"
- task_description = "${task_description}"
- output_path = "${output_path}"
- decision_record_path = "thoughts/shared/decisions/issue-${issue_number}.md"

Allowed tools only: Read, Grep, Glob, LS, Write, Edit, MultiEdit, TodoWrite
Disallowed: Bash/shell, MCP/CLI/web/Linear/external calls.
Do everything INSIDE THIS ONE SESSION.

Overall goal:
Conduct comprehensive codebase research to answer the user's question by spawning parallel sub-agents INSIDE THIS SESSION, then synthesize a single research document at ${output_path} and a decision record at ${decision_record_path}.

Follow this order (exactly as in the slash command):

(1) Read any directly mentioned files FIRST
- If "${task_description}" mentions specific files/docs/JSON, Read them FULLY (no limit/offset) BEFORE decomposing. Summarize notes to scratch, do not write final docs yet.

(2) Analyze and decompose the research question
- Break down "${task_description}" into clear research areas.
- Use TodoWrite to create a short, explicit research plan (subtasks).

(3) Spawn parallel sub-agents (conceptually; do work sequentially in-session)
Create scratch files for each:
  a) locator → WHERE relevant files/dirs live + 1-line purpose each
     - Write: thoughts/shared/research/tmp/${issue_number}_locator.md

  b) analyzer → HOW key flows work with file:line citations
     - Read the referenced files thoroughly.
     - Write: thoughts/shared/research/tmp/${issue_number}_analyzer.md

  c) patterns → similar implementations, tests, conventions that relate
     - Write: thoughts/shared/research/tmp/${issue_number}_patterns.md

  d) thoughts-locator → relevant docs under thoughts/ related to the topic
     - Write: thoughts/shared/research/tmp/${issue_number}_thoughts_locator.md

  e) thoughts-analyzer → extract key insights from the most relevant thoughts/ docs
     - Cite them by their actual paths.
     - Write: thoughts/shared/research/tmp/${issue_number}_thoughts_analyzer.md

Important:
- Base every claim on files you actually read. Use concrete file:line citations for code.
- thoughts/ material is historical context; code is the primary truth.

(4) Synthesize
- After the five scratch files exist, synthesize them with your initial notes into final outputs.

(5) Write the Research Document at ${output_path}
Frontmatter (only fields you can fill without placeholders):
---
date: [ISO 8601 timestamp you generate]
branch: "${feature_ref}"
repository: "${repository}"
issue: "${issue_number}"
topic: "${task_description}"
runner: "claude-code"
status: "complete"
---

# Research: ${task_description}

## Research Question
- Restate the question in 1–2 lines.

## Summary
- Concise, actionable summary answering the question.

## Detailed Findings
- Organize by component/area discovered from the repo (do not assume tech stack names).
- For each area:
  - Findings with precise file:line citations.
  - Connections to other components.
  - Notable implementation details.

## Code References
- Compact list of concrete file:line citations you used above.

## Architecture Insights
- Patterns, conventions, design decisions discovered.

## Historical Context (from thoughts/)
- Relevant insights with actual thoughts/ paths.

## Related Research
- Paths to other relevant docs inside thoughts/ if applicable.

## Open Questions
- Any gaps or items needing follow-up.

(6) Write the Decision Record at ${decision_record_path}
Frontmatter (only what you can fill):
---
date: [ISO 8601 timestamp you generate]
issue: "${issue_number}"
topic: "${task_description}"
status: "draft"
runner: "claude-code"
---

# Decision Record: Research for Issue #${issue_number}

## Context
- Brief context of the repo and the research topic.

## Options Considered
- Option A: …
- Option B: …
- (List only options you actually found in code/docs.)

## Recommendation
- Your research-based recommendation (keep concise and evidence-backed).

## Risks & Unknowns
- Any notable risks, edge cases, or missing information.

## Next Actions
- Concrete next actions tied to findings (bulleted, short).

Quality guardrails:
- Read mentioned files before decomposition.
- Use TodoWrite for the plan.
- No external calls or web access.
- Cite file:line for code; cite thoughts/ by actual paths.
- Do not invent placeholder metadata fields; omit what you can't derive.
- Ensure all 5 scratch files exist, plus the 2 final artifacts.
- Ensure ${output_path} and ${decision_record_path} exist and are non-empty.

================
File: .atriumn-shared/taskpacks/plan.json
================
{
  "id": "plan",
  "version": 1,
  "runner": "claude-code",
  "allowed_tools": ["Read","Write","Grep","Glob","LS","Edit","MultiEdit","TodoWrite"],
  "output_path_template": "thoughts/shared/plans/issue-${issue_number}.md",
  "prompt_template_path": ".atriumn-shared/prompts/plan.md",
  "check_name": "Phase: Plan",
  "labels": ["phase:plan","bot:claude"],
  "acceptance": {
    "must_create": "thoughts/shared/plans/issue-${issue_number}.md",
    "must_be_non_empty": "thoughts/shared/plans/issue-${issue_number}.md"
  }
}

================
File: .atriumn-shared/taskpacks/research.json
================
{
  "id": "research",
  "version": 1,
  "runner": "claude-code",
  "allowed_tools": ["Read","Write","Grep","Glob","LS","Edit","MultiEdit","TodoWrite"],
  "output_path_template": "thoughts/shared/research/issue-${issue_number}.md",
  "prompt_template_path": ".atriumn-shared/prompts/research.md",
  "check_name": "Phase: Research",
  "labels": ["phase:research","bot:claude"],
  "acceptance": {
    "must_create": "thoughts/shared/research/issue-${issue_number}.md",
    "must_be_non_empty": "thoughts/shared/research/issue-${issue_number}.md"
  }
}

================
File: .github/workflows/development-pipeline.yml
================
name: Atriumn Task Runner
on:
  workflow_call:
    inputs:
      repo_name:      { required: true,  type: string }
      issue_number:   { required: true,  type: string }
      feature_ref:    { required: false, type: string }
      action:         { required: true,  type: string } # run | approve
      phase:          { required: true,  type: string } # research|plan|implement|validate
      task_description: { required: false, type: string }
      trigger_comment:  { required: false, type: string }
jobs:
  run-phase:
    runs-on: ubuntu-latest
    permissions:
      contents: write
      issues: write
      pull-requests: write
      actions: read
      id-token: write
    steps:
      - name: Checkout consumer repo (target)
        uses: actions/checkout@v4
        with:
          repository: ${{ inputs.repo_name }}
          ref: ${{ inputs.feature_ref || '' }}
          fetch-depth: 1
          path: consumer
      - name: Checkout shared repo (this)
        uses: actions/checkout@v4
        with:
          # this workflow already runs from the shared repo; still checkout to get files in workspace
          fetch-depth: 1
          path: shared
      - name: Load prompt from task-pack file
        id: loadprompt
        run: |
          case "${{ inputs.phase }}" in
            research)  PROMPT_FILE="shared/.atriumn/task-packs/research.md" ;;
            plan)      PROMPT_FILE="shared/.atriumn/task-packs/plan.md" ;;
            implement) PROMPT_FILE="shared/.atriumn/task-packs/implement.md" ;;
            validate)  PROMPT_FILE="shared/.atriumn/task-packs/validate.md" ;;
            *) echo "Unknown phase: ${{ inputs.phase }}"; exit 1 ;;
          esac
          echo "PROMPT_FILE=$PROMPT_FILE" >> $GITHUB_ENV
          # Slurp file into env var safely
          echo "DIRECT_PROMPT<<'EOF'" >> $GITHUB_ENV
          cat "$PROMPT_FILE" >> $GITHUB_ENV
          echo "EOF" >> $GITHUB_ENV
      - name: Allowed tools
        run: |
          TOOLS='Read
          Write
          Edit
          Grep
          Glob
          LS
          TodoWrite
          MultiEdit'
          {
            echo "ALLOWED_TOOLS<<TOOLS"
            echo "$TOOLS"
            echo "TOOLS"
          } >> $GITHUB_ENV
      - name: Run Claude Code (phase: ${{ inputs.phase }})
        uses: anthropics/claude-code-action@beta
        with:
          claude_code_oauth_token: ${{ secrets.CLAUDE_CODE_OAUTH_TOKEN }}
          github_token:            ${{ secrets.GITHUB_TOKEN }}
          allowed_bots:            "*"    # allow App/bot actors
          allowed_tools:           ${{ env.ALLOWED_TOOLS }}
          # DO NOT set 'mode: direct' (causes the invalid-mode error).
          working-directory: consumer
          direct_prompt: |
            You are coordinating the "${{ inputs.phase }}" phase for:
            - repository: "${{ inputs.repo_name }}"
            - issue:      "${{ inputs.issue_number }}"
            - feature_ref:"${{ inputs.feature_ref }}"
            - task_desc:  "${{ inputs.task_description }}"
            - trigger:    "${{ inputs.trigger_comment }}"
            # Output locations (write in the consumer repo working dir):
            - Research doc:   thoughts/shared/research/issue-${{ inputs.issue_number }}.md
            - Decision record:thoughts/shared/decisions/issue-${{ inputs.issue_number }}.md
            # Tools allowed: Read, Grep, Glob, LS, Write, Edit, MultiEdit, TodoWrite.
            # No Bash/MCP/web.
            # Do all work in this single session.
            ${{ env.DIRECT_PROMPT }}
      - name: Show artifacts (debug)
        run: |
          ls -la consumer/thoughts || true

================
File: .github/workflows/multi-repo-test.yml
================
# Multi-Repository Pipeline Test
name: Multi-Repository Pipeline Test
on:
  workflow_dispatch:
    inputs:
      test_repos:
        description: 'Comma-separated list of test repositories'
        required: true
        default: 'curatefor.me,platform-api'
      test_scenarios:
        description: 'Test scenarios to run'
        required: true
        type: choice
        options:
          - 'basic-validation'
          - 'full-pipeline'
          - 'config-variations'
          - 'concurrent-pipelines'
jobs:
  setup-test-matrix:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.matrix.outputs.matrix }}
    steps:
      - name: Generate test matrix
        id: matrix
        run: |
          # Create test matrix based on inputs
          REPOS="${{ github.event.inputs.test_repos }}"
          SCENARIOS="${{ github.event.inputs.test_scenarios }}"
          # Convert to JSON matrix
          python3 << 'EOF'
          import json
          import os
          repos = os.environ['REPOS'].split(',')
          scenarios = [os.environ['SCENARIOS']]
          # Create test configurations for each repo
          matrix = []
          for repo in repos:
              for scenario in scenarios:
                  matrix.append({
                      'repo_name': repo.strip(),
                      'scenario': scenario,
                      'config_file': f'{repo.strip()}.yml'
                  })
          print(json.dumps({'include': matrix}))
          EOF
  test-multi-repo-config:
    needs: setup-test-matrix
    strategy:
      matrix: ${{fromJson(needs.setup-test-matrix.outputs.matrix)}}
      fail-fast: false
    runs-on: ubuntu-latest
    steps:
      - name: Checkout shared workflows
        uses: actions/checkout@v4
      - name: Install dependencies
        run: |
          sudo wget -qO /usr/local/bin/yq https://github.com/mikefarah/yq/releases/latest/download/yq_linux_amd64
          sudo chmod +x /usr/local/bin/yq
          sudo apt-get update && sudo apt-get install -y jq
          pip install jsonschema pyyaml
      - name: Test configuration loading
        run: |
          CONFIG_FILE="configs/${{ matrix.config_file }}"
          if [ ! -f "$CONFIG_FILE" ]; then
            echo "❌ Configuration file not found: $CONFIG_FILE"
            exit 1
          fi
          echo "✅ Configuration file exists: $CONFIG_FILE"
          # Validate configuration using our validation script
          if python3 scripts/validate-config.py "$CONFIG_FILE"; then
            echo "✅ Configuration validation passed"
          else
            echo "❌ Configuration validation failed"
            exit 1
          fi
      - name: Test multi-repo validation scripts
        run: |
          # Test the multi-repo aware validation script
          CONFIG_FILE="configs/${{ matrix.config_file }}"
          # Create test documents based on repo configuration
          CONFIG_JSON=$(yq eval -o=json '.' "$CONFIG_FILE")
          THOUGHTS_DIR=$(echo "$CONFIG_JSON" | jq -r '.thoughts_directory')
          mkdir -p "test/$THOUGHTS_DIR/shared/research"
          # Create test research document with repo-specific content
          REPO_NAME="${{ matrix.repo_name }}"
          cat > "test/$THOUGHTS_DIR/shared/research/test-research.md" << EOF
          ---
          date: 2025-08-17T14:30:00-05:00
          researcher: test-user
          topic: "Multi-repo test research for $REPO_NAME"
          status: complete
          ---
          # Research: Multi-repo Test for $REPO_NAME
          ## Research Question
          How does the pipeline work for $REPO_NAME repository configuration?
          ## Summary
          This is a test research document for multi-repo validation of $REPO_NAME.
          ## Detailed Findings
          Repository-specific findings for $REPO_NAME implementation.
          ## Code References
          - \`src/main.js:45\` - Main function initialization
          - \`lib/utils.ts:67\` - Utility functions for validation
          - \`config/app.yml:12\` - Configuration settings
          EOF
          # Add repo-specific content
          case "$REPO_NAME" in
            "platform-api")
              cat >> "test/$THOUGHTS_DIR/shared/research/test-research.md" << EOF
          - \`api/routes.js:23\` - API route definitions
          - \`security/auth.js:15\` - Authentication middleware
          ## Architecture Insights
          Platform API architecture supports microservice patterns with security considerations.
          API endpoints are designed for scalability and performance optimization.
          EOF
              ;;
            "curatefor.me")
              cat >> "test/$THOUGHTS_DIR/shared/research/test-research.md" << EOF
          - \`src/humanlayer.js:89\` - Human layer integration
          ## Architecture Insights
          CurateFor.me architecture integrates human workflows with automated curation.
          The system follows a modular approach with clear user experience patterns.
          EOF
              ;;
          esac
          # Test validation with repo-specific config
          echo "🧪 Testing multi-repo validation for $REPO_NAME..."
          if ./scripts/validate-research-multi.sh "$CONFIG_FILE" "test/$THOUGHTS_DIR/shared/research/test-research.md"; then
            echo "✅ Multi-repo validation passed for $REPO_NAME"
          else
            echo "❌ Multi-repo validation failed for $REPO_NAME"
            exit 1
          fi
      - name: Test repo-specific customizations
        run: |
          CONFIG_FILE="configs/${{ matrix.config_file }}"
          CONFIG_JSON=$(yq eval -o=json '.' "$CONFIG_FILE")
          # Test repository-specific features
          REPO_NAME="${{ matrix.repo_name }}"
          echo "🔍 Testing $REPO_NAME specific features..."
          case "$REPO_NAME" in
            "platform-api")
              echo "Validating platform API configuration..."
              MIN_REFS=$(echo "$CONFIG_JSON" | jq -r '.validation.research_min_refs')
              if [ "$MIN_REFS" -ne 5 ]; then
                echo "❌ Platform API should require 5 file references, got $MIN_REFS"
                exit 1
              fi
              BASE_BRANCH=$(echo "$CONFIG_JSON" | jq -r '.base_branch')
              if [ "$BASE_BRANCH" != "main" ]; then
                echo "❌ Platform API should use main branch, got $BASE_BRANCH"
                exit 1
              fi
              echo "✅ Platform API configuration validated"
              ;;
            "curatefor.me")
              echo "Validating curatefor.me configuration..."
              BASE_BRANCH=$(echo "$CONFIG_JSON" | jq -r '.base_branch')
              if [ "$BASE_BRANCH" != "develop" ]; then
                echo "❌ curatefor.me should use develop branch, got $BASE_BRANCH"
                exit 1
              fi
              PARALLEL_PIPELINES=$(echo "$CONFIG_JSON" | jq -r '.workflow_customization.parallel_pipelines')
              if [ "$PARALLEL_PIPELINES" -ne 3 ]; then
                echo "❌ curatefor.me should allow 3 parallel pipelines, got $PARALLEL_PIPELINES"
                exit 1
              fi
              echo "✅ curatefor.me configuration validated"
              ;;
          esac
      - name: Test configuration recommendations
        run: |
          CONFIG_FILE="configs/${{ matrix.config_file }}"
          echo "📊 Generating configuration recommendations..."
          python3 scripts/validate-config.py "$CONFIG_FILE" --report --output json > config_report.json
          # Check that report was generated successfully
          if jq -e '.valid' config_report.json > /dev/null; then
            echo "✅ Configuration report generated"
            echo "📋 Report summary:"
            jq -r '.summary | to_entries[] | "\(.key): \(.value)"' config_report.json
            # Check for recommendations
            RECOMMENDATIONS=$(jq '.recommendations | length' config_report.json)
            echo "💡 Recommendations: $RECOMMENDATIONS"
          else
            echo "❌ Configuration report generation failed"
            jq '.errors[]' config_report.json
            exit 1
          fi
  test-concurrent-pipelines:
    if: github.event.inputs.test_scenarios == 'concurrent-pipelines'
    runs-on: ubuntu-latest
    steps:
      - name: Checkout shared workflows
        uses: actions/checkout@v4
      - name: Simulate concurrent pipeline limits
        run: |
          echo "🧪 Testing concurrent pipeline limitations..."
          # Test that different repos can have different pipeline limits
          CURATEFOR_LIMIT=$(yq eval '.workflow_customization.parallel_pipelines' configs/curatefor.me.yml)
          PLATFORM_LIMIT=$(yq eval '.workflow_customization.parallel_pipelines' configs/platform-api.yml)
          echo "curatefor.me parallel limit: $CURATEFOR_LIMIT"
          echo "platform-api parallel limit: $PLATFORM_LIMIT"
          if [ "$CURATEFOR_LIMIT" -ne 3 ] || [ "$PLATFORM_LIMIT" -ne 2 ]; then
            echo "❌ Concurrent pipeline limits not configured correctly"
            exit 1
          fi
          echo "✅ Concurrent pipeline limits validated"
  test-cross-repo-isolation:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout shared workflows
        uses: actions/checkout@v4
      - name: Install dependencies
        run: |
          sudo wget -qO /usr/local/bin/yq https://github.com/mikefarah/yq/releases/latest/download/yq_linux_amd64
          sudo chmod +x /usr/local/bin/yq
          sudo apt-get update && sudo apt-get install -y jq
      - name: Test repository isolation
        run: |
          echo "🧪 Testing that pipelines in different repos don't interfere..."
          # Test that branch naming patterns are different
          CURATEFOR_PREFIX=$(yq eval '.branches.prefix' configs/curatefor.me.yml)
          PLATFORM_PREFIX=$(yq eval '.branches.prefix' configs/platform-api.yml)
          echo "curatefor.me branch prefix: $CURATEFOR_PREFIX"
          echo "platform-api branch prefix: $PLATFORM_PREFIX"
          # Test that thoughts directories can be different
          CURATEFOR_THOUGHTS=$(yq eval '.thoughts_directory' configs/curatefor.me.yml)
          PLATFORM_THOUGHTS=$(yq eval '.thoughts_directory' configs/platform-api.yml)
          echo "curatefor.me thoughts dir: $CURATEFOR_THOUGHTS"
          echo "platform-api thoughts dir: $PLATFORM_THOUGHTS"
          if [ "$CURATEFOR_THOUGHTS" = "$PLATFORM_THOUGHTS" ]; then
            echo "ℹ️  Both repos use same thoughts directory (this is OK)"
          else
            echo "ℹ️  Repos use different thoughts directories (this provides isolation)"
          fi
          # Test that validation rules are repo-specific
          CURATEFOR_MIN_REFS=$(yq eval '.validation.research_min_refs' configs/curatefor.me.yml)
          PLATFORM_MIN_REFS=$(yq eval '.validation.research_min_refs' configs/platform-api.yml)
          if [ "$CURATEFOR_MIN_REFS" -eq "$PLATFORM_MIN_REFS" ]; then
            echo "⚠️  Both repos have same validation requirements"
          else
            echo "✅ Repos have different validation requirements (good isolation)"
          fi
          echo "✅ Repository isolation test passed"
  test-config-schema-validation:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout shared workflows
        uses: actions/checkout@v4
      - name: Install dependencies
        run: |
          pip install jsonschema pyyaml
      - name: Test all configurations against schema
        run: |
          echo "🧪 Testing all configurations against schema..."
          # Test each config file
          for config_file in configs/*.yml; do
            # Skip schema file itself
            if [[ "$config_file" == "configs/schema.yml" ]]; then
              continue
            fi
            echo "Testing $config_file..."
            if python3 scripts/validate-config.py "$config_file"; then
              echo "✅ $config_file validation passed"
            else
              echo "❌ $config_file validation failed"
              exit 1
            fi
          done
          echo "✅ All configurations passed schema validation"
  report-multi-repo-results:
    needs: [test-multi-repo-config, test-concurrent-pipelines, test-cross-repo-isolation, test-config-schema-validation]
    if: always()
    runs-on: ubuntu-latest
    steps:
      - name: Collect test results
        run: |
          echo "📊 Multi-Repository Test Results"
          echo "==============================="
          # Collect results from matrix jobs
          echo "Configuration Tests:"
          echo "- Matrix jobs: ${{ needs.test-multi-repo-config.result }}"
          echo ""
          echo "Concurrent Pipeline Tests:"
          echo "- Status: ${{ needs.test-concurrent-pipelines.result }}"
          echo ""
          echo "Cross-repo Isolation Tests:"
          echo "- Status: ${{ needs.test-cross-repo-isolation.result }}"
          echo ""
          echo "Schema Validation Tests:"
          echo "- Status: ${{ needs.test-config-schema-validation.result }}"
          # Overall status
          OVERALL_SUCCESS=true
          if [ "${{ needs.test-multi-repo-config.result }}" != "success" ]; then
            OVERALL_SUCCESS=false
          fi
          if [ "${{ needs.test-cross-repo-isolation.result }}" != "success" ]; then
            OVERALL_SUCCESS=false
          fi
          if [ "${{ needs.test-config-schema-validation.result }}" != "success" ]; then
            OVERALL_SUCCESS=false
          fi
          if [ "$OVERALL_SUCCESS" = "true" ]; then
            echo ""
            echo "✅ Multi-repository pipeline system ready!"
            echo ""
            echo "🎯 Test Summary:"
            echo "- Multi-repo configurations validated"
            echo "- Repository-specific validation working"
            echo "- Cross-repository isolation confirmed"
            echo "- Configuration schema validation passed"
          else
            echo ""
            echo "❌ Multi-repository tests failed - review results above"
            exit 1
          fi

================
File: .github/workflows/run-claude-task.yml
================
name: Run Claude Task
on:
  workflow_call:
    inputs:
      task_pack_id:
        required: true
        type: string
      feature_ref:
        required: true
        type: string
      issue_number:
        required: true
        type: string
      task_description:
        required: true
        type: string
      task_name:
        required: false
        type: string
        default: research
      runner:
        required: false
        type: string
        default: claude-code
    secrets:
      CLAUDE_CODE_OAUTH_TOKEN:
        required: true
      GH_TOKEN_FOR_BOT:
        required: true
jobs:
  run:
    runs-on: ubuntu-latest
    permissions:
      contents: write
      pull-requests: write
      issues: write
      actions: read
    steps:
      - name: Checkout repository (caller repo)
        uses: actions/checkout@v4
        with:
          ref: ${{ inputs.feature_ref }}
          fetch-depth: 0
      - name: Checkout shared workflows
        uses: actions/checkout@v4
        with:
          repository: atriumn/atriumn-issue-driven-development
          path: _shared
      - name: Resolve Task Pack (JSON) + Prompt
        id: resolve
        uses: actions/github-script@v7
        env:
          TASK_PACK_ID: ${{ inputs.task_pack_id }}
          FEATURE_REF:  ${{ inputs.feature_ref }}
          ISSUE_NUMBER: ${{ inputs.issue_number }}
          TASK_DESC:    ${{ inputs.task_description }}
          REPO_FULL:    ${{ github.repository }}
        with:
          script: |
            const fs = require('fs');
            const path = require('path');
            const packPath = path.join('_shared', '.atriumn-shared', 'taskpacks', `${process.env.TASK_PACK_ID}.json`);
            const pack = JSON.parse(fs.readFileSync(packPath, 'utf8'));
            function render(str, vars) {
              return str.replace(/\$\{(\w+)\}/g, (_, k) => String(vars[k] ?? ''));
            }
            const vars = {
              feature_ref: process.env.FEATURE_REF,
              issue_number: process.env.ISSUE_NUMBER,
              task_description: process.env.TASK_DESC,
              repository: process.env.REPO_FULL,
            };
            const outputPath = render(pack.output_path_template, vars);
            vars.output_path = outputPath;
            const promptPath = path.join('_shared', pack.prompt_template_path);
            const rawPrompt = fs.readFileSync(promptPath, 'utf8');
            const directPrompt = render(rawPrompt, vars);
            core.setOutput('allowed_tools', JSON.stringify(pack.allowed_tools || []));
            core.setOutput('direct_prompt', directPrompt);
            core.setOutput('output_path', outputPath);
            core.setOutput('check_name', pack.check_name || 'Claude Task');
            core.setOutput('labels', JSON.stringify(pack.labels || []));
            core.setOutput('pack_id', pack.id);
            core.setOutput('pack_version', String(pack.version || '1'));
      - name: Ensure output directory exists (filesystem only)
        run: |
          mkdir -p "$(dirname '${{ steps.resolve.outputs.output_path }}')"
      - name: Run Claude Code
        if: ${{ inputs.runner == 'claude-code' }}
        uses: anthropics/claude-code-action@beta
        with:
          claude_code_oauth_token: ${{ secrets.CLAUDE_CODE_OAUTH_TOKEN }}
          github_token:            ${{ secrets.GH_TOKEN_FOR_BOT }}
          mode: agent
          timeout_minutes: 40
          max_turns: 40
          allowed_tools: ${{ steps.resolve.outputs.allowed_tools }}
          direct_prompt: ${{ steps.resolve.outputs.direct_prompt }}
      - name: Assert output exists and is non-empty
        run: |
          test -s "${{ steps.resolve.outputs.output_path }}" \
            || (echo "Expected output missing or empty: ${{ steps.resolve.outputs.output_path }}" && exit 1)
      - name: Commit & push changes
        run: |
          git config user.name  "Pipeline Bot"
          git config user.email "pipeline@atriumn.com"
          git add -A
          git commit -m "taskpack:${{ steps.resolve.outputs.pack_id }} issue #${{ inputs.issue_number }} (${{
            steps.resolve.outputs.output_path
          }})" || echo "No changes to commit"
          git push origin "${{ inputs.feature_ref }}" || true
      - name: Emit meta manifest
        run: |
          META="${{ steps.resolve.outputs.output_path }}.meta.json"
          cat > "$META" <<JSON
          {
            "task_pack_id": "${{ steps.resolve.outputs.pack_id }}",
            "task_pack_version": "${{ steps.resolve.outputs.pack_version }}",
            "runner": "${{ inputs.runner }}",
            "output_path": "${{ steps.resolve.outputs.output_path }}",
            "issue_number": "${{ inputs.issue_number }}",
            "feature_ref": "${{ inputs.feature_ref }}",
            "repository": "${{ github.repository }}",
            "commit_sha": "$(git rev-parse HEAD)",
            "status": "completed"
          }
          JSON
          git add "$META"
          git commit -m "meta: $META" || true
          git push origin "${{ inputs.feature_ref }}" || true
      # PR creation moved to GitHub App due to repository restrictions
      # Check run creation moved to GitHub App for marketplace compatibility
      - name: Comment on issue with artifact link
        if: always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const { owner, repo } = context.repo;
            const issue_number = Number("${{ inputs.issue_number }}");
            const path = "${{ steps.resolve.outputs.output_path }}";
            const taskPackId = "${{ steps.resolve.outputs.pack_id }}";
            const url = `https://github.com/${owner}/${repo}/blob/${{ inputs.feature_ref }}/${path}`;
            // Phase display names and next actions
            const phaseInfo = {
              'research': { name: 'Research', next: 'plan', nextAction: '/atriumn-approve-research' },
              'plan': { name: 'Plan', next: 'implement', nextAction: '/atriumn-approve-plan' },
              'implement': { name: 'Implementation', next: 'validate', nextAction: '/atriumn-approve-implement' },
              'validate': { name: 'Validation', next: null, nextAction: null }
            };
            const current = phaseInfo[taskPackId] || { name: taskPackId.charAt(0).toUpperCase() + taskPackId.slice(1), next: null, nextAction: null };
            // Try to extract key content from the artifact for richer commentary
            let summary = '';
            let codebaseOverview = '';
            try {
              if (fs.existsSync(path)) {
                const content = fs.readFileSync(path, 'utf8');
                // Try different section patterns based on task pack
                let sectionMatch;
                if (taskPackId === 'research') {
                  sectionMatch = content.match(/## Summary\s*\n([\s\S]*?)(?=\n##|\n---|\n\*\*|$)/);
                } else if (taskPackId === 'plan') {
                  sectionMatch = content.match(/## Overview\s*\n([\s\S]*?)(?=\n##|\n---|\n\*\*|$)/);
                } else if (taskPackId === 'implement') {
                  sectionMatch = content.match(/## Implementation Summary\s*\n([\s\S]*?)(?=\n##|\n---|\n\*\*|$)/);
                } else if (taskPackId === 'validate') {
                  sectionMatch = content.match(/## Validation Results\s*\n([\s\S]*?)(?=\n##|\n---|\n\*\*|$)/);
                }
                if (sectionMatch) {
                  const fullSection = sectionMatch[1].trim();
                  // For research: split into codebase overview and key findings
                  if (taskPackId === 'research') {
                    const parts = fullSection.split(/\*\*Key risks|Key risks/);
                    if (parts.length > 1) {
                      codebaseOverview = parts[0].trim();
                      summary = `**Key risks${parts[1]}`.substring(0, 600);
                      if (parts[1].length > 600) summary += '...';
                    } else {
                      summary = fullSection.substring(0, 800);
                      if (fullSection.length > 800) summary += '...';
                    }
                  } else {
                    // For other phases, just use the section content
                    summary = fullSection.substring(0, 800);
                    if (fullSection.length > 800) summary += '...';
                  }
                }
              }
            } catch (e) {
              console.log('Could not read artifact for summary:', e.message);
            }
            // Build the comment body with improved formatting
            const bodyParts = [
              `<img src="https://atriumn.com/logos/atriumn-logo.png" width="16" height="16" style="vertical-align: middle;"> **${current.name} phase complete**`,
              ""
            ];
            // Add codebase overview if available (for research phase)
            if (codebaseOverview && taskPackId === 'research') {
              bodyParts.push("### 📊 Codebase Overview");
              bodyParts.push(codebaseOverview);
              bodyParts.push("");
            }
            // Add key content if available
            if (summary) {
              const sectionTitles = {
                'research': '🔍 Key Findings',
                'plan': '📋 Implementation Overview',  
                'implement': '⚒️ Implementation Details',
                'validate': '✅ Validation Results'
              };
              const sectionTitle = sectionTitles[taskPackId] || '📋 Key Details';
              bodyParts.push(`### ${sectionTitle}`);
              bodyParts.push(summary);
              bodyParts.push("");
            }
            // Add artifact link and next steps
            bodyParts.push("---");
            bodyParts.push(`📄 **Full Analysis:** [\`${path}\`](${url})`);
            bodyParts.push("");
            if (current.nextAction) {
              bodyParts.push(`🚀 **Next Step:** Comment \`${current.nextAction}\` to proceed to ${current.next}.`);
              bodyParts.push("");
              bodyParts.push(`> **Quick Copy:** \`${current.nextAction}\``);
            } else {
              bodyParts.push('🎉 **All phases complete!**');
            }
            const body = bodyParts.join("\n");
            await github.rest.issues.createComment({ owner, repo, issue_number, body });

================
File: .github/workflows/simple-pipeline.yml
================
name: Simple Development Pipeline
on:
  workflow_call:
    inputs:
      repo_name:
        required: true
        type: string
      issue_number:
        required: false
        type: string
      trigger_comment:
        required: false
        type: string
    secrets:
      REPO_TOKEN:
        required: true
  repository_dispatch:
    types: [pipeline-start]
jobs:
  start-pipeline:
    if: github.event_name == 'repository_dispatch' && github.event.action == 'pipeline-start'
    runs-on: ubuntu-latest
    steps:
      - name: Checkout target repository
        uses: actions/checkout@v4
        with:
          repository: ${{ inputs.repo_name || github.event.repository.full_name }}
          token: ${{ secrets.REPO_TOKEN }}
          fetch-depth: 0
      - name: Start pipeline
        env:
          GH_TOKEN: ${{ secrets.REPO_TOKEN }}
        run: |
          if [ "${{ github.event_name }}" = "repository_dispatch" ]; then
            ISSUE_NUMBER="${{ github.event.client_payload.issue_number }}"
            ISSUE_TITLE="${{ github.event.client_payload.issue_title }}"
            REPO_NAME="${{ github.event.repository.full_name }}"
          else
            ISSUE_NUMBER="${{ inputs.issue_number }}"
            REPO_NAME="${{ inputs.repo_name }}"
            ISSUE_TITLE=$(gh issue view $ISSUE_NUMBER --repo $REPO_NAME --json title --jq '.title')
          fi
          echo "Starting pipeline for issue #$ISSUE_NUMBER: $ISSUE_TITLE"
          gh issue comment $ISSUE_NUMBER --repo $REPO_NAME --body "
          🚀 **Development Pipeline Started**
          **Issue**: #$ISSUE_NUMBER
          **Title**: $ISSUE_TITLE
          **Repository**: $REPO_NAME
          Pipeline is now running...
          "

================
File: .github/workflows/test-minimal.yml
================
name: Test Minimal Pipeline
on:
  repository_dispatch:
    types: [pipeline-start]
jobs:
  test-start:
    if: github.event_name == 'repository_dispatch' && github.event.action == 'pipeline-start'
    runs-on: ubuntu-latest
    steps:
      - name: Test repository dispatch
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          echo "✅ Repository dispatch working!"
          echo "Event: ${{ github.event.action }}"
          echo "Issue: ${{ github.event.client_payload.issue_number }}"
          echo "Title: ${{ github.event.client_payload.issue_title }}"
          echo "User: ${{ github.event.client_payload.issue_user }}"
          ISSUE_NUM="${{ github.event.client_payload.issue_number }}"
          ISSUE_TITLE="${{ github.event.client_payload.issue_title }}"
          echo "Pipeline started successfully for issue #$ISSUE_NUM: $ISSUE_TITLE"

================
File: .github/workflows/test-pipeline.yml
================
# Test workflow that can be triggered manually
name: Test Development Pipeline
on:
  workflow_dispatch:
    inputs:
      test_scenario:
        description: 'Test scenario to run'
        required: true
        type: choice
        options:
          - 'full-pipeline-automated'
          - 'full-pipeline-manual'
          - 'research-only'
          - 'validation-failure'
      test_repo:
        description: 'Target repository for testing (owner/repo)'
        required: true
        type: string
        default: 'atriumn/test-repo'
jobs:
  test-pipeline:
    runs-on: ubuntu-latest
    steps:
      - name: Install GitHub CLI
        run: |
          sudo apt-get update && sudo apt-get install -y gh
      - name: Create test issue
        id: issue
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          # Create test issue for pipeline
          ISSUE_BODY="Test issue for development pipeline
          **Task**: Test the automated development pipeline
          **Scenario**: ${{ github.event.inputs.test_scenario }}
          **Test Date**: $(date)
          **Test Repository**: ${{ github.event.inputs.test_repo }}
          This is an automated test issue created to validate the development pipeline workflow.
          "
          ISSUE_NUMBER=$(gh issue create \
            --repo ${{ github.event.inputs.test_repo }} \
            --title "Test: Development Pipeline - ${{ github.event.inputs.test_scenario }}" \
            --body "$ISSUE_BODY" \
            --label "test,pipeline" \
            --json number --jq '.number')
          echo "number=$ISSUE_NUMBER" >> $GITHUB_OUTPUT
          echo "Created test issue #$ISSUE_NUMBER in ${{ github.event.inputs.test_repo }}"
      - name: Trigger pipeline based on scenario
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          case "${{ github.event.inputs.test_scenario }}" in
            "full-pipeline-automated")
              COMMENT="@claude run development pipeline
              - Base branch: main
              - Human validation: false
              - Auto-proceed: true
              This is a test of the fully automated pipeline."
              ;;
            "full-pipeline-manual")
              COMMENT="@claude run development pipeline
              - Base branch: main
              - Human validation: true
              This is a test of the manual approval pipeline."
              ;;
            "research-only")
              COMMENT="@claude run development pipeline
              - Base branch: main
              - Human validation: true
              - Stop after: research
              This is a test of research phase only."
              ;;
            "validation-failure")
              COMMENT="@claude run development pipeline
              - Base branch: main
              - Human validation: false
              This test is designed to trigger validation failures for testing error handling."
              ;;
            *)
              COMMENT="@claude run development pipeline
              Default test scenario."
              ;;
          esac
          gh issue comment ${{ steps.issue.outputs.number }} \
            --repo ${{ github.event.inputs.test_repo }} \
            --body "$COMMENT"
          echo "Pipeline triggered for issue #${{ steps.issue.outputs.number }}"
      - name: Monitor pipeline progress
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          echo "✅ Test pipeline started successfully"
          echo ""
          echo "📊 Test Details:"
          echo "  - Issue: #${{ steps.issue.outputs.number }}"
          echo "  - Repository: ${{ github.event.inputs.test_repo }}"
          echo "  - Scenario: ${{ github.event.inputs.test_scenario }}"
          echo "  - Monitor at: https://github.com/${{ github.event.inputs.test_repo }}/issues/${{ steps.issue.outputs.number }}"
          echo ""
          echo "🔍 Expected Behavior:"
          case "${{ github.event.inputs.test_scenario }}" in
            "full-pipeline-automated")
              echo "  - Pipeline should proceed automatically through all phases"
              echo "  - No human approval required"
              echo "  - Should create branch, research, plan, implement, and create PR"
              ;;
            "full-pipeline-manual")
              echo "  - Pipeline should pause for human approval at each phase"
              echo "  - Manual approval required with 'approve research' and 'approve plan'"
              echo "  - Full pipeline completion after approvals"
              ;;
            "research-only")
              echo "  - Pipeline should complete research phase only"
              echo "  - Should stop after research validation"
              echo "  - No automatic progression to planning"
              ;;
            "validation-failure")
              echo "  - Pipeline should encounter validation failures"
              echo "  - Error handling should be demonstrated"
              echo "  - Retry mechanisms should be available"
              ;;
          esac
  # Monitor test results (runs after 5 minutes)
  check-test-results:
    runs-on: ubuntu-latest
    needs: test-pipeline
    if: always()
    steps:
      - name: Wait for pipeline execution
        run: sleep 300  # Wait 5 minutes for initial pipeline steps
      - name: Install GitHub CLI
        run: |
          sudo apt-get update && sudo apt-get install -y gh
      - name: Check pipeline status
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          echo "🔍 Checking test results after 5 minutes..."
          echo ""
          # Get issue number from previous job
          ISSUE_NUMBER="${{ needs.test-pipeline.outputs.issue.number }}"
          # Check if issue has pipeline branch label
          LABELS=$(gh issue view $ISSUE_NUMBER --repo ${{ github.event.inputs.test_repo }} --json labels --jq '.labels[].name')
          if echo "$LABELS" | grep -q "branch:"; then
            BRANCH_NAME=$(echo "$LABELS" | grep "branch:" | sed 's/branch://')
            echo "✅ Pipeline branch created: $BRANCH_NAME"
            # Check if branch exists
            if gh api repos/${{ github.event.inputs.test_repo }}/branches/$BRANCH_NAME > /dev/null 2>&1; then
              echo "✅ Branch exists in repository"
            else
              echo "❌ Branch not found in repository"
            fi
          else
            echo "❌ No pipeline branch label found"
          fi
          # Check for decision record
          echo ""
          echo "📋 Issue Comments:"
          gh issue view $ISSUE_NUMBER --repo ${{ github.event.inputs.test_repo }} --json comments --jq '.comments[].body' | tail -3
          echo ""
          echo "🔗 Monitor continued progress at:"
          echo "   https://github.com/${{ github.event.inputs.test_repo }}/issues/$ISSUE_NUMBER"
  # Final cleanup (runs after 1 hour)
  cleanup-test:
    runs-on: ubuntu-latest
    needs: [test-pipeline, check-test-results]
    if: always()
    steps:
      - name: Wait for test completion
        run: sleep 3600  # Wait 1 hour total
      - name: Install GitHub CLI
        run: |
          sudo apt-get update && sudo apt-get install -y gh
      - name: Cleanup test resources
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          ISSUE_NUMBER="${{ needs.test-pipeline.outputs.issue.number }}"
          echo "🧹 Cleaning up test resources..."
          # Check if issue is still open
          STATUS=$(gh issue view $ISSUE_NUMBER --repo ${{ github.event.inputs.test_repo }} --json state --jq '.state')
          if [ "$STATUS" = "OPEN" ]; then
            echo "📝 Adding cleanup comment to open issue"
            gh issue comment $ISSUE_NUMBER --repo ${{ github.event.inputs.test_repo }} --body "
            🧹 **Test Cleanup**
            This was a test issue for the development pipeline. 
            Test scenario: ${{ github.event.inputs.test_scenario }}
            If the pipeline is still running, it will continue to completion.
            If the pipeline has stalled, this issue can be safely closed.
            "
            # For test scenarios, close the issue after adding cleanup comment
            if [[ "${{ github.event.inputs.test_scenario }}" == *"test"* ]]; then
              echo "🔒 Closing test issue"
              gh issue close $ISSUE_NUMBER --repo ${{ github.event.inputs.test_repo }} --reason "not planned"
            fi
          else
            echo "✅ Issue already closed - test completed successfully"
          fi
          # Try to clean up any test branches
          LABELS=$(gh issue view $ISSUE_NUMBER --repo ${{ github.event.inputs.test_repo }} --json labels --jq '.labels[].name' 2>/dev/null || echo "")
          if echo "$LABELS" | grep -q "branch:"; then
            BRANCH_NAME=$(echo "$LABELS" | grep "branch:" | sed 's/branch://')
            echo "🌿 Found test branch: $BRANCH_NAME"
            # Note: We don't automatically delete branches in case the test is still running
            echo "ℹ️  Test branch $BRANCH_NAME can be manually deleted if no longer needed"
          fi
          echo ""
          echo "✅ Test cleanup completed"
          echo "📊 Final test issue: https://github.com/${{ github.event.inputs.test_repo }}/issues/$ISSUE_NUMBER"

================
File: configs/curatefor.me.yml
================
# curatefor.me specific configuration
repo_name: "curatefor.me"
base_branch: "develop"
thoughts_directory: "thoughts/"
validation:
  research_min_refs: 3
  implementation_test_commands:
    - "make test"
    - "make lint"
    - "make typecheck"
  pr_required_sections:
    - "Summary"
    - "Related Documents" 
    - "Testing"
    - "Changes Made"
    - "Verification"
notifications:
  slack_channel: "#dev-team"
  escalation_hours: 2
team:
  default_reviewers: ["@jeff-atriumn"]
  domain_experts:
    frontend: "@frontend-expert"
    backend: "@backend-expert"
    infrastructure: "@devops-lead"
  tech_lead: "@tech-lead"
branches:
  prefix: "feature/"
  naming: "issue-{number}-{title-slug}"
  cleanup_merged: true
workflow_customization:
  auto_proceed_default: false
  phase_timeouts:
    research_hours: 4
    planning_hours: 2
    implementation_hours: 24
  parallel_pipelines: 3
integration_settings:
  deployment_environments: ["staging", "production"]

================
File: configs/default.yml
================
# Default configuration that all repos inherit
repo_name: "default"
base_branch: "main"
thoughts_directory: "thoughts/"
version: "1.0.0"
# Validation settings
validation:
  enabled: true
  strict_mode: false
  research_min_refs: 3
  plan_required_sections: 
    - "## Implementation Approach"
    - "## Phase 1:"
    - "#### Automated Verification:"
    - "#### Manual Verification:"
  implementation_test_commands:
    - "make test"
# Pipeline settings
pipeline:
  timeout: 30
  retry_count: 3
# Notification settings
notifications:
  enabled: true
  channels: []
  escalation_hours: 2
# Branch settings
branches:
  prefix: "feature/"
  naming: "issue-{number}-{title-slug}"
# File patterns
file_patterns:
  research: "{thoughts_directory}/shared/research/"
  plans: "{thoughts_directory}/shared/plans/"
  decisions: "{thoughts_directory}/shared/decisions/"
# Team settings
team:
  default_reviewers: []

================
File: configs/platform-api.yml
================
# platform-api configuration showing variations
repo_name: "platform-api"
base_branch: "main"
thoughts_directory: "docs/decisions/"
validation:
  research_min_refs: 5  # Stricter requirements
  implementation_test_commands:
    - "npm test"
    - "npm run lint"
    - "npm run security-scan"
    - "docker build -t test ."
  pr_required_sections:
    - "API Changes"
    - "Database Migrations"
    - "Security Review"
    - "Performance Impact"
notifications:
  slack_channel: "#platform-engineering"
  email_list: ["platform-team@company.com"]
  escalation_hours: 1  # Faster escalation for critical platform
team:
  default_reviewers: ["@platform-lead", "@security-reviewer"]
  domain_experts:
    backend: "@senior-backend-dev"
    infrastructure: "@platform-architect"
    security: "@security-lead"
  tech_lead: "@platform-lead"
branches:
  prefix: "feat/"
  naming: "{number}-{title-slug}"
  cleanup_merged: false  # Keep branches for audit
workflow_customization:
  auto_proceed_default: false
  phase_timeouts:
    research_hours: 8  # More time for platform research
    planning_hours: 4
    implementation_hours: 48
  parallel_pipelines: 2  # Limit concurrent work
integration_settings:
  jira_project: "PLAT"
  deployment_environments: ["dev", "staging", "prod-us", "prod-eu"]

================
File: configs/schema.yml
================
# configs/schema.yml - Complete multi-repository configuration schema
configuration_schema:
  version: "1.0"
  required_fields:
    - repo_name
    - base_branch
    - thoughts_directory
  optional_fields:
    - validation
    - notifications  
    - team
    - branches
    - file_patterns
    - workflow_customization
    - integration_settings
field_definitions:
  repo_name:
    type: string
    description: "Repository identifier for configuration lookup"
    example: "curatefor.me"
  base_branch:
    type: string
    description: "Default branch to create feature branches from"
    default: "main"
    allowed_values: ["main", "master", "develop", "dev"]
  thoughts_directory:
    type: string
    description: "Root directory for thoughts/documentation"
    default: "thoughts/"
    pattern: "^[a-zA-Z0-9_/-]+/$"
  validation:
    type: object
    properties:
      research_min_refs:
        type: integer
        default: 3
        minimum: 1
        description: "Minimum file references required in research documents"
      plan_required_sections:
        type: array
        default: ["## Implementation Approach", "## Phase", "#### Automated Verification:", "#### Manual Verification:"]
        description: "Sections that must be present in implementation plans"
      implementation_test_commands:
        type: array
        default: ["make test"]
        description: "Commands to run for implementation validation"
      pr_required_sections:
        type: array
        default: ["Summary", "Related Documents", "Testing", "Changes Made"]
        description: "Required sections in PR descriptions"
  notifications:
    type: object
    properties:
      slack_channel:
        type: string
        pattern: "^#[a-zA-Z0-9_-]+$"
        description: "Slack channel for pipeline notifications"
      email_list:
        type: array
        items:
          type: string
          format: email
        description: "Email addresses for notifications"
      escalation_hours:
        type: integer
        default: 2
        minimum: 1
        maximum: 24
        description: "Hours before escalating stalled pipelines"
  team:
    type: object
    properties:
      default_reviewers:
        type: array
        items:
          type: string
          pattern: "^@[a-zA-Z0-9_-]+$"
        description: "Default PR reviewers"
      domain_experts:
        type: object
        properties:
          frontend:
            type: string
            pattern: "^@[a-zA-Z0-9_-]+$"
          backend:
            type: string
            pattern: "^@[a-zA-Z0-9_-]+$"
          infrastructure:
            type: string
            pattern: "^@[a-zA-Z0-9_-]+$"
          design:
            type: string
            pattern: "^@[a-zA-Z0-9_-]+$"
          security:
            type: string
            pattern: "^@[a-zA-Z0-9_-]+$"
        description: "Domain-specific expert assignments"
      tech_lead:
        type: string
        pattern: "^@[a-zA-Z0-9_-]+$"
        description: "Technical lead for escalations"
  branches:
    type: object
    properties:
      prefix:
        type: string
        default: "feature/"
        description: "Prefix for feature branch names"
      naming:
        type: string
        default: "issue-{number}-{title-slug}"
        description: "Branch naming pattern"
      cleanup_merged:
        type: boolean
        default: true
        description: "Auto-delete merged feature branches"
  file_patterns:
    type: object
    properties:
      research:
        type: string
        default: "{thoughts_directory}/shared/research/"
        description: "Research document location pattern"
      plans:
        type: string
        default: "{thoughts_directory}/shared/plans/"
        description: "Implementation plan location pattern"
      decisions:
        type: string
        default: "{thoughts_directory}/shared/decisions/"
        description: "Decision record location pattern"
  workflow_customization:
    type: object
    properties:
      auto_proceed_default:
        type: boolean
        default: false
        description: "Default human validation setting"
      phase_timeouts:
        type: object
        properties:
          research_hours:
            type: integer
            default: 4
          planning_hours:
            type: integer
            default: 2
          implementation_hours:
            type: integer
            default: 24
      parallel_pipelines:
        type: integer
        default: 3
        minimum: 1
        maximum: 10
        description: "Maximum concurrent pipelines per repo"
  integration_settings:
    type: object
    properties:
      linear_workspace:
        type: string
        description: "Linear workspace ID for issue sync"
      jira_project:
        type: string
        description: "Jira project key for issue sync"
      deployment_environments:
        type: array
        items:
          type: string
        description: "Available deployment environments"
# Schema validation rules
validation_rules:
  repo_name:
    - "Must be unique across all repositories"
    - "Should match actual repository name"
    - "Use kebab-case format (lowercase with hyphens)"
  base_branch:
    - "Must exist in the target repository"
    - "Should be the main development branch"
  thoughts_directory:
    - "Must end with trailing slash"
    - "Should be relative path from repository root"
    - "Directory will be created if it doesn't exist"
  validation:
    - "Commands in implementation_test_commands must be valid for the repository"
    - "Required sections should match repository documentation standards"
  team:
    - "All usernames must be valid GitHub usernames"
    - "Users should have repository access"
  notifications:
    - "Slack channels should exist and bot should have access"
    - "Email addresses should be valid and monitored"
# Configuration examples by repository type
repository_type_examples:
  frontend_application:
    description: "Single-page applications, React/Vue/Angular apps"
    typical_config:
      validation:
        research_min_refs: 3
        implementation_test_commands:
          - "npm test"
          - "npm run lint" 
          - "npm run typecheck"
          - "npm run build"
      workflow_customization:
        phase_timeouts:
          research_hours: 2
          planning_hours: 1
          implementation_hours: 8
  backend_api:
    description: "REST APIs, GraphQL services, microservices"
    typical_config:
      validation:
        research_min_refs: 5
        implementation_test_commands:
          - "make test"
          - "make lint"
          - "make security-scan"
          - "docker build -t test ."
        pr_required_sections:
          - "API Changes"
          - "Database Migrations" 
          - "Security Review"
      workflow_customization:
        phase_timeouts:
          research_hours: 4
          planning_hours: 2
          implementation_hours: 16
  infrastructure_platform:
    description: "Infrastructure as code, platform services"
    typical_config:
      validation:
        research_min_refs: 8
        implementation_test_commands:
          - "terraform plan"
          - "terraform validate"
          - "make security-scan"
          - "make compliance-check"
      notifications:
        escalation_hours: 1
      workflow_customization:
        auto_proceed_default: false
        parallel_pipelines: 1
        phase_timeouts:
          research_hours: 8
          planning_hours: 4
          implementation_hours: 48

================
File: docs/phase2-implementation-summary.md
================
# Phase 2: GitHub Actions Implementation - Complete

## Overview

Phase 2 successfully implements a complete GitHub Actions workflow system that automates the entire development pipeline from issue research through PR creation. The implementation provides both automated and human-validated modes, with comprehensive error handling and cross-repository compatibility.

## Architecture

### Shared Workflow Design

The architecture uses GitHub's shared workflow feature to centralize pipeline logic while respecting security boundaries:

```
┌─────────────────────────────────────┐
│        Shared Workflow              │
│  (atriumn-shared-workflows)         │
│                                     │
│  • Issue research & analysis        │
│  • Planning & validation            │
│  • Implementation validation        │
│  • Human approval workflow          │
│  • Decision record management       │
│  • Cross-repo orchestration         │
└─────────────────────────────────────┘
                    │
                    │ triggers
                    ▼
┌─────────────────────────────────────┐
│        Repository Workflow          │
│    (individual repositories)       │
│                                     │
│  • Issue comment detection          │
│  • Pipeline trigger logic           │
│  • Secret management                │
│  • Repository-specific config       │
└─────────────────────────────────────┘
```

### Key Components

1. **Main Shared Workflow** (`.github/workflows/development-pipeline.yml`)
   - Complete pipeline orchestration
   - Research, planning, and implementation validation
   - Human approval checkpoints
   - Decision record management
   - Error handling and retry logic

2. **Repository Template** (`templates/repo-workflow-template.yml`)
   - Easy adoption for any repository
   - Configurable trigger patterns
   - Secret management setup
   - Usage documentation

3. **Test Framework** (`.github/workflows/test-development-pipeline.yml`)
   - Multiple test scenarios
   - Automated validation
   - Progress monitoring
   - Cleanup automation

## Workflow Jobs

### Core Pipeline Jobs

1. **load-config**
   - Loads repository-specific or default configuration
   - Extracts base branch, thoughts directory, branch prefix
   - Makes configuration available to all subsequent jobs

2. **start-pipeline** 
   - Triggered by `@claude run development pipeline` comment
   - Parses pipeline configuration from comment
   - Creates feature branch from base branch
   - Initializes decision record
   - Triggers research phase with detailed instructions

3. **validate-research**
   - Triggered by `✅ Research Phase Complete` comment
   - Finds and validates research document using Phase 1 scripts
   - Updates decision record with research results
   - Handles validation failures with clear error messages
   - Routes to human approval or auto-proceeds based on configuration

4. **trigger-planning**
   - Triggered by `approve research` comment
   - Records human approval in decision record
   - Triggers planning phase with detailed instructions

### Validation Jobs (Extended)

The extended workflow includes additional jobs for planning and implementation phases:

- **validate-planning**: Validates implementation plans using Phase 1 scripts
- **trigger-implementation**: Handles plan approval and triggers implementation
- **validate-implementation**: Validates completed implementation using Phase 1 scripts

## Configuration System

### Repository-Specific Configuration

Each repository can override defaults by creating `.github/development-pipeline-config.yml`:

```yaml
repo_name: "my-repo"
base_branch: "develop"
thoughts_directory: "docs/"

validation:
  research_min_refs: 5
  implementation_test_commands:
    - "npm test"
    - "npm run lint"
    - "npm run typecheck"

team:
  default_reviewers: ["@team-lead"]

notifications:
  slack_channel: "#dev-team"
```

### Pipeline Modes

1. **Human Validation Mode** (default)
   - Research → Human approval → Planning → Human approval → Implementation → PR
   - Provides review checkpoints at each phase
   - Allows for course correction and quality control

2. **Automated Mode**
   - Research → Planning → Implementation → PR (fully automated)
   - Enabled with `Human validation: false` in trigger comment
   - Uses automated validation only

3. **Custom Configuration**
   - Base branch override: `Base branch: develop`
   - Test mode: `Test mode: true`
   - Stop after phase: `Stop after: research`

## Error Handling

### Validation Failures

Each validation job includes comprehensive error handling:

- Clear error messages explaining what failed
- Specific guidance on how to fix issues
- Automatic retry capability after fixes
- Links to validation documentation

### Common Issues Addressed

1. **Research Validation Failures**
   - Missing required sections
   - Insufficient file references
   - Invalid YAML frontmatter
   - Documents not committed to correct branch

2. **Planning Validation Failures**
   - Missing implementation phases
   - Improperly formatted success criteria
   - Unresolved questions or TODOs
   - Missing YAML frontmatter

3. **Implementation Validation Failures**
   - Test failures
   - Code quality issues
   - Merge conflicts
   - Missing decision record updates

## Security Model

### Cross-Repository Access

- Uses Personal Access Token (PAT) for cross-repository operations
- Repository-specific `PIPELINE_TOKEN` secret required
- Shared workflow cannot directly write to external repositories
- All file operations happen in target repository context

### Permission Requirements

- PAT needs `repo` scope for full repository access
- Workflow needs `issues: write` for comment management
- Repository admin access required for secret configuration

## Testing Framework

### Test Scenarios

1. **full-pipeline-automated**: Complete automated flow test
2. **full-pipeline-manual**: Complete manual validation flow test
3. **research-only**: Research phase validation test
4. **validation-failure**: Error handling test
5. **config-test**: Configuration system test

### Test Automation

- Creates test issues automatically
- Triggers appropriate pipeline configurations
- Monitors progress and validates results
- Cleans up test artifacts
- Provides comprehensive test reporting

## Adoption Process

### For Repository Owners

1. **Copy Template**: Copy `templates/repo-workflow-template.yml` to `.github/workflows/development-pipeline.yml`

2. **Configure Secrets**: Add `PIPELINE_TOKEN` repository secret with PAT

3. **Test Pipeline**: Create test issue and comment `@claude run development pipeline`

4. **Optional Configuration**: Create `.github/development-pipeline-config.yml` for customization

### For Developers

1. **Create Issue**: Standard GitHub issue creation
2. **Trigger Pipeline**: Comment `@claude run development pipeline` on issue
3. **Monitor Progress**: Follow pipeline comments and validation results
4. **Approve Phases**: Comment approval when human validation enabled
5. **Review PR**: Standard code review process for generated PR

## Integration with Phase 1

Phase 2 seamlessly integrates with Phase 1 validation scripts:

- Uses `validate-research.sh` for research document validation
- Uses `validate-plan.sh` for implementation plan validation
- Uses `validate-implementation.sh` for code quality validation
- Uses `validate-pr.sh` for pull request validation

All validation scripts are executed within the GitHub Actions environment with proper error handling and reporting.

## Success Metrics

### Automated Validation

- ✅ YAML syntax validation passes
- ✅ Job dependency graph correctly structured
- ✅ All validation scripts integrate properly
- ✅ Cross-repository access works correctly
- ✅ Decision record management functions

### Manual Testing Required

- Test issue creation and pipeline triggering
- Validation of human approval flow
- Error handling for validation failures
- Multi-repository configuration testing
- End-to-end workflow completion

## Next Steps

Phase 2 is complete and ready for production use. The next phases will build upon this foundation:

- **Phase 3**: Branch safety and context preservation
- **Phase 4**: Multi-repository configuration and testing
- **Phase 5**: Advanced monitoring and analytics
- **Phase 6**: Integration with external tools and services

## Files Created

- `.github/workflows/development-pipeline.yml` - Main shared workflow
- `.github/workflows/development-pipeline-extended.yml` - Extended jobs for reference
- `templates/repo-workflow-template.yml` - Repository adoption template
- `.github/workflows/test-development-pipeline.yml` - Testing framework
- `docs/phase2-implementation-summary.md` - This documentation

Phase 2 provides a robust, scalable foundation for automated development workflows across all Atriumn repositories.

================
File: docs/phase3-branch-safety-context-preservation.md
================
# Phase 3: Branch Safety & Context Preservation

This document describes the enhanced development pipeline features introduced in Phase 3, which provide bulletproof branch tracking, context preservation, error recovery, and partial completion handling.

## Overview

Phase 3 introduces comprehensive safety mechanisms and context preservation to ensure pipeline integrity and handle complex real-world scenarios including:

- **Branch Safety**: Ensures pipeline branches exist and are properly tracked
- **Context Validation**: Validates that each phase has proper context from previous phases
- **Error Recovery**: Provides retry mechanisms and clear recovery guidance
- **Partial Completion Handling**: Detects and manages 75%-25% completion scenarios
- **Decision Record Management**: Keeps decision records manageable while preserving context

## Features

### 1. Enhanced Branch Management

**Bulletproof branch tracking and validation**

The enhanced pipeline validates that:
- Pipeline branches exist and are properly labeled
- Branches haven't been deleted or corrupted
- Branch state is consistent with pipeline expectations
- Commit history is maintained properly

```yaml
validate-branch-continuity:
  # Ensures pipeline branch exists and is valid
  # Checks branch state against base branch
  # Validates branch labels on issues
```

**Key Benefits:**
- Prevents pipeline failures due to missing branches
- Detects manual branch deletions
- Validates branch state consistency
- Provides clear error messages for branch issues

### 2. Context Validation Between Phases

**Ensures each phase has proper context from previous phases**

The pipeline now validates:
- Decision records exist and are properly structured
- Required documents exist for each phase
- Phase progression is logical and complete
- Document integrity is maintained

```yaml
validate-context-continuity:
  # Validates decision record exists and is well-formed
  # Checks for required documents based on current phase
  # Ensures proper phase progression
```

**Phase-Specific Context Requirements:**
- **Research → Planning**: Research document must exist and be validated
- **Planning → Implementation**: Both research and plan documents required
- **Implementation → PR**: All pipeline documents must be present
- **PR Creation**: Complete context validation

### 3. Enhanced Error Recovery

**Robust error handling and retry mechanisms**

The enhanced pipeline provides:
- Automatic failure detection and analysis
- Clear recovery guidance for common failure scenarios
- Retry mechanisms for each phase
- Complete pipeline restart capability

**Recovery Commands:**
```bash
@claude retry research        # Retry research phase
@claude retry planning        # Retry planning phase  
@claude retry implementation  # Retry implementation phase
@claude restart pipeline     # Complete restart
```

**Failure Analysis:**
- Identifies failure type and cause
- Provides specific recovery instructions
- Maintains decision record context during recovery
- Prevents data loss during retry operations

### 4. Partial Completion Handling

**Detects and manages the 75%-25% scenario and similar partial completions**

The pipeline analyzes implementation completion by:
- Examining files mentioned in the implementation plan
- Checking git history for phase-related commits
- Calculating completion percentage
- Providing human decision points

**Completion Detection:**
```yaml
detect-partial-completion:
  # Analyzes implementation plan phases
  # Checks file modifications against plan requirements
  # Calculates completion percentage
  # Identifies completed vs. remaining items
```

**Human Decision Options:**
- **Continue**: `continue implementing remaining items`
- **Accept**: `accept current implementation` 
- **Modify**: `modify implementation: [specific changes]`
- **Cancel**: `cancel implementation`

### 5. Context Size Management

**Keeps decision records manageable while preserving essential context**

The pipeline automatically manages decision record size through:
- **Compression**: Collapsible sections for completed phases
- **Summarization**: Archive detailed sections, keep essentials
- **Archiving**: Move detailed logs to separate files
- **Backup**: Automatic backups before any modifications

**Size Thresholds:**
- **< 150 lines**: No action needed
- **150-200 lines**: Compression applied (collapsible sections)
- **> 200 lines**: Summarization applied (archive details)

## Implementation Guide

### Using the Enhanced Workflow

1. **Copy the enhanced template to your repository:**
   ```bash
   cp templates/enhanced-repo-workflow-template.yml .github/workflows/development-pipeline-enhanced.yml
   ```

2. **Start a pipeline with enhanced features:**
   ```bash
   @claude run development pipeline
   ```

3. **The enhanced pipeline automatically provides:**
   - Branch safety validation
   - Context preservation
   - Error recovery mechanisms
   - Partial completion detection

### Decision Record Structure

Enhanced decision records maintain this structure:

```markdown
# Development Pipeline Decision Record - Issue #123

## Issue Context
- Issue details and metadata
- Pipeline configuration

## Current Status  
- Current phase and state
- Context validation results

## [Phase Name] (Starting/Complete ✅)
- Phase-specific information
- Validation results
- Next steps

## Pipeline Progress
- [✅/❌] Research Phase
- [✅/❌] Planning Phase  
- [✅/❌] Implementation Phase
- [✅/❌] PR Creation
```

### Error Recovery Workflow

When failures occur:

1. **Automatic Detection**: Pipeline detects and analyzes failures
2. **Context Preservation**: Decision record maintains state
3. **Recovery Guidance**: Clear instructions provided to user
4. **Retry Mechanism**: Specific retry commands available
5. **State Restoration**: Clean retry with preserved context

### Partial Completion Workflow

When partial completion is detected:

1. **Analysis**: Pipeline analyzes completion percentage
2. **Human Decision**: User decides how to proceed
3. **Context Update**: Decision recorded in decision record
4. **Continuation**: Claude receives clear instructions for remaining work

## Configuration

### Pipeline Configuration

The enhanced pipeline uses the same configuration structure with additional features:

```yaml
# .github/development-pipeline-config.yml
repo_name: "your-repo"
base_branch: "main"
thoughts_directory: "thoughts/"

validation:
  enabled: true
  strict_mode: true  # Enhanced validation
  
pipeline:
  error_recovery: true
  partial_completion_handling: true
  context_preservation: true
  
decision_record:
  auto_compression: true
  size_threshold: 150
  backup_retention: 30  # days
```

### Branch Safety Configuration

```yaml
branches:
  prefix: "feature/"
  naming: "issue-{number}-{title-slug}"
  auto_cleanup: false  # Keep branches for recovery
  validation: strict
```

## Monitoring and Debugging

### Decision Record Analysis

Use the decision record management script:

```bash
# Analyze decision record size and structure
python scripts/manage-decision-record.py thoughts/shared/decisions/pipeline-issue-123.md --action analyze

# Auto-manage based on size
python scripts/manage-decision-record.py thoughts/shared/decisions/pipeline-issue-123.md --auto

# Manual compression
python scripts/manage-decision-record.py thoughts/shared/decisions/pipeline-issue-123.md --action compress
```

### Pipeline State Inspection

Check pipeline state through GitHub Actions:
- View workflow runs for failure analysis
- Check decision records for context preservation
- Monitor branch state and commits
- Review error recovery attempts

## Best Practices

### For Users

1. **Trust the Recovery System**: Use provided retry commands rather than manual fixes
2. **Read Error Messages**: Enhanced error messages provide specific guidance
3. **Preserve Context**: Don't manually edit decision records
4. **Use Partial Completion Options**: Make informed decisions about partial implementations

### For Repository Maintainers

1. **Configure Thresholds**: Adjust decision record size thresholds based on team preferences
2. **Monitor Decision Records**: Regular cleanup of archived sections
3. **Review Recovery Patterns**: Identify common failure patterns for improvement
4. **Backup Strategy**: Ensure decision record backups are included in repository backups

## Troubleshooting

### Common Issues

**Branch Not Found Error:**
- Cause: Branch was manually deleted or never created
- Solution: Use `@claude restart pipeline` to recreate

**Context Validation Failed:**
- Cause: Decision record missing or corrupted
- Solution: Restore from backup or restart pipeline

**Partial Completion Not Detected:**
- Cause: Implementation plan doesn't match actual files
- Solution: Update plan or use manual completion commands

**Decision Record Too Large:**
- Cause: Long-running pipeline with many updates
- Solution: Automatic compression will be applied

### Recovery Procedures

**Complete Pipeline Recovery:**
```bash
# 1. Clean up current attempt
@claude restart pipeline

# 2. Verify clean state
# Check that branch labels are removed
# Verify no orphaned decision records

# 3. Start fresh
@claude run development pipeline
```

**Phase-Specific Recovery:**
```bash
# Research phase issues
@claude retry research

# Planning phase issues  
@claude retry planning

# Implementation phase issues
@claude retry implementation
```

## Integration with Existing Workflows

The enhanced pipeline is backward compatible with existing Phase 1 and Phase 2 implementations:

- **Validation Scripts**: All existing validation scripts work unchanged
- **Configuration**: Existing configurations are supported
- **Commands**: All existing pipeline commands continue to work
- **Templates**: Original templates remain functional

The enhanced features are additive and can be adopted incrementally.

## Future Enhancements

Phase 3 provides the foundation for future enhancements:

- **AI-Powered Recovery**: Automatic issue resolution
- **Advanced Analytics**: Pipeline performance metrics
- **Team Collaboration**: Multi-user pipeline support
- **Integration APIs**: External tool integration

---

*Phase 3 enhances the development pipeline with enterprise-grade reliability, context preservation, and error recovery capabilities while maintaining simplicity for common use cases.*

================
File: docs/phase3-implementation-summary.md
================
# Phase 3: Branch Safety & Context Preservation - Complete

## Overview

Phase 3 successfully implements comprehensive branch safety and context preservation features that ensure pipeline integrity across all phases. The implementation provides advanced error recovery, partial completion handling, decision record management, and phase transition safety checks.

## Architecture

### Phase 3 Components

```
┌─────────────────────────────────────┐
│     Branch Safety & Context         │
│        Preservation System          │
│                                     │
│  ┌─────────────────────────────────┐ │
│  │    Branch Management            │ │
│  │  • Branch existence validation  │ │
│  │  • Branch state analysis        │ │
│  │  • Commit tracking             │ │
│  │  • Base branch comparison       │ │
│  └─────────────────────────────────┘ │
│                                     │
│  ┌─────────────────────────────────┐ │
│  │    Context Validation           │ │
│  │  • Decision record integrity    │ │
│  │  • Phase prerequisite checking  │ │
│  │  • Document synchronization     │ │
│  │  • Pipeline state validation    │ │
│  └─────────────────────────────────┘ │
│                                     │
│  ┌─────────────────────────────────┐ │
│  │    Error Recovery               │ │
│  │  • Research phase retry         │ │
│  │  • Pipeline restart             │ │
│  │  • Validation failure diagnosis │ │
│  │  • Automated error detection    │ │
│  └─────────────────────────────────┘ │
│                                     │
│  ┌─────────────────────────────────┐ │
│  │    Partial Completion           │ │
│  │  • 75%-25% scenario handling    │ │
│  │  • Progress analysis            │ │
│  │  • Phase continuation           │ │
│  │  • Implementation resumption    │ │
│  └─────────────────────────────────┘ │
│                                     │
│  ┌─────────────────────────────────┐ │
│  │    Phase Transition Safety      │ │
│  │  • Transition validation        │ │
│  │  • Prerequisites checking       │ │
│  │  • Quality gate enforcement     │ │
│  │  • Safe advancement control     │ │
│  └─────────────────────────────────┘ │
└─────────────────────────────────────┘
```

### Key Features

1. **Enhanced Branch Management** - Comprehensive branch validation and state tracking
2. **Context Validation** - Pipeline integrity and document synchronization
3. **Error Recovery** - Automated diagnosis and recovery mechanisms
4. **Partial Completion** - Handle incomplete pipelines gracefully
5. **Decision Record Management** - Size optimization and archiving
6. **Phase Transition Safety** - Quality gates between phases

## Workflow Jobs

### Core Safety Jobs

#### validate-branch-continuity
- **Purpose**: Ensures pipeline branch exists and validates its state
- **Outputs**: branch_name, branch_status, commits_ahead, commits_behind
- **Features**:
  - Branch existence verification from issue labels
  - Repository access validation
  - Commit state analysis vs base branch
  - Automated issue diagnosis and user guidance

#### validate-context-continuity
- **Purpose**: Validates decision record and phase prerequisites
- **Outputs**: decision_record_path, current_phase, context_integrity
- **Features**:
  - Decision record structure validation
  - Phase-specific prerequisite checking
  - Document integrity verification
  - Context synchronization validation

### Error Recovery Jobs

#### retry-research
- **Trigger**: `@claude retry research`
- **Purpose**: Reset and restart research phase
- **Features**:
  - Clean decision record reset
  - Preserve git history
  - Detailed retry instructions
  - Focus on validation fixes

#### restart-pipeline
- **Trigger**: `@claude restart pipeline`
- **Purpose**: Complete pipeline restart with work archival
- **Features**:
  - Archive existing work to dated branch
  - Clean slate initialization
  - Automatic pipeline restart trigger
  - Full work preservation

#### handle-validation-failure
- **Trigger**: `@claude fix validation`
- **Purpose**: Diagnose and guide validation issue resolution
- **Features**:
  - Phase-specific issue detection
  - Automated diagnosis
  - Detailed resolution guidance
  - Multiple recovery options

### Partial Completion Jobs

#### detect-partial-completion
- **Trigger**: `@claude check completion`
- **Purpose**: Analyze pipeline completion status
- **Outputs**: completion_status, completion_percentage, remaining_work
- **Features**:
  - 4-phase completion tracking
  - Percentage calculation (75%-25% scenario)
  - Detailed status reporting
  - Phase-by-phase analysis

#### handle-partial-completion
- **Trigger**: `@claude continue pipeline`
- **Purpose**: Resume pipeline from current phase
- **Features**:
  - Phase-specific continuation instructions
  - Context preservation
  - Proper branch management
  - Tailored guidance per phase

#### continue-partial-implementation
- **Trigger**: `@claude continue implementation`
- **Purpose**: Handle 75%-25% implementation scenario
- **Features**:
  - Implementation progress assessment
  - Commit and file change analysis
  - Detailed continuation instructions
  - Success criteria tracking

### Decision Record Management

#### manage-decision-record-size
- **Trigger**: `@claude optimize decision record`
- **Purpose**: Optimize large decision records
- **Features**:
  - Size threshold detection (50KB/1000 lines)
  - Archive creation with timestamps
  - Essential information preservation
  - Optimization reporting

### Phase Transition Safety

#### validate-phase-transition
- **Trigger**: Phase completion comments (`✅ Research Phase Complete`, etc.)
- **Purpose**: Validate safe phase transitions
- **Outputs**: transition_safe, current_phase, next_phase
- **Features**:
  - Comprehensive prerequisite checking
  - Quality gate enforcement
  - Document validation
  - Safe/unsafe transition handling

#### orchestrate-phase-transition
- **Trigger**: `@claude advance phase`
- **Purpose**: Intelligent phase advancement
- **Features**:
  - Current phase assessment
  - Readiness validation
  - Automatic progression triggers
  - Phase-specific guidance

## Command Reference

### User Commands

| Command | Purpose | Phase |
|---------|---------|-------|
| `@claude retry research` | Restart research phase | Any |
| `@claude restart pipeline` | Complete pipeline restart | Any |
| `@claude fix validation` | Diagnose validation issues | Any |
| `@claude check completion` | Analyze pipeline progress | Any |
| `@claude continue pipeline` | Resume from current phase | Any |
| `@claude continue implementation` | Resume implementation (75%-25%) | Implementation |
| `@claude optimize decision record` | Optimize large decision records | Any |
| `@claude advance phase` | Intelligent phase advancement | Any |

### Automatic Triggers

| Trigger | Job | Purpose |
|---------|-----|---------|
| `✅ Research Phase Complete` | validate-phase-transition | Validate research → planning |
| `✅ Planning Phase Complete` | validate-phase-transition | Validate planning → implementation |
| `✅ Implementation Phase Complete` | validate-phase-transition | Validate implementation → PR |

## Safety Features

### Branch Safety
- **Existence Validation**: Ensures pipeline branch exists before operations
- **State Tracking**: Monitors commits ahead/behind base branch
- **Access Verification**: Validates repository access permissions
- **Issue Diagnosis**: Automated problem detection with user guidance

### Context Preservation
- **Decision Record Integrity**: Validates structure and completeness
- **Phase Prerequisites**: Ensures previous phases are properly completed
- **Document Synchronization**: Verifies decision record reflects current state
- **Quality Gates**: Enforces quality standards at phase transitions

### Error Recovery
- **Graceful Failures**: Clear error messages with actionable guidance
- **Multiple Recovery Paths**: Various options based on failure type
- **Work Preservation**: Archives work before destructive operations
- **Automated Diagnosis**: Intelligent issue detection and recommendations

### Partial Completion Handling
- **Progress Tracking**: Accurate completion percentage calculation
- **Resume Capability**: Continue from any point in pipeline
- **75%-25% Scenario**: Special handling for partially implemented features
- **Context Restoration**: Proper context for continuation

## Integration with Previous Phases

### Phase 1 Integration
- Uses all validation scripts for quality checks
- Leverages configuration system for customization
- Maintains decision record standards

### Phase 2 Integration
- Extends GitHub Actions workflow system
- Integrates with existing job dependency patterns
- Preserves cross-repository security model

## Error Handling Matrix

| Error Type | Detection | Recovery Options | Automation Level |
|------------|-----------|------------------|------------------|
| Missing Branch | Automatic | Restart Pipeline | Full |
| Missing Decision Record | Automatic | Restart Pipeline | Full |
| Validation Failure | Automatic | Retry Phase, Fix Issues | Guided |
| Incomplete Phase | Manual Check | Continue Pipeline | Guided |
| Large Decision Record | Manual/Automatic | Optimize Record | Semi-Automatic |
| Phase Transition Issues | Automatic | Fix Prerequisites | Guided |

## Testing Framework

### Validation Tests
- YAML syntax validation with yq
- Job dependency verification
- Output parameter validation
- Command trigger testing

### Integration Tests
- End-to-end error recovery scenarios
- Partial completion handling
- Phase transition validation
- Decision record optimization

### Manual Testing Scenarios
1. **Error Recovery Testing**
   - Create invalid research document
   - Trigger validation failure
   - Test recovery mechanisms

2. **Partial Completion Testing**
   - Stop pipeline mid-implementation
   - Test continuation commands
   - Verify context preservation

3. **Phase Transition Testing**
   - Test each phase transition
   - Verify safety checks
   - Test invalid transitions

## Configuration Options

### Repository Configuration (`.github/development-pipeline-config.yml`)
```yaml
# Phase 3 specific settings
safety:
  require_phase_validation: true
  decision_record_size_limit: 50000
  transition_quality_gates: true

error_recovery:
  auto_archive_on_restart: true
  preserve_history: true
  
completion_tracking:
  enable_partial_detection: true
  completion_threshold: 75
```

## Success Metrics

### Automated Validation
- ✅ YAML syntax passes validation
- ✅ All 12 jobs properly structured
- ✅ Job dependencies correctly defined
- ✅ Command triggers properly configured
- ✅ Output parameters properly defined

### Manual Testing Required
- Branch safety validation across scenarios
- Error recovery mechanism effectiveness
- Partial completion handling accuracy
- Phase transition safety enforcement
- Decision record optimization functionality

## Command Workflows

### Error Recovery Workflow
```
Issue Detection → Automated Diagnosis → User Notification → Recovery Options → Resolution Guidance
```

### Partial Completion Workflow
```
Progress Check → Completion Analysis → Status Report → Continuation Options → Phase Resumption
```

### Phase Transition Workflow
```
Completion Signal → Prerequisites Check → Quality Gates → Safety Validation → Transition Approval
```

## Files Created

- `.github/workflows/phase3-branch-safety.yml` - Complete Phase 3 implementation (1,655 lines)
- `docs/phase3-implementation-summary.md` - This documentation

## Phase 3 Completion Status

### ✅ Completed Features
- Enhanced branch management and validation
- Context validation between phases  
- Comprehensive error recovery system
- Partial completion handling (75%-25% scenario)
- Decision record size management
- Phase transition safety checks
- Comprehensive command interface
- Automated diagnosis and guidance
- Work preservation and archival
- Quality gate enforcement

### 📊 Implementation Stats
- **Jobs**: 12 specialized workflow jobs
- **Commands**: 8 user-facing commands
- **Triggers**: 3 automatic phase triggers
- **Safety Features**: 6 major safety systems
- **Error Recovery**: 3 recovery mechanisms
- **Lines of Code**: 1,655 lines of workflow YAML

## Next Steps

Phase 3 provides the safety foundation for production pipeline usage. The next phases will build upon this robust safety system:

- **Phase 4**: Multi-repository configuration and testing
- **Phase 5**: Advanced monitoring and analytics  
- **Phase 6**: Integration with external tools and services

## Integration Notes

Phase 3 is designed to be integrated into the main development-pipeline.yml workflow by including the safety jobs as needed. The phase transition validation can be added to the existing workflow to provide safety checks at each phase completion.

Example integration:
```yaml
# In main development-pipeline.yml
validate-research:
  needs: [validate-branch-continuity, validate-context-continuity]
  # ... existing validation logic
```

Phase 3 successfully delivers comprehensive pipeline safety and context preservation, ensuring reliable and recoverable development workflows across all scenarios.

================
File: docs/phase4-implementation-summary.md
================
# Phase 4: Multi-Repository Configuration & Testing - Complete

## Overview

Phase 4 successfully implements a comprehensive multi-repository configuration and testing system that enables the development pipeline to work seamlessly across different repository types with customized validation rules, team configurations, and workflow behaviors. The implementation provides flexible configuration schema, enhanced validation scripts, comprehensive testing framework, and detailed onboarding documentation.

## Architecture

### Multi-Repository Configuration System

```
┌─────────────────────────────────────────────────────────────┐
│                Configuration Management                     │
│                                                             │
│  ┌─────────────────┐    ┌─────────────────┐                │
│  │   Schema        │    │   Validation    │                │
│  │   Definition    │    │   Engine        │                │
│  │                 │    │                 │                │
│  │ • Field types   │    │ • JSON Schema   │                │
│  │ • Constraints   │    │ • Type checking │                │
│  │ • Defaults      │    │ • Pattern match │                │
│  │ • Examples      │    │ • Constraint    │                │
│  └─────────────────┘    │   validation    │                │
│           │              └─────────────────┘                │
│           │                       │                        │
│           ▼                       ▼                        │
│  ┌─────────────────────────────────────────────────────────┐ │
│  │           Repository Configurations                     │ │
│  │                                                         │ │
│  │  Frontend Apps    Backend APIs    Infrastructure       │ │
│  │  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐     │ │
│  │  │ • React     │  │ • REST APIs │  │ • Terraform │     │ │
│  │  │ • Vue       │  │ • GraphQL   │  │ • K8s       │     │ │
│  │  │ • Angular   │  │ • gRPC      │  │ • Ansible   │     │ │
│  │  │             │  │             │  │             │     │ │
│  │  │ Light       │  │ Balanced    │  │ Strict      │     │ │
│  │  │ validation  │  │ validation  │  │ validation  │     │ │
│  │  └─────────────┘  └─────────────┘  └─────────────┘     │ │
│  └─────────────────────────────────────────────────────────┘ │
└─────────────────────────────────────────────────────────────┘
                                │
                                ▼
┌─────────────────────────────────────────────────────────────┐
│                Repository-Specific Features                 │
│                                                             │
│  ┌─────────────────┐    ┌─────────────────┐                │
│  │   Validation    │    │   Team & Workflow│               │
│  │   Scripts       │    │   Configuration  │               │
│  │                 │    │                  │               │
│  │ • Multi-format  │    │ • Team structure │               │
│  │ • JSON & YAML   │    │ • Notifications  │               │
│  │ • Repo-specific │    │ • Phase timeouts │               │
│  │ • Pattern rules │    │ • Parallel limits│               │
│  └─────────────────┘    └─────────────────┘                │
│           │                       │                        │
│           ▼                       ▼                        │
│  ┌─────────────────────────────────────────────────────────┐ │
│  │              Testing Framework                          │ │
│  │                                                         │ │
│  │ • Multi-repo test matrix                                │ │
│  │ • Configuration validation tests                        │ │
│  │ • Cross-repository isolation tests                      │ │
│  │ • Concurrent pipeline limit tests                       │ │
│  │ • Repository-specific customization tests               │ │
│  └─────────────────────────────────────────────────────────┘ │
└─────────────────────────────────────────────────────────────┘
```

## Key Components

### 1. Configuration Schema System

#### Enhanced Schema Definition (`configs/schema.yml`)
- **Comprehensive Field Definitions**: 8 major configuration sections
- **Type Validation**: String, integer, boolean, array, object types
- **Constraint Enforcement**: Min/max values, pattern matching, required fields
- **Default Values**: Sensible defaults for all optional fields
- **Repository Type Examples**: Frontend, backend, infrastructure patterns

#### Configuration Hierarchy
```
1. Repository-specific: .github/development-pipeline-config.yml
2. Alternative naming: .github/dev-pipeline.yml  
3. Shared repository: configs/{repo-name}.yml
4. Global fallback: configs/default.yml
```

### 2. Enhanced Configuration Loading

#### Multi-Source Configuration Loading
- **Priority System**: Repository → shared → default configuration
- **Schema Validation**: Real-time validation against JSON Schema
- **Default Application**: Automatic default value application
- **Error Handling**: Clear validation error messages

#### Configuration Processing Pipeline
```yaml
load-and-validate-config:
  steps:
    - Install dependencies (yq, python, jsonschema)
    - Try multiple config sources in priority order
    - Validate against schema with Python/jsonschema
    - Apply defaults for missing optional fields
    - Extract specialized config sections
    - Make available to all workflow jobs
```

### 3. Repository-Specific Validation

#### Enhanced Validation Scripts
- **Dual Mode Support**: JSON config (from workflow) + file config (legacy)
- **Repository-Specific Rules**: Custom validation per repository type
- **Flexible Requirements**: Configurable sections and reference counts
- **Pattern Validation**: Repository-specific code pattern checks

#### Repository-Specific Validation Examples
```bash
# Platform API specific validations
- API considerations required
- Security implications required  
- Performance impact required

# curatefor.me specific validations
- HLD → humanlayer context required
- Frontend → UX considerations required
```

### 4. Multi-Repository Testing Framework

#### Test Matrix Generation
- **Dynamic Matrix**: Generate test combinations based on inputs
- **Repository Types**: Frontend, backend, infrastructure, etc.
- **Test Scenarios**: Basic validation, full pipeline, config variations
- **Parallel Execution**: Test multiple repositories simultaneously

#### Test Categories
1. **Configuration Tests**: Schema validation, required fields, type checking
2. **Validation Tests**: Repository-specific validation rules
3. **Customization Tests**: Repository-specific feature validation
4. **Isolation Tests**: Cross-repository interference prevention
5. **Concurrency Tests**: Parallel pipeline limit enforcement

### 5. Repository Onboarding System

#### Quick Start Process (5 minutes)
1. **Copy Workflow Template**: Standard GitHub Actions workflow
2. **Create Configuration**: Repository-specific config file
3. **Set Secrets**: PIPELINE_TOKEN for cross-repository access
4. **Test Pipeline**: Verify functionality with test issue

#### Comprehensive Configuration Examples
- **Frontend Applications**: React, Vue, Angular specific settings
- **Backend APIs**: REST, GraphQL, microservice configurations
- **Infrastructure**: Terraform, Kubernetes, compliance-focused setups

## Detailed Implementation

### Configuration Schema Features

#### Field Definitions with Validation
```yaml
field_definitions:
  repo_name:
    type: string
    description: "Repository identifier for configuration lookup"
    example: "curatefor.me"
    
  validation:
    type: object
    properties:
      research_min_refs:
        type: integer
        default: 3
        minimum: 1
        description: "Minimum file references required in research documents"
```

#### Repository Type Templates
```yaml
repository_type_examples:
  frontend_application:
    validation:
      research_min_refs: 3
      implementation_test_commands:
        - "npm test"
        - "npm run lint" 
        - "npm run typecheck"
        - "npm run build"
    workflow_customization:
      phase_timeouts:
        research_hours: 2
        planning_hours: 1
        implementation_hours: 8
```

### Enhanced Workflow Integration

#### Configuration Loading Job
```yaml
load-and-validate-config:
  outputs:
    - config: Full JSON configuration
    - config_source: Source file used
    - validation_config: Validation-specific settings
    - team_config: Team assignment configuration
    - workflow_config: Workflow behavior settings
```

#### Validation Script Integration
```bash
# New JSON mode for workflow integration
./scripts/validate-research.sh "$CONFIG_JSON" "$RESEARCH_DOC"

# Legacy file mode for local use
./scripts/validate-research.sh "config.yml" "$RESEARCH_DOC"
```

### Repository-Specific Customizations

#### curatefor.me Configuration
```yaml
repo_name: "curatefor.me"
base_branch: "develop"
validation:
  research_min_refs: 3
  implementation_test_commands:
    - "make test"
    - "make lint"
    - "make typecheck"
team:
  default_reviewers: ["@jeff-atriumn"]
workflow_customization:
  parallel_pipelines: 3
```

#### platform-api Configuration  
```yaml
repo_name: "platform-api"
base_branch: "main"
validation:
  research_min_refs: 5  # Stricter requirements
  implementation_test_commands:
    - "npm test"
    - "npm run security-scan"
    - "docker build -t test ."
notifications:
  escalation_hours: 1  # Faster escalation
workflow_customization:
  parallel_pipelines: 2  # Limit concurrent work
```

## Testing Framework

### Test Workflow Structure
```yaml
test-multi-repo.yml:
  jobs:
    - setup-test-matrix: Dynamic test matrix generation
    - test-multi-repo-config: Configuration validation tests
    - test-concurrent-pipelines: Concurrency limit tests
    - test-cross-repo-isolation: Isolation verification
    - report-multi-repo-results: Comprehensive reporting
```

### Test Scenarios

#### Basic Validation Test
- Configuration file existence and format
- Schema validation against requirements
- Required field presence verification
- Type and constraint validation

#### Repository-Specific Tests
- Custom validation rule verification
- Repository pattern matching
- Configuration value validation
- Integration setting verification

#### Cross-Repository Isolation
- Branch naming conflict prevention
- Configuration leak prevention
- Validation rule isolation
- Pipeline state separation

## Onboarding Documentation

### Quick Start Guide
- **5-minute setup**: Minimal configuration to get started
- **Copy-paste examples**: Ready-to-use workflow and config files
- **Secret configuration**: PIPELINE_TOKEN setup instructions
- **Test verification**: Simple test procedure

### Detailed Configuration Guide
- **Repository type patterns**: Frontend, backend, infrastructure examples
- **Team configuration**: Small teams vs large teams with specialists
- **Validation customization**: Strict vs relaxed validation examples
- **Advanced features**: Monorepo, multi-environment, compliance configurations

### Troubleshooting Guide
- **Common issues**: Pipeline not triggering, configuration errors, validation failures
- **Debugging tools**: Log analysis, minimal config testing, validation scripts
- **Migration guidance**: From manual processes, from other CI/CD systems
- **Best practices**: Configuration management, team adoption, maintenance

## Validation Tools

### Configuration Validation Script
```bash
# scripts/validate-config.sh
./validate-config.sh .github/development-pipeline-config.yml
```

#### Validation Features
- **YAML syntax checking**: Basic syntax validation
- **Schema validation**: Full schema compliance checking
- **Field value validation**: Type, constraint, and pattern validation
- **Security checking**: Dangerous command detection
- **Summary generation**: Configuration overview and recommendations

## Success Metrics

### Implementation Completeness
- ✅ **Multi-repo configuration system**: Complete schema with validation
- ✅ **Enhanced config loading**: Priority system with error handling
- ✅ **Repository-specific validation**: Custom rules per repo type
- ✅ **Testing framework**: Comprehensive multi-repo test matrix
- ✅ **Onboarding documentation**: Complete guide with examples
- ✅ **Validation tools**: Configuration validation script

### Technical Metrics
- **Configuration Fields**: 25+ configurable fields across 8 sections
- **Repository Types**: 3 major types with specific configurations
- **Test Scenarios**: 5 comprehensive test scenarios
- **Validation Rules**: Repository-specific pattern validation
- **Documentation Pages**: 200+ lines of comprehensive onboarding guide

### Quality Assurance
- **Schema Validation**: JSON Schema compliance checking
- **Type Safety**: Strong typing for all configuration fields
- **Default Handling**: Graceful fallback for missing optional fields
- **Error Messages**: Clear, actionable error reporting
- **Example Configurations**: Working examples for common repository types

## Repository Support Matrix

| Repository Type | Min Refs | Test Commands | Special Features |
|----------------|----------|---------------|------------------|
| Frontend Apps | 3 | npm test, lint, build | UX considerations |
| Backend APIs | 5 | test, security-scan, docker | API/security validation |
| Infrastructure | 8 | terraform, compliance | Strict approval, audit trails |
| Internal Tools | 2 | Basic testing | Relaxed validation |
| Monorepos | 5 | Multi-service testing | Service-specific patterns |

## Integration Points

### GitHub Actions Integration
- **Workflow Templates**: Ready-to-use workflow files
- **Secret Management**: PIPELINE_TOKEN configuration
- **Matrix Testing**: Dynamic test generation
- **Cross-Repository**: Secure inter-repo communication

### Development Workflow Integration
- **Issue Comments**: Pipeline trigger integration
- **Branch Management**: Repository-specific branch naming
- **Team Notifications**: Slack, email integration
- **Review Assignment**: Automatic reviewer assignment

## Security Model

### Repository Isolation
- **Configuration Isolation**: No config leakage between repositories
- **Validation Isolation**: Repository-specific validation rules
- **Branch Isolation**: Repository-specific branch naming patterns
- **Token Scope**: Minimal required permissions

### Access Control
- **Repository Secrets**: PIPELINE_TOKEN per repository
- **Cross-Repository Access**: API-based, no direct file access
- **Configuration Validation**: Schema-enforced constraints
- **Team Assignment**: Repository-specific reviewer assignments

## Migration Strategy

### Existing Repository Migration
1. **Assessment**: Current workflow analysis
2. **Configuration**: Create repository-specific config
3. **Testing**: Validate with test issues
4. **Gradual Rollout**: Team-by-team adoption
5. **Full Migration**: Complete workflow replacement

### Legacy System Integration
- **Parallel Operation**: Keep existing CI/CD for builds
- **Gradual Migration**: Workflow-first, then validation rules
- **Integration Points**: Deployment trigger integration
- **Rollback Plan**: Easy reversion if needed

## Monitoring and Analytics

### Configuration Analytics
- **Adoption Metrics**: Repository onboarding rate
- **Configuration Patterns**: Most common customizations
- **Validation Effectiveness**: Rule success/failure rates
- **Performance Metrics**: Pipeline execution times

### Usage Patterns
- **Repository Types**: Frontend vs backend vs infrastructure usage
- **Team Configurations**: Small vs large team patterns
- **Validation Strictness**: Strict vs relaxed validation adoption
- **Feature Usage**: Most/least used configuration features

## Files Created

### Core Implementation
- `configs/schema.yml` - Enhanced comprehensive configuration schema
- `configs/curatefor.me.yml` - Example frontend application configuration
- `configs/platform-api.yml` - Example backend API configuration
- `.github/workflows/development-pipeline.yml` - Enhanced with multi-repo config loading

### Validation and Testing
- `scripts/validate-research.sh` - Enhanced with multi-repo support
- `scripts/validate-config.sh` - New configuration validation tool
- `.github/workflows/test-multi-repo.yml` - Multi-repository testing framework

### Documentation
- `docs/repository-onboarding.md` - Enhanced comprehensive onboarding guide
- `docs/phase4-implementation-summary.md` - This documentation

## Phase 4 Completion Status

### ✅ All Tasks Completed
1. **Multi-repo configuration system**: Complete schema with validation
2. **Enhanced configuration loading**: Priority system with error handling  
3. **Repository-specific validation**: Custom validation per repository type
4. **Multi-repo testing framework**: Comprehensive test matrix and scenarios
5. **Repository onboarding documentation**: Complete guide with examples

### 📊 Implementation Statistics
- **Lines of Code**: 2,000+ lines across all components
- **Configuration Options**: 25+ configurable fields
- **Repository Types**: 3 major types with specific patterns
- **Test Cases**: 5 comprehensive test scenarios
- **Documentation**: 400+ lines of onboarding guidance

## Next Steps

Phase 4 completes the core multi-repository pipeline system. Future phases can build upon this foundation:

- **Phase 5**: Advanced monitoring, analytics, and performance optimization
- **Phase 6**: External tool integrations (Linear, Jira, Slack webhooks)
- **Phase 7**: Advanced deployment pipeline integration
- **Phase 8**: Machine learning for pipeline optimization

## Production Readiness

Phase 4 delivers a production-ready multi-repository development pipeline system with:
- **Comprehensive configuration management**
- **Repository-specific customization**
- **Robust validation and testing**
- **Clear onboarding process**
- **Security and isolation**

The system is ready for organization-wide deployment across diverse repository types and team structures.

================
File: docs/repository-onboarding.md
================
# Repository Onboarding Guide

This guide walks through adding a new repository to the shared development pipeline system.

## Quick Start (5 minutes)

### 1. Add Shared Workflow
Create `.github/workflows/development-pipeline.yml` in your repository:

```yaml
name: Development Pipeline

on:
  issue_comment:
    types: [created]

jobs:
  check-trigger:
    if: |
      contains(github.event.comment.body, '@atriumn-pipeline start') ||
      contains(github.event.comment.body, '✅ Research Phase Complete') ||
      contains(github.event.comment.body, 'approve research') ||
      contains(github.event.comment.body, '✅ Planning Phase Complete') ||
      contains(github.event.comment.body, 'approve plan') ||
      contains(github.event.comment.body, '✅ Implementation Phase Complete') ||
      contains(github.event.comment.body, 'approve implementation')
    runs-on: ubuntu-latest
    steps:
      - run: echo "Triggering development pipeline"

  development-pipeline:
    needs: check-trigger
    uses: atriumn/atriumn-shared-workflows/.github/workflows/development-pipeline.yml@main
    with:
      repo_name: ${{ github.repository }}
      issue_number: ${{ github.event.issue.number }}
    secrets:
      REPO_TOKEN: ${{ secrets.PIPELINE_TOKEN }}
```

### 2. Add Repository Configuration
Create `.github/development-pipeline-config.yml`:

```yaml
repo_name: "your-repo-name"
base_branch: "main"  # or "develop"
thoughts_directory: "thoughts/"  # or "docs/" etc.

validation:
  research_min_refs: 3
  implementation_test_commands:
    - "make test"
    - "make lint"

team:
  default_reviewers: ["@your-username"]
  tech_lead: "@tech-lead"

notifications:
  slack_channel: "#your-team"
```

### 3. Test the Pipeline
1. Create a test issue
2. Comment: `@atriumn-pipeline start`
3. Verify the pipeline starts correctly

## Detailed Configuration

### Repository Types and Recommended Settings

#### Frontend Applications
```yaml
repo_name: "frontend-app"
base_branch: "main"
thoughts_directory: "docs/"

validation:
  research_min_refs: 3
  implementation_test_commands:
    - "npm test"
    - "npm run lint"
    - "npm run typecheck"
    - "npm run build"

workflow_customization:
  phase_timeouts:
    research_hours: 2
    planning_hours: 1
    implementation_hours: 8
```

#### Backend APIs
```yaml
repo_name: "api-service"
base_branch: "develop"
thoughts_directory: "thoughts/"

validation:
  research_min_refs: 5
  implementation_test_commands:
    - "make test"
    - "make lint"
    - "make security-scan"
    - "docker build -t test ."
  pr_required_sections:
    - "API Changes"
    - "Database Migrations"
    - "Security Review"

workflow_customization:
  phase_timeouts:
    research_hours: 4
    planning_hours: 2
    implementation_hours: 16
```

#### Infrastructure/Platform
```yaml
repo_name: "platform-infrastructure"
base_branch: "main"
thoughts_directory: "docs/decisions/"

validation:
  research_min_refs: 8
  implementation_test_commands:
    - "terraform plan"
    - "terraform validate"
    - "make security-scan"
    - "make compliance-check"

notifications:
  escalation_hours: 1  # Faster escalation for critical infrastructure

workflow_customization:
  auto_proceed_default: false  # Always require human approval
  parallel_pipelines: 1  # Limit concurrent infrastructure changes
```

### Team Configuration

#### Small Team
```yaml
team:
  default_reviewers: ["@team-lead"]
  tech_lead: "@team-lead"
```

#### Large Team with Specialists
```yaml
team:
  default_reviewers: ["@senior-dev1", "@senior-dev2"]
  domain_experts:
    frontend: "@frontend-specialist"
    backend: "@backend-specialist"
    infrastructure: "@devops-lead"
    security: "@security-engineer"
    design: "@ux-designer"
  tech_lead: "@architecture-lead"
```

### Validation Customization

#### Strict Validation (Critical Systems)
```yaml
validation:
  research_min_refs: 8
  implementation_test_commands:
    - "make test-unit"
    - "make test-integration"
    - "make test-e2e"
    - "make security-scan"
    - "make performance-test"
    - "make compliance-check"
  pr_required_sections:
    - "Security Impact"
    - "Performance Impact"
    - "Rollback Plan"
    - "Monitoring Plan"
```

#### Relaxed Validation (Internal Tools)
```yaml
validation:
  research_min_refs: 2
  implementation_test_commands:
    - "npm test"
  pr_required_sections:
    - "Summary"
    - "Testing"
```

## Testing Your Configuration

### 1. Validation Script Test
```bash
# Test your config locally
curl -O https://raw.githubusercontent.com/atriumn/atriumn-shared-workflows/main/scripts/validate-config.py
chmod +x validate-config.py
python3 validate-config.py .github/development-pipeline-config.yml --report
```

### 2. Dry Run Test
```bash
# Test pipeline without actually running it
gh workflow run multi-repo-test.yml \
  -f test_repos="your-repo-name" \
  -f test_scenarios="basic-validation"
```

### 3. Full Pipeline Test
1. Create test issue: "Test: Pipeline onboarding"
2. Comment: `@atriumn-pipeline start`
3. Monitor progress and validate each phase
4. Clean up test artifacts

## Common Configuration Patterns

### Monorepo Support
```yaml
file_patterns:
  research: "docs/research/"
  plans: "docs/plans/"
  decisions: "docs/decisions/"

validation:
  implementation_test_commands:
    - "make test-service-a"
    - "make test-service-b"
    - "make test-integration"
```

### Multi-Environment Deployment
```yaml
integration_settings:
  deployment_environments: ["dev", "staging", "prod-us", "prod-eu"]

workflow_customization:
  phase_timeouts:
    implementation_hours: 48  # More time for multi-env testing
```

### Compliance Requirements
```yaml
validation:
  pr_required_sections:
    - "Compliance Impact"
    - "Data Privacy Review"
    - "Security Assessment"
    - "Change Control Reference"

notifications:
  email_list: ["compliance@company.com"]
```

## Advanced Configuration

### Custom Validation Rules
```yaml
validation:
  custom_rules:
    - name: "Database Migration Check"
      pattern: "migration|schema|database"
      required_sections: ["Database Impact", "Rollback Plan"]
    - name: "API Breaking Change Check"
      pattern: "breaking.*change|api.*version"
      additional_reviewers: ["@api-team-lead"]
```

### Integration Settings
```yaml
integration_settings:
  linear_workspace: "ACME-123"
  jira_project: "PROJ"
  slack_webhook: "https://hooks.slack.com/..."
  deployment_environments: ["staging", "production"]
  
  # Custom webhook endpoints
  webhooks:
    pipeline_start: "https://api.company.com/webhooks/pipeline-start"
    phase_complete: "https://api.company.com/webhooks/phase-complete"
```

### Branch Strategy Configuration
```yaml
branches:
  prefix: "feature/"
  naming: "issue-{number}-{title-slug}"
  cleanup_merged: true
  
  # Advanced branch rules
  protection_rules:
    require_pr_reviews: true
    required_reviewers: 2
    dismiss_stale_reviews: true
    require_code_owner_reviews: true
```

## Troubleshooting

### Common Issues

#### Pipeline Not Triggering
- Check workflow file is in `.github/workflows/`
- Verify issue comment patterns match exactly
- Check repository has necessary permissions

#### Configuration Validation Errors
- Use schema validation: `python3 validate-config.py your-config.yml`
- Check YAML syntax: `yq eval '.' your-config.yml`
- Verify all required fields are present

#### Validation Scripts Failing
- Check test commands work locally
- Verify file paths in configuration
- Test with minimal configuration first

### Getting Help

1. **Check the logs**: GitHub Actions logs show detailed error messages
2. **Test minimal config**: Start with minimal configuration and add complexity
3. **Review examples**: Check working configurations in `configs/` directory
4. **Ask for help**: Create issue in atriumn-shared-workflows repository

## Migration from Existing Systems

### From Manual Processes
1. Document current workflow steps
2. Map steps to pipeline phases (research → planning → implementation → PR)
3. Configure validation rules to match current quality gates
4. Test with non-critical changes first

### From Other CI/CD Systems
1. Keep existing CI/CD for builds and deployments
2. Use pipeline for development workflow only
3. Gradually migrate validation rules
4. Consider integration points for deployment triggers

## Best Practices

### Configuration Management
- Start with default configuration and customize gradually
- Use version control for configuration changes
- Test configuration changes in non-production repositories first
- Document repository-specific validation rules

### Team Adoption
- Start with volunteer early adopters
- Provide training on pipeline commands and workflow
- Monitor pipeline usage and iterate based on feedback
- Create team-specific documentation and examples

### Maintenance
- Regularly review and update configuration
- Monitor pipeline performance and timeout settings
- Keep validation rules up to date with technology changes
- Archive completed pipeline artifacts periodically

## Configuration Examples by Use Case

### High-Security Environment
```yaml
repo_name: "secure-app"
base_branch: "main"
thoughts_directory: "security-docs/"

validation:
  research_min_refs: 10
  implementation_test_commands:
    - "make security-scan"
    - "make vulnerability-test"
    - "make compliance-check"
    - "make penetration-test"
  pr_required_sections:
    - "Security Impact Analysis"
    - "Threat Model Review"
    - "Compliance Verification"
    - "Security Testing Results"

notifications:
  escalation_hours: 0.5  # 30 minutes
  email_list: ["security-team@company.com", "compliance@company.com"]

workflow_customization:
  auto_proceed_default: false
  parallel_pipelines: 1
  phase_timeouts:
    research_hours: 16
    planning_hours: 8
    implementation_hours: 72

team:
  default_reviewers: ["@security-lead", "@compliance-officer"]
  domain_experts:
    security: "@security-architect"
    compliance: "@compliance-lead"
```

### Rapid Development Environment
```yaml
repo_name: "prototype-app"
base_branch: "develop"
thoughts_directory: "docs/"

validation:
  research_min_refs: 2
  implementation_test_commands:
    - "npm test"
  pr_required_sections:
    - "Summary"
    - "Testing"

workflow_customization:
  auto_proceed_default: true  # Skip human validation for speed
  parallel_pipelines: 5
  phase_timeouts:
    research_hours: 1
    planning_hours: 1
    implementation_hours: 4

team:
  default_reviewers: ["@dev-lead"]
```

### Open Source Project
```yaml
repo_name: "open-source-lib"
base_branch: "main"
thoughts_directory: "docs/development/"

validation:
  research_min_refs: 4
  implementation_test_commands:
    - "npm test"
    - "npm run lint"
    - "npm run docs"
    - "npm run compatibility-test"
  pr_required_sections:
    - "Summary"
    - "Breaking Changes"
    - "Documentation Updates"
    - "Backward Compatibility"

notifications:
  slack_channel: "#contributors"

workflow_customization:
  parallel_pipelines: 10  # Many contributors
  phase_timeouts:
    research_hours: 8
    planning_hours: 4
    implementation_hours: 24

team:
  default_reviewers: ["@maintainer1", "@maintainer2"]
  domain_experts:
    architecture: "@core-maintainer"
    documentation: "@docs-maintainer"
```

---

*This guide provides comprehensive instructions for onboarding repositories to the multi-repository development pipeline system. For additional support, refer to the main documentation or create an issue in the shared workflows repository.*

================
File: github-app/app.js
================
require('dotenv').config();
const { App } = require('@octokit/app');
const { createNodeMiddleware } = require('@octokit/webhooks');
// GitHub App configuration
const app = new App({
  appId: process.env.GITHUB_APP_ID,
  privateKey: process.env.GITHUB_PRIVATE_KEY,
  webhooks: {
    secret: process.env.GITHUB_WEBHOOK_SECRET
  }
});
// New task pack system triggers
const TASK_PACK_TRIGGERS = {
  '/atriumn-research': 'research',
  '/atriumn-plan': 'plan', 
  '/atriumn-implement': 'implement',
  '/atriumn-validate': 'validate'
};
// Approval triggers (handled by phase-approvals.yml)
const APPROVAL_TRIGGERS = {
  '/atriumn-approve-research': 'approve-research',
  '/atriumn-revise-research': 'revise-research',
  '/atriumn-approve-plan': 'approve-plan', 
  '/atriumn-revise-plan': 'revise-plan',
  '/atriumn-approve-implement': 'approve-implement',
  '/atriumn-revise-implement': 'revise-implement',
  '/atriumn-approve-validate': 'approve-validate',
  '/atriumn-revise-validate': 'revise-validate'
};
// GitHub App now dispatches directly to task pack workflows
// No static workflow template needed - repos use their own claude.yml dispatcher
// Function to create PR after workflow creates commits
async function createPRWhenReady(octokit, repo, issue, featureRef, maxRetries = 6) {
  const prTitle = `Feature: Issue #${issue.number} – multi-phase development`;
  const prBody = `Tracking PR for Issue #${issue.number}.
## Development Phases:
- [ ] Research
- [ ] Plan  
- [ ] Implement
- [ ] Validate
**Issue:** ${issue.title}
**Branch:** \`${featureRef}\`
This PR will be updated automatically as each phase completes.`;
  for (let attempt = 1; attempt <= maxRetries; attempt++) {
    try {
      // Check if PR already exists
      const existingPRs = await octokit.request('GET /repos/{owner}/{repo}/pulls', {
        owner: repo.owner.login,
        repo: repo.name,
        head: `${repo.owner.login}:${featureRef}`,
        state: 'open'
      });
      if (existingPRs.data.length > 0) {
        console.log(`Draft PR already exists for ${featureRef}: #${existingPRs.data[0].number}`);
        return;
      }
      // Try to create PR
      const newPR = await octokit.request('POST /repos/{owner}/{repo}/pulls', {
        owner: repo.owner.login,
        repo: repo.name,
        title: prTitle,
        head: featureRef,
        base: 'develop',
        body: prBody,
        draft: true
      });
      console.log(`Created draft PR #${newPR.data.number} for ${featureRef}`);
      return;
    } catch (error) {
      if (error.message.includes('No commits between') && attempt < maxRetries) {
        console.log(`Attempt ${attempt}: No commits yet, waiting 30s before retry...`);
        await new Promise(resolve => setTimeout(resolve, 30000)); // Wait 30 seconds
        continue;
      }
      throw error; // Re-throw if it's not a "no commits" error or we've exhausted retries
    }
  }
  throw new Error(`Failed to create PR after ${maxRetries} attempts - workflow may not have created commits`);
}
// Auto-setup workflow when app is installed
app.webhooks.on('installation.created', async ({ payload }) => {
  console.log('App installed on repositories:', payload.repositories?.map(r => r.full_name));
  const installationId = payload.installation.id;
  const octokit = await app.getInstallationOctokit(installationId);
  // Setup workflow for each repository
  for (const repo of payload.repositories || []) {
    try {
      await setupWorkflow(octokit, repo.owner.login, repo.name);
      console.log(`✅ Workflow setup complete for ${repo.full_name}`);
    } catch (error) {
      console.error(`❌ Failed to setup workflow for ${repo.full_name}:`, error.message);
    }
  }
});
// Auto-setup workflow when app is installed on additional repositories
app.webhooks.on('installation_repositories.added', async ({ payload }) => {
  console.log('App installed on additional repositories:', payload.repositories_added?.map(r => r.full_name));
  const installationId = payload.installation.id;
  const octokit = await app.getInstallationOctokit(installationId);
  // Setup workflow for each new repository
  for (const repo of payload.repositories_added || []) {
    try {
      await setupWorkflow(octokit, repo.owner.login, repo.name);
      console.log(`✅ Workflow setup complete for ${repo.full_name}`);
    } catch (error) {
      console.error(`❌ Failed to setup workflow for ${repo.full_name}:`, error.message);
    }
  }
});
// Setup workflow in a repository
async function setupWorkflow(octokit, owner, repo) {
  const workflowPath = '.github/workflows/development-pipeline.yml';
  try {
    // Check if workflow already exists
    await octokit.request('GET /repos/{owner}/{repo}/contents/{path}', {
      owner,
      repo,
      path: workflowPath
    });
    console.log(`Workflow already exists in ${owner}/${repo}`);
    return;
  } catch (error) {
    // File doesn't exist, create it
    if (error.status !== 404) {
      throw error;
    }
  }
  // Create the workflow file
  await octokit.request('PUT /repos/{owner}/{repo}/contents/{path}', {
    owner,
    repo,
    path: workflowPath,
    message: 'Add Atriumn Issue-Driven Development Pipeline',
    content: Buffer.from(WORKFLOW_TEMPLATE).toString('base64'),
    committer: {
      name: 'Atriumn Bot',
      email: 'bot@atriumn.com'
    }
  });
  console.log(`Created workflow file in ${owner}/${repo}`);
}
// Test version that creates the full workflow structure
async function setupWorkflowTest(octokit, owner, repo) {
  const workflowPath = '.github/workflows/atriumn-pipeline-test.yml';
  // First, try to create the .github directory
  try {
    await octokit.request('PUT /repos/{owner}/{repo}/contents/{path}', {
      owner,
      repo,
      path: '.github/.gitkeep',
      message: 'Create .github directory',
      content: Buffer.from('').toString('base64'),
      committer: {
        name: 'Atriumn Bot',
        email: 'bot@atriumn.com'
      }
    });
    console.log(`Created .github directory in ${owner}/${repo}`);
  } catch (error) {
    if (error.status !== 422) { // 422 means file already exists
      console.log(`Directory creation failed: ${error.message}`);
    }
  }
  // Then try to create the workflows directory
  try {
    await octokit.request('PUT /repos/{owner}/{repo}/contents/{path}', {
      owner,
      repo,
      path: '.github/workflows/.gitkeep', 
      message: 'Create workflows directory',
      content: Buffer.from('').toString('base64'),
      committer: {
        name: 'Atriumn Bot',
        email: 'bot@atriumn.com'
      }
    });
    console.log(`Created workflows directory in ${owner}/${repo}`);
  } catch (error) {
    if (error.status !== 422) {
      console.log(`Workflows directory creation failed: ${error.message}`);
    }
  }
  try {
    // Check if workflow already exists
    await octokit.request('GET /repos/{owner}/{repo}/contents/{path}', {
      owner,
      repo,
      path: workflowPath
    });
    console.log(`Workflow already exists in ${owner}/${repo}`);
    return;
  } catch (error) {
    // File doesn't exist, create it
    if (error.status !== 404) {
      throw error;
    }
  }
  // Create the workflow file
  await octokit.request('PUT /repos/{owner}/{repo}/contents/{path}', {
    owner,
    repo,
    path: workflowPath,
    message: 'Add Atriumn Issue-Driven Development Pipeline',
    content: Buffer.from(WORKFLOW_TEMPLATE).toString('base64'),
    committer: {
      name: 'Atriumn Bot',
      email: 'bot@atriumn.com'
    }
  });
  console.log(`Created workflow file in ${owner}/${repo}`);
}
// Handle issue comments
app.webhooks.on('issue_comment.created', async ({ payload }) => {
  const comment = payload.comment.body;
  const repo = payload.repository;
  const issue = payload.issue;
  const commentUser = payload.comment.user.login;
  // Ignore comments from bot accounts or automated systems
  if (commentUser === 'github-actions[bot]' || commentUser === 'github-actions' || commentUser.includes('[bot]') || commentUser === 'atriumn-bot') {
    console.log(`Ignoring comment from bot user: ${commentUser}`);
    return;
  }
  console.log(`Processing comment from ${commentUser}: "${comment}"`);
  // Get installation octokit instance
  const installationId = payload.installation?.id;
  if (!installationId) {
    console.error('No installation ID found in payload');
    return;
  }
  const octokit = await app.getInstallationOctokit(installationId);
  // Check for task pack triggers (e.g., "/research description")
  for (const [trigger, taskPackId] of Object.entries(TASK_PACK_TRIGGERS)) {
    const pattern = new RegExp(`^\\s*${trigger.replace('/', '\\/')}(?:\\s+(.+))?\\s*$`, 'm');
    const match = comment.match(pattern);
    if (match) {
      // Use provided description or generate from issue context
      const taskDescription = match[1] || `${taskPackId.charAt(0).toUpperCase() + taskPackId.slice(1)} phase for: "${issue.title}"
Context from issue description:
${issue.body ? issue.body.substring(0, 500) + (issue.body.length > 500 ? '...' : '') : 'No additional context provided'}`;
      console.log(`Task pack trigger matched: ${trigger} -> ${taskPackId}`);
      console.log(`Task description: ${taskDescription}`);
      try {
        const featureRef = `feature/issue-${issue.number}`;
        // Create feature branch from develop if it doesn't exist
        try {
          // Check if branch exists
          await octokit.request('GET /repos/{owner}/{repo}/git/ref/{ref}', {
            owner: repo.owner.login,
            repo: repo.name,
            ref: `heads/${featureRef}`
          });
          console.log(`Branch ${featureRef} already exists`);
        } catch (error) {
          if (error.status === 404) {
            // Branch doesn't exist, create it
            console.log(`Creating branch ${featureRef} from develop`);
            // Get develop branch SHA
            const developRef = await octokit.request('GET /repos/{owner}/{repo}/git/ref/{ref}', {
              owner: repo.owner.login,
              repo: repo.name,
              ref: 'heads/develop'
            });
            // Create new branch
            await octokit.request('POST /repos/{owner}/{repo}/git/refs', {
              owner: repo.owner.login,
              repo: repo.name,
              ref: `refs/heads/${featureRef}`,
              sha: developRef.data.object.sha
            });
            console.log(`Successfully created branch ${featureRef}`);
          } else {
            throw error;
          }
        }
        // Post comment back to issue to trigger issue_comment workflow
        await octokit.request('POST /repos/{owner}/{repo}/issues/{issue_number}/comments', {
          owner: repo.owner.login,
          repo: repo.name,
          issue_number: issue.number,
          body: trigger // Post the same trigger comment that will be picked up by workflow
        });
        console.log(`Successfully posted ${trigger} comment for issue #${issue.number}`);
        // Create PR after workflow creates commits (async, don't wait)
        createPRWhenReady(octokit, repo, issue, featureRef).catch(err => 
          console.error(`Failed to create PR for ${featureRef}:`, err.message)
        );
        // Comment on issue to acknowledge
        await octokit.request('POST /repos/{owner}/{repo}/issues/{issue_number}/comments', {
          owner: repo.owner.login,
          repo: repo.name,
          issue_number: issue.number,
          body: `🤖 **${taskPackId.charAt(0).toUpperCase() + taskPackId.slice(1)} task started**\n\nTriggered by: ${trigger}\nBranch: \`feature/issue-${issue.number}\`\n\nWatch progress in [Actions](https://github.com/${repo.owner.login}/${repo.name}/actions)`
        });
        return; // Only trigger once per comment
      } catch (error) {
        console.error(`Failed to dispatch ${taskPackId} task pack:`, error);
        // Comment on issue about error
        await octokit.request('POST /repos/{owner}/{repo}/issues/{issue_number}/comments', {
          owner: repo.owner.login,
          repo: repo.name,
          issue_number: issue.number,
          body: `❌ **Failed to start ${taskPackId} task**\n\nError: ${error.message}\n\nPlease check the GitHub App configuration.`
        });
      }
    }
  }
  // Check for approval triggers (same dispatch pattern as tasks)
  for (const [trigger, approvalType] of Object.entries(APPROVAL_TRIGGERS)) {
    const pattern = new RegExp(`^\\s*${trigger.replace('/', '\\/')}(?:\\s+(.+))?\\s*$`, 'm');
    const match = comment.match(pattern);
    if (match) {
      // Extract phase from approval type (approve-research -> research)
      const phase = approvalType.replace('approve-', '').replace('revise-', '');
      console.log(`Approval trigger matched: ${trigger} -> approve ${phase}`);
      try {
        // Post comment back to issue to trigger issue_comment workflow  
        await octokit.request('POST /repos/{owner}/{repo}/issues/{issue_number}/comments', {
          owner: repo.owner.login,
          repo: repo.name,
          issue_number: issue.number,
          body: trigger // Post the same trigger comment that will be picked up by workflow
        });
        console.log(`Successfully posted ${trigger} comment for issue #${issue.number}`);
        // Comment on issue to acknowledge
        await octokit.request('POST /repos/{owner}/{repo}/issues/{issue_number}/comments', {
          owner: repo.owner.login,
          repo: repo.name,
          issue_number: issue.number,
          body: `✅ **${phase.charAt(0).toUpperCase() + phase.slice(1)} Approved**\n\nApproved by: @${commentUser}\nProcessing approval...`
        });
        return;
      } catch (error) {
        console.error(`Failed to dispatch ${phase} approval:`, error);
        await octokit.request('POST /repos/{owner}/{repo}/issues/{issue_number}/comments', {
          owner: repo.owner.login,
          repo: repo.name,
          issue_number: issue.number,
          body: `❌ **Failed to process approval**\n\nError: ${error.message}`
        });
      }
    }
  }
});
// Health check endpoint
app.webhooks.on('ping', async ({ payload }) => {
  console.log('Webhook ping received:', payload.zen);
});
// Error handling
app.webhooks.onError((error) => {
  console.error('Webhook error:', error);
});
// Export for deployment
module.exports = app;
// Local development server
if (require.main === module) {
  const port = process.env.PORT || 3000;
  const middleware = createNodeMiddleware(app.webhooks, { path: '/api/webhook' });
  // Add test endpoint for manual workflow creation
  const server = require('http').createServer(async (req, res) => {
    if (req.url === '/test-setup' && req.method === 'POST') {
      try {
        const installationId = 81630447; // Your installation ID
        const octokit = await app.getInstallationOctokit(installationId);
        // Test PR creation directly - try without draft first
        const testPR = await octokit.request('POST /repos/{owner}/{repo}/pulls', {
          owner: 'atriumn',
          repo: 'curatefor.me',
          title: 'Test PR from GitHub App',
          head: 'atriumn:feature/issue-97',
          base: 'develop',
          body: 'Testing GitHub App PR creation'
        });
        res.writeHead(200, { 'Content-Type': 'application/json' });
        res.end(JSON.stringify({ success: true, message: 'PR created', pr: testPR.data.number }));
      } catch (error) {
        console.error('Test setup error:', error);
        res.writeHead(500, { 'Content-Type': 'application/json' });
        res.end(JSON.stringify({ success: false, error: error.message, status: error.status }));
      }
    } else {
      // Pass to webhook middleware
      middleware(req, res);
    }
  });
  server.listen(port, () => {
    console.log(`Atriumn Issue-Driven Development app listening on port ${port}`);
    console.log('Configured task pack triggers:', Object.keys(TASK_PACK_TRIGGERS));
    console.log('Configured approval triggers:', Object.keys(APPROVAL_TRIGGERS));
    console.log('Test endpoint: POST http://localhost:' + port + '/test-setup');
  });
}

================
File: github-app/app.yml
================
# GitHub App manifest for Atriumn Issue-Driven Development
name: Atriumn Issue-Driven Development
description: AI-powered development pipeline that turns GitHub issues into working code
url: https://github.com/atriumn/atriumn-issue-driven-development
hook_attributes:
  url: https://90a45654962b.ngrok-free.app/api/webhook
redirect_url: https://github.com/atriumn/atriumn-issue-driven-development
# Permissions required by the app
default_permissions:
  issues: read
  contents: read
  actions: write
  metadata: read
# Events the app subscribes to
default_events:
  - issue_comment
# Public app settings
public: false
request_oauth_on_install: false

================
File: github-app/create-app.html
================
<!DOCTYPE html>
<html>
<head>
    <title>Create Atriumn GitHub App</title>
</head>
<body>
    <h1>Create Atriumn Issue-Driven Development App</h1>
    <form action="https://github.com/settings/apps/new" method="post">
        <input type="hidden" name="manifest" value='{
            "name": "Atriumn Issue-Driven Development",
            "description": "AI-powered development pipeline that turns GitHub issues into working code",
            "url": "https://github.com/atriumn/atriumn-issue-driven-development",
            "hook_attributes": {
                "url": "https://90a45654962b.ngrok-free.app/api/webhook"
            },
            "redirect_url": "https://github.com/atriumn/atriumn-issue-driven-development",
            "default_permissions": {
                "issues": "read",
                "contents": "read", 
                "actions": "write",
                "metadata": "read"
            },
            "default_events": ["issue_comment"],
            "public": false,
            "request_oauth_on_install": false
        }'>
        <input type="submit" value="Create GitHub App">
    </form>
</body>
</html>

================
File: github-app/package.json
================
{
  "name": "atriumn-issue-driven-development",
  "version": "1.0.0",
  "description": "GitHub App for Atriumn Issue-Driven Development pipeline triggers",
  "main": "app.js",
  "scripts": {
    "start": "node app.js",
    "dev": "nodemon app.js",
    "test": "node test.js"
  },
  "keywords": [
    "github-app",
    "atriumn",
    "issue-driven-development",
    "ai",
    "automation",
    "claude"
  ],
  "author": "Atriumn",
  "license": "MIT",
  "dependencies": {
    "@octokit/app": "^14.0.0",
    "@octokit/webhooks": "^12.0.0",
    "dotenv": "^17.2.1"
  },
  "devDependencies": {
    "nodemon": "^3.0.0"
  },
  "engines": {
    "node": ">=18.0.0"
  }
}

================
File: github-app/README.md
================
# Atriumn Issue-Driven Development GitHub App

This GitHub App efficiently routes issue comments to trigger development pipeline workflows, eliminating the wasteful "skipped runs" from traditional `issue_comment` triggers.

## How It Works

1. **Install the app** on your repositories
2. **Comment triggers** are filtered and converted to `repository_dispatch` events
3. **Workflows run only when needed** - no more skipped runs!

## Supported Triggers

### Human Commands
- `@atriumn start` → Triggers research phase
- `@atriumn approve-research` → Approves research and starts planning
- `@atriumn approve-plan` → Approves plan and starts implementation

### AI Status Updates
- `🟣 ATRIUMN-RESEARCH-COMPLETE` → Research phase completed
- `🟣 ATRIUMN-PLANNING-COMPLETE` → Planning phase completed  
- `🟣 ATRIUMN-IMPLEMENTATION-COMPLETE` → Implementation phase completed

## Setup

### 1. Create GitHub App
```bash
# Use the app.yml manifest to create your GitHub App
gh api --method POST /app-manifests/YOUR-MANIFEST-CODE \
  --field manifest=@app.yml
```

### 2. Install Dependencies
```bash
npm install
```

### 3. Configure Environment
```bash
export GITHUB_APP_ID=your_app_id
export GITHUB_PRIVATE_KEY="-----BEGIN RSA PRIVATE KEY-----..."
export GITHUB_WEBHOOK_SECRET=your_webhook_secret
```

### 4. Deploy
Deploy to your preferred platform (Vercel, Railway, Heroku, etc.)

### 5. Update Workflows
Change your workflow triggers from:
```yaml
on:
  issue_comment:
    types: [created]
```

To:
```yaml
on:
  repository_dispatch:
    types: [pipeline-start, research-complete, approve-research, planning-complete, approve-plan, implementation-complete]
```

## Development

```bash
npm run dev  # Local development with nodemon
npm start    # Production server
```

## Benefits

- **90% fewer GitHub Actions runs** (no more skipped runs)
- **Faster trigger response** (direct dispatch vs comment filtering)
- **Cleaner Actions history** (only meaningful runs)
- **Cost savings** on GitHub Actions minutes

================
File: scripts/create-manual-pr.sh
================
#!/bin/bash
set -e
# Script to create PR with manual approval
# Usage: ./create-manual-pr.sh ISSUE_NUM ISSUE_TITLE BRANCH_NAME BASE_BRANCH REPO_NAME APPROVER_LOGIN RESEARCH_DOC PLAN_DOC DECISION_DOC
ISSUE_NUM="$1"
ISSUE_TITLE="$2"
BRANCH_NAME="$3"
BASE_BRANCH="$4"
REPO_NAME="$5"
APPROVER_LOGIN="$6"
RESEARCH_DOC="$7"
PLAN_DOC="$8"
DECISION_DOC="$9"
PR_TITLE="Implement: $ISSUE_TITLE (#$ISSUE_NUM)"
# Create PR body
cat > pr_body.md << 'EOF'
## Summary
Implementation for issue #{ISSUE_NUM}: {ISSUE_TITLE}
**Human-approved implementation** - Reviewed and approved by @{APPROVER_LOGIN}
## Context Documents
- **Research**: [View Research Doc](https://github.com/{REPO_NAME}/blob/{BRANCH_NAME}/{RESEARCH_DOC})
- **Plan**: [View Implementation Plan](https://github.com/{REPO_NAME}/blob/{BRANCH_NAME}/{PLAN_DOC})  
- **Decision Record**: [View Decision Record](https://github.com/{REPO_NAME}/blob/{BRANCH_NAME}/{DECISION_DOC})
## Test Plan
- [x] Automated validation passed
- [x] Implementation reviewed and approved
- [x] All success criteria met
## Review Guidelines
This PR was generated through the Atriumn Issue-Driven Development pipeline with:
- Human-validated research and planning phases
- Implementation review and approval
- Quality checks
**Closes #{ISSUE_NUM}**
🟣 Generated with [Atriumn Issue-Driven Development](https://github.com/atriumn/atriumn-issue-driven-development)
EOF
# Substitute variables
sed -i.bak "s/{ISSUE_NUM}/$ISSUE_NUM/g" pr_body.md
sed -i.bak "s/{ISSUE_TITLE}/$ISSUE_TITLE/g" pr_body.md
sed -i.bak "s/{APPROVER_LOGIN}/$APPROVER_LOGIN/g" pr_body.md
sed -i.bak "s/{REPO_NAME}/$REPO_NAME/g" pr_body.md
sed -i.bak "s/{BRANCH_NAME}/$BRANCH_NAME/g" pr_body.md
sed -i.bak "s/{RESEARCH_DOC}/$RESEARCH_DOC/g" pr_body.md
sed -i.bak "s/{PLAN_DOC}/$PLAN_DOC/g" pr_body.md
sed -i.bak "s/{DECISION_DOC}/$DECISION_DOC/g" pr_body.md
# Create PR
gh pr create \
  --repo "$REPO_NAME" \
  --title "$PR_TITLE" \
  --head "$BRANCH_NAME" \
  --base "$BASE_BRANCH" \
  --body-file pr_body.md
# Get PR number and return it
PR_NUMBER=$(gh pr view $BRANCH_NAME --repo "$REPO_NAME" --json number --jq -r '.number')
echo "PR_NUMBER=$PR_NUMBER"

================
File: scripts/create-pr.sh
================
#!/bin/bash
set -e
# Script to create PR with proper formatting
# Usage: ./create-pr.sh ISSUE_NUM ISSUE_TITLE BRANCH_NAME BASE_BRANCH REPO_NAME VALIDATION_RESULTS RESEARCH_DOC PLAN_DOC DECISION_DOC
ISSUE_NUM="$1"
ISSUE_TITLE="$2"
BRANCH_NAME="$3"
BASE_BRANCH="$4"
REPO_NAME="$5"
VALIDATION_RESULTS="$6"
RESEARCH_DOC="$7"
PLAN_DOC="$8"
DECISION_DOC="$9"
PR_TITLE="Implement: $ISSUE_TITLE (#$ISSUE_NUM)"
# Create PR body
cat > pr_body.md << 'EOF'
## Summary
Automated implementation for issue #{ISSUE_NUM}: {ISSUE_TITLE}
**Validation Results:**
{VALIDATION_RESULTS}
## Context Documents
- **Research**: [View Research Doc](https://github.com/{REPO_NAME}/blob/{BRANCH_NAME}/{RESEARCH_DOC})
- **Plan**: [View Implementation Plan](https://github.com/{REPO_NAME}/blob/{BRANCH_NAME}/{PLAN_DOC})  
- **Decision Record**: [View Decision Record](https://github.com/{REPO_NAME}/blob/{BRANCH_NAME}/{DECISION_DOC})
## Test Plan
- [x] Automated validation passed
- [x] Implementation follows approved plan
- [x] All success criteria met
## Review Guidelines
This PR was generated through the Atriumn Issue-Driven Development pipeline with:
- Automated research and planning phases
- Implementation validation
- Quality checks
**Closes #{ISSUE_NUM}**
🟣 Generated with [Atriumn Issue-Driven Development](https://github.com/atriumn/atriumn-issue-driven-development)
EOF
# Substitute variables
sed -i.bak "s/{ISSUE_NUM}/$ISSUE_NUM/g" pr_body.md
sed -i.bak "s/{ISSUE_TITLE}/$ISSUE_TITLE/g" pr_body.md
sed -i.bak "s/{VALIDATION_RESULTS}/$VALIDATION_RESULTS/g" pr_body.md
sed -i.bak "s/{REPO_NAME}/$REPO_NAME/g" pr_body.md
sed -i.bak "s/{BRANCH_NAME}/$BRANCH_NAME/g" pr_body.md
sed -i.bak "s/{RESEARCH_DOC}/$RESEARCH_DOC/g" pr_body.md
sed -i.bak "s/{PLAN_DOC}/$PLAN_DOC/g" pr_body.md
sed -i.bak "s/{DECISION_DOC}/$DECISION_DOC/g" pr_body.md
# Create PR
gh pr create \
  --repo "$REPO_NAME" \
  --title "$PR_TITLE" \
  --head "$BRANCH_NAME" \
  --base "$BASE_BRANCH" \
  --body-file pr_body.md
# Get PR number and return it
PR_NUMBER=$(gh pr view $BRANCH_NAME --repo "$REPO_NAME" --json number --jq -r '.number')
echo "PR_NUMBER=$PR_NUMBER"

================
File: scripts/manage-decision-record.py
================
#!/usr/bin/env python3
"""
Decision Record Management Script
Handles compression, summarization, and archiving of pipeline decision records
"""
import os
import re
import sys
import argparse
import shutil
from datetime import datetime
from pathlib import Path
class DecisionRecordManager:
    """Manages decision record size and readability"""
    def __init__(self, decision_file_path):
        self.decision_file = Path(decision_file_path)
        self.backup_dir = self.decision_file.parent / f"{self.decision_file.stem}-archive"
        self.backup_dir.mkdir(exist_ok=True)
    def analyze_size(self):
        """Analyze decision record size and complexity"""
        if not self.decision_file.exists():
            return {"exists": False}
        with open(self.decision_file, 'r') as f:
            content = f.read()
        lines = content.split('\n')
        words = len(content.split())
        # Count phases
        completed_phases = len(re.findall(r'## \w+ Phase.*?Complete ✅', content))
        total_phases = len(re.findall(r'## \w+ Phase', content))
        # Count sections
        sections = len(re.findall(r'^## ', content, re.MULTILINE))
        subsections = len(re.findall(r'^### ', content, re.MULTILINE))
        return {
            "exists": True,
            "lines": len(lines),
            "words": words,
            "completed_phases": completed_phases,
            "total_phases": total_phases,
            "sections": sections,
            "subsections": subsections,
            "file_size": self.decision_file.stat().st_size
        }
    def create_backup(self):
        """Create timestamped backup of current decision record"""
        timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
        backup_file = self.backup_dir / f"decision-record-backup-{timestamp}.md"
        shutil.copy2(self.decision_file, backup_file)
        return backup_file
    def compress_completed_phases(self):
        """Compress completed phases into collapsible sections"""
        backup_file = self.create_backup()
        with open(self.decision_file, 'r') as f:
            content = f.read()
        def compress_phase(match):
            section_header = match.group(1)
            section_content = match.group(2)
            if "(Complete ✅)" in section_header:
                # Extract key information for summary
                summary_lines = []
                key_patterns = ['Status', 'Validated', 'Document', 'Completed', 'Next Phase']
                for line in section_content.split('\n'):
                    line = line.strip()
                    if line.startswith('- **') and any(pattern in line for pattern in key_patterns):
                        summary_lines.append(line)
                summary = '\n'.join(summary_lines[:4])  # Keep top 4 key facts
                return f"""{section_header}
<details>
<summary>📋 Phase Summary (click to expand)</summary>
{summary}
<details>
<summary>📝 Complete Phase Details</summary>
{section_content}
</details>
</details>
"""
            return match.group(0)
        # Apply compression to completed phases
        compressed = re.sub(
            r'(## \w+ Phase.*?Complete ✅.*?)\n(.*?)(?=\n## |\Z)',
            compress_phase,
            content,
            flags=re.DOTALL
        )
        with open(self.decision_file, 'w') as f:
            f.write(compressed)
        return {
            "action": "compressed",
            "backup_file": str(backup_file),
            "original_lines": len(content.split('\n')),
            "compressed_lines": len(compressed.split('\n'))
        }
    def summarize_record(self):
        """Create summary version and archive detailed sections"""
        backup_file = self.create_backup()
        with open(self.decision_file, 'r') as f:
            content = f.read()
        # Extract key sections to keep in summary
        summary_sections = []
        archive_sections = []
        # Split content into sections
        sections = re.split(r'\n(?=## )', content)
        for section in sections:
            if not section.strip():
                continue
            section_header = section.split('\n')[0]
            # Keep these sections in summary
            if any(keyword in section_header for keyword in [
                'Issue Context', 'Current Status', 'Pipeline Progress', 'Decision'
            ]):
                summary_sections.append(section)
            elif "(Complete ✅)" in section_header:
                # Compress completed phases heavily
                lines = section.split('\n')
                header = lines[0]
                # Extract only the most essential info
                essential_info = []
                for line in lines[1:]:
                    if '**Status**:' in line or '**Validated**:' in line or '**Document**:' in line:
                        essential_info.append(line)
                compressed_section = f"{header}\n" + '\n'.join(essential_info[:2])
                summary_sections.append(compressed_section)
                # Archive full section
                phase_name = re.search(r'## (\w+ Phase)', header)
                if phase_name:
                    archive_file = self.backup_dir / f"{phase_name.group(1).lower().replace(' ', '-')}-details.md"
                    with open(archive_file, 'w') as f:
                        f.write(section)
                    archive_sections.append(str(archive_file))
            else:
                # Keep current/active sections
                summary_sections.append(section)
        # Create summary content
        summary_content = '\n\n'.join(summary_sections)
        # Add archive reference
        if archive_sections:
            summary_content += f"""
## Archived Sections
Detailed phase information has been archived for space efficiency:
{chr(10).join(f"- [{Path(f).stem}]({f})" for f in archive_sections)}
---
*Summary generated on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
*Full backup: [{backup_file.name}]({backup_file})*
"""
        with open(self.decision_file, 'w') as f:
            f.write(summary_content)
        return {
            "action": "summarized",
            "backup_file": str(backup_file),
            "archived_files": archive_sections,
            "original_lines": len(content.split('\n')),
            "summary_lines": len(summary_content.split('\n'))
        }
    def restore_from_backup(self, backup_file=None):
        """Restore decision record from backup"""
        if backup_file:
            backup_path = Path(backup_file)
        else:
            # Get most recent backup
            backups = list(self.backup_dir.glob("decision-record-backup-*.md"))
            if not backups:
                raise FileNotFoundError("No backup files found")
            backup_path = max(backups, key=lambda p: p.stat().st_mtime)
        shutil.copy2(backup_path, self.decision_file)
        return {"action": "restored", "from_backup": str(backup_path)}
def main():
    parser = argparse.ArgumentParser(description="Manage pipeline decision records")
    parser.add_argument("decision_file", help="Path to decision record file")
    parser.add_argument("--action", choices=["analyze", "compress", "summarize", "restore"], 
                       default="analyze", help="Action to perform")
    parser.add_argument("--backup-file", help="Specific backup file to restore from")
    parser.add_argument("--auto", action="store_true", 
                       help="Automatically choose action based on file size")
    args = parser.parse_args()
    manager = DecisionRecordManager(args.decision_file)
    if args.action == "analyze" or args.auto:
        analysis = manager.analyze_size()
        if not analysis["exists"]:
            print("Decision record file does not exist")
            sys.exit(1)
        print(f"Decision Record Analysis:")
        print(f"  Lines: {analysis['lines']}")
        print(f"  Words: {analysis['words']}")
        print(f"  File Size: {analysis['file_size']} bytes")
        print(f"  Completed Phases: {analysis['completed_phases']}/{analysis['total_phases']}")
        print(f"  Sections: {analysis['sections']}")
        print(f"  Subsections: {analysis['subsections']}")
        if args.auto:
            if analysis["lines"] > 200:
                print("\nFile is large - applying summarization...")
                result = manager.summarize_record()
                print(f"Summarized: {result['original_lines']} → {result['summary_lines']} lines")
            elif analysis["lines"] > 150:
                print("\nFile is getting large - applying compression...")
                result = manager.compress_completed_phases()
                print(f"Compressed: {result['original_lines']} → {result['compressed_lines']} lines")
            else:
                print("\nFile size is manageable - no action needed")
    elif args.action == "compress":
        result = manager.compress_completed_phases()
        print(f"Compressed completed phases")
        print(f"Lines: {result['original_lines']} → {result['compressed_lines']}")
        print(f"Backup: {result['backup_file']}")
    elif args.action == "summarize":
        result = manager.summarize_record()
        print(f"Created summary version")
        print(f"Lines: {result['original_lines']} → {result['summary_lines']}")
        print(f"Backup: {result['backup_file']}")
        print(f"Archived: {len(result['archived_files'])} sections")
    elif args.action == "restore":
        result = manager.restore_from_backup(args.backup_file)
        print(f"Restored from: {result['from_backup']}")
if __name__ == "__main__":
    main()

================
File: scripts/validate-config.py
================
#!/usr/bin/env python3
"""
Configuration Validation Script
Validates repository configuration files against the schema
"""
import argparse
import json
import os
import sys
import yaml
from pathlib import Path
class ConfigValidator:
    """Validates repository configurations against schema"""
    def __init__(self, schema_file):
        self.schema_file = Path(schema_file)
        self.schema = self._load_schema()
    def _load_schema(self):
        """Load the configuration schema"""
        with open(self.schema_file, 'r') as f:
            return yaml.safe_load(f)
    def validate_config(self, config_file):
        """Validate a configuration file"""
        config_path = Path(config_file)
        if not config_path.exists():
            return {
                "valid": False,
                "error": f"Configuration file not found: {config_file}"
            }
        try:
            with open(config_path, 'r') as f:
                config = yaml.safe_load(f)
        except yaml.YAMLError as e:
            return {
                "valid": False,
                "error": f"Invalid YAML syntax: {e}"
            }
        # Validate against schema
        validation_result = self._validate_against_schema(config)
        if validation_result["valid"]:
            # Apply defaults and enhancements
            enhanced_config = self._apply_defaults(config)
            validation_result["enhanced_config"] = enhanced_config
        return validation_result
    def _validate_against_schema(self, config):
        """Validate configuration against schema"""
        schema_def = self.schema["configuration_schema"]
        field_definitions = self.schema["field_definitions"]
        errors = []
        warnings = []
        # Check required fields
        for field in schema_def["required_fields"]:
            if field not in config:
                errors.append(f"Missing required field: {field}")
        # Validate field types and constraints
        for field, value in config.items():
            if field in field_definitions:
                field_errors = self._validate_field(field, value, field_definitions[field])
                errors.extend(field_errors)
        # Check for unknown fields
        known_fields = schema_def["required_fields"] + schema_def["optional_fields"]
        for field in config:
            if field not in known_fields:
                warnings.append(f"Unknown field (will be ignored): {field}")
        return {
            "valid": len(errors) == 0,
            "errors": errors,
            "warnings": warnings
        }
    def _validate_field(self, field_name, value, field_def):
        """Validate a specific field"""
        errors = []
        # Type validation
        expected_type = field_def["type"]
        if expected_type == "string" and not isinstance(value, str):
            errors.append(f"{field_name}: Expected string, got {type(value).__name__}")
        elif expected_type == "integer" and not isinstance(value, int):
            errors.append(f"{field_name}: Expected integer, got {type(value).__name__}")
        elif expected_type == "boolean" and not isinstance(value, bool):
            errors.append(f"{field_name}: Expected boolean, got {type(value).__name__}")
        elif expected_type == "array" and not isinstance(value, list):
            errors.append(f"{field_name}: Expected array, got {type(value).__name__}")
        elif expected_type == "object" and not isinstance(value, dict):
            errors.append(f"{field_name}: Expected object, got {type(value).__name__}")
        # Pattern validation
        if "pattern" in field_def and isinstance(value, str):
            import re
            if not re.match(field_def["pattern"], value):
                errors.append(f"{field_name}: Value '{value}' doesn't match pattern '{field_def['pattern']}'")
        # Range validation
        if isinstance(value, int):
            if "minimum" in field_def and value < field_def["minimum"]:
                errors.append(f"{field_name}: Value {value} below minimum {field_def['minimum']}")
            if "maximum" in field_def and value > field_def["maximum"]:
                errors.append(f"{field_name}: Value {value} above maximum {field_def['maximum']}")
        # Allowed values validation
        if "allowed_values" in field_def and value not in field_def["allowed_values"]:
            errors.append(f"{field_name}: Value '{value}' not in allowed values {field_def['allowed_values']}")
        # Recursive validation for objects
        if expected_type == "object" and "properties" in field_def:
            for prop_name, prop_value in value.items():
                if prop_name in field_def["properties"]:
                    prop_errors = self._validate_field(
                        f"{field_name}.{prop_name}", 
                        prop_value, 
                        field_def["properties"][prop_name]
                    )
                    errors.extend(prop_errors)
        return errors
    def _apply_defaults(self, config):
        """Apply default values to configuration"""
        field_definitions = self.schema["field_definitions"]
        enhanced_config = config.copy()
        for field_name, field_def in field_definitions.items():
            if field_name not in enhanced_config and "default" in field_def:
                enhanced_config[field_name] = field_def["default"]
        return enhanced_config
    def get_config_recommendations(self, config):
        """Provide recommendations based on configuration"""
        recommendations = []
        repo_name = config.get("repo_name", "")
        # Repository type detection and recommendations
        if "api" in repo_name.lower() or "service" in repo_name.lower():
            recommendations.append({
                "type": "repository_type",
                "message": "Detected API/service repository. Consider increasing research_min_refs to 5 and adding security validation commands."
            })
        if "platform" in repo_name.lower() or "infrastructure" in repo_name.lower():
            recommendations.append({
                "type": "repository_type", 
                "message": "Detected infrastructure repository. Consider stricter validation, longer timeouts, and limiting parallel pipelines."
            })
        # Validation recommendations
        validation = config.get("validation", {})
        if validation.get("research_min_refs", 3) < 3:
            recommendations.append({
                "type": "validation",
                "message": "Consider increasing research_min_refs to at least 3 for better documentation quality."
            })
        # Team recommendations
        team = config.get("team", {})
        if not team.get("default_reviewers"):
            recommendations.append({
                "type": "team",
                "message": "Consider adding default_reviewers to ensure all PRs have reviewers assigned."
            })
        return recommendations
    def generate_config_report(self, config_file):
        """Generate a comprehensive configuration report"""
        validation_result = self.validate_config(config_file)
        if not validation_result["valid"]:
            return validation_result
        config = validation_result["enhanced_config"]
        recommendations = self.get_config_recommendations(config)
        return {
            "valid": True,
            "config_file": str(config_file),
            "repo_name": config.get("repo_name"),
            "enhanced_config": config,
            "recommendations": recommendations,
            "warnings": validation_result.get("warnings", []),
            "summary": {
                "base_branch": config.get("base_branch"),
                "thoughts_directory": config.get("thoughts_directory"),
                "research_min_refs": config.get("validation", {}).get("research_min_refs"),
                "team_size": len(config.get("team", {}).get("default_reviewers", [])),
                "has_notifications": bool(config.get("notifications", {})),
                "parallel_pipelines": config.get("workflow_customization", {}).get("parallel_pipelines", 3)
            }
        }
def main():
    parser = argparse.ArgumentParser(description="Validate repository configuration files")
    parser.add_argument("config_file", help="Path to configuration file to validate")
    parser.add_argument("--schema", default="configs/schema.yml", 
                       help="Path to schema file")
    parser.add_argument("--output", choices=["text", "json"], default="text",
                       help="Output format")
    parser.add_argument("--report", action="store_true",
                       help="Generate comprehensive report")
    args = parser.parse_args()
    # Find schema file
    script_dir = Path(__file__).parent
    schema_path = script_dir.parent / args.schema
    if not schema_path.exists():
        print(f"❌ Schema file not found: {schema_path}")
        sys.exit(1)
    validator = ConfigValidator(schema_path)
    if args.report:
        result = validator.generate_config_report(args.config_file)
    else:
        result = validator.validate_config(args.config_file)
    if args.output == "json":
        print(json.dumps(result, indent=2))
    else:
        # Text output
        if result["valid"]:
            print("✅ Configuration validation PASSED")
            if args.report:
                print(f"\n📊 Configuration Report for {result['repo_name']}")
                print("=" * 50)
                summary = result["summary"]
                print(f"Base branch: {summary['base_branch']}")
                print(f"Thoughts directory: {summary['thoughts_directory']}")
                print(f"Research min refs: {summary['research_min_refs']}")
                print(f"Team reviewers: {summary['team_size']}")
                print(f"Notifications: {'✅' if summary['has_notifications'] else '❌'}")
                print(f"Parallel pipelines: {summary['parallel_pipelines']}")
                if result["recommendations"]:
                    print(f"\n💡 Recommendations:")
                    for rec in result["recommendations"]:
                        print(f"  • [{rec['type']}] {rec['message']}")
            if result.get("warnings"):
                print(f"\n⚠️  Warnings:")
                for warning in result["warnings"]:
                    print(f"  • {warning}")
        else:
            print("❌ Configuration validation FAILED")
            if "error" in result:
                print(f"Error: {result['error']}")
            if "errors" in result:
                print("\nErrors:")
                for error in result["errors"]:
                    print(f"  • {error}")
    sys.exit(0 if result["valid"] else 1)
if __name__ == "__main__":
    main()

================
File: scripts/validate-config.sh
================
#!/bin/bash
# scripts/validate-config.sh - Configuration validation script
set -e
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
CONFIG_FILE="${1}"
if [ -z "$CONFIG_FILE" ]; then
    echo "Usage: $0 <config-file>"
    echo ""
    echo "Examples:"
    echo "  $0 .github/development-pipeline-config.yml"
    echo "  $0 configs/curatefor.me.yml"
    exit 1
fi
if [ ! -f "$CONFIG_FILE" ]; then
    echo "❌ Configuration file not found: $CONFIG_FILE"
    exit 1
fi
echo "🔍 Validating configuration: $CONFIG_FILE"
# Check if required tools are available
check_dependencies() {
    if ! command -v yq >/dev/null 2>&1; then
        echo "❌ yq is required but not installed."
        echo "   Install with: wget -qO /usr/local/bin/yq https://github.com/mikefarah/yq/releases/latest/download/yq_linux_amd64"
        exit 1
    fi
    if ! command -v python3 >/dev/null 2>&1; then
        echo "❌ python3 is required but not installed."
        exit 1
    fi
}
# Basic YAML syntax check
validate_yaml_syntax() {
    if ! yq eval '.' "$CONFIG_FILE" >/dev/null 2>&1; then
        echo "❌ Invalid YAML syntax in $CONFIG_FILE"
        exit 1
    fi
    echo "✅ YAML syntax valid"
}
# Validate against schema
validate_schema() {
    python3 << EOF
import yaml
import sys
import os
# Load schema
schema_file = os.path.join('$SCRIPT_DIR', '..', 'configs', 'schema.yml')
if not os.path.exists(schema_file):
    print("⚠️  Schema file not found, skipping schema validation")
    sys.exit(0)
with open(schema_file, 'r') as f:
    schema_doc = yaml.safe_load(f)
# Load config
with open('$CONFIG_FILE', 'r') as f:
    config = yaml.safe_load(f)
# Check required fields
required_fields = schema_doc['configuration_schema']['required_fields']
for field in required_fields:
    if field not in config:
        print(f"❌ Missing required field: {field}")
        sys.exit(1)
print("✅ All required fields present")
# Validate field types and constraints
field_definitions = schema_doc['field_definitions']
for field, value in config.items():
    if field in field_definitions:
        definition = field_definitions[field]
        # Check type
        expected_type = definition['type']
        if expected_type == 'string' and not isinstance(value, str):
            print(f"❌ Field '{field}' should be string, got {type(value).__name__}")
            sys.exit(1)
        elif expected_type == 'integer' and not isinstance(value, int):
            print(f"❌ Field '{field}' should be integer, got {type(value).__name__}")
            sys.exit(1)
        elif expected_type == 'boolean' and not isinstance(value, bool):
            print(f"❌ Field '{field}' should be boolean, got {type(value).__name__}")
            sys.exit(1)
        elif expected_type == 'array' and not isinstance(value, list):
            print(f"❌ Field '{field}' should be array, got {type(value).__name__}")
            sys.exit(1)
        elif expected_type == 'object' and not isinstance(value, dict):
            print(f"❌ Field '{field}' should be object, got {type(value).__name__}")
            sys.exit(1)
        # Check constraints
        if expected_type == 'integer':
            if 'minimum' in definition and value < definition['minimum']:
                print(f"❌ Field '{field}' value {value} below minimum {definition['minimum']}")
                sys.exit(1)
            if 'maximum' in definition and value > definition['maximum']:
                print(f"❌ Field '{field}' value {value} above maximum {definition['maximum']}")
                sys.exit(1)
print("✅ Schema validation passed")
EOF
}
# Validate specific field values
validate_field_values() {
    # Check repo_name
    REPO_NAME=$(yq eval '.repo_name' "$CONFIG_FILE")
    if [ "$REPO_NAME" = "null" ] || [ -z "$REPO_NAME" ]; then
        echo "❌ repo_name is required"
        exit 1
    fi
    echo "✅ Repository name: $REPO_NAME"
    # Check base_branch
    BASE_BRANCH=$(yq eval '.base_branch' "$CONFIG_FILE")
    if [ "$BASE_BRANCH" = "null" ] || [ -z "$BASE_BRANCH" ]; then
        echo "❌ base_branch is required"
        exit 1
    fi
    ALLOWED_BRANCHES=("main" "master" "develop" "dev")
    if [[ ! " ${ALLOWED_BRANCHES[@]} " =~ " ${BASE_BRANCH} " ]]; then
        echo "⚠️  Base branch '$BASE_BRANCH' is not in recommended list: ${ALLOWED_BRANCHES[*]}"
    fi
    echo "✅ Base branch: $BASE_BRANCH"
    # Check thoughts_directory
    THOUGHTS_DIR=$(yq eval '.thoughts_directory' "$CONFIG_FILE")
    if [ "$THOUGHTS_DIR" = "null" ] || [ -z "$THOUGHTS_DIR" ]; then
        echo "❌ thoughts_directory is required"
        exit 1
    fi
    if [[ ! "$THOUGHTS_DIR" =~ /$ ]]; then
        echo "❌ thoughts_directory must end with '/'"
        exit 1
    fi
    echo "✅ Thoughts directory: $THOUGHTS_DIR"
    # Validate team configuration if present
    if yq eval '.team' "$CONFIG_FILE" >/dev/null 2>&1 && [ "$(yq eval '.team' "$CONFIG_FILE")" != "null" ]; then
        # Check reviewer format
        if yq eval '.team.default_reviewers[]?' "$CONFIG_FILE" >/dev/null 2>&1; then
            while IFS= read -r reviewer; do
                if [[ ! "$reviewer" =~ ^@[a-zA-Z0-9_-]+$ ]]; then
                    echo "❌ Invalid reviewer format: '$reviewer' (should be @username)"
                    exit 1
                fi
            done < <(yq eval '.team.default_reviewers[]' "$CONFIG_FILE")
            echo "✅ Team reviewer format valid"
        fi
    fi
    # Validate notification configuration if present
    if yq eval '.notifications.slack_channel' "$CONFIG_FILE" >/dev/null 2>&1 && [ "$(yq eval '.notifications.slack_channel' "$CONFIG_FILE")" != "null" ]; then
        SLACK_CHANNEL=$(yq eval '.notifications.slack_channel' "$CONFIG_FILE")
        if [[ ! "$SLACK_CHANNEL" =~ ^#[a-zA-Z0-9_-]+$ ]]; then
            echo "❌ Invalid slack channel format: '$SLACK_CHANNEL' (should be #channel-name)"
            exit 1
        fi
        echo "✅ Slack channel format valid"
    fi
}
# Check test commands are reasonable
validate_test_commands() {
    if yq eval '.validation.implementation_test_commands[]?' "$CONFIG_FILE" >/dev/null 2>&1; then
        echo "📋 Test commands configured:"
        while IFS= read -r command; do
            echo "   - $command"
            # Basic sanity checks
            if [[ "$command" =~ rm.*-rf|sudo|curl.*\|.*sh ]]; then
                echo "⚠️  Potentially dangerous command detected: $command"
            fi
        done < <(yq eval '.validation.implementation_test_commands[]' "$CONFIG_FILE")
        echo "✅ Test commands validated"
    else
        echo "⚠️  No test commands configured - using defaults"
    fi
}
# Generate configuration summary
generate_summary() {
    echo ""
    echo "📋 Configuration Summary"
    echo "========================"
    echo "Repository: $(yq eval '.repo_name' "$CONFIG_FILE")"
    echo "Base branch: $(yq eval '.base_branch' "$CONFIG_FILE")"
    echo "Thoughts directory: $(yq eval '.thoughts_directory' "$CONFIG_FILE")"
    if yq eval '.validation.research_min_refs' "$CONFIG_FILE" >/dev/null 2>&1; then
        echo "Min file references: $(yq eval '.validation.research_min_refs' "$CONFIG_FILE")"
    fi
    if yq eval '.team.default_reviewers[]?' "$CONFIG_FILE" >/dev/null 2>&1; then
        REVIEWER_COUNT=$(yq eval '.team.default_reviewers | length' "$CONFIG_FILE")
        echo "Default reviewers: $REVIEWER_COUNT configured"
    fi
    if yq eval '.workflow_customization.parallel_pipelines' "$CONFIG_FILE" >/dev/null 2>&1; then
        echo "Parallel pipelines: $(yq eval '.workflow_customization.parallel_pipelines' "$CONFIG_FILE")"
    fi
    echo ""
}
# Main validation flow
main() {
    check_dependencies
    validate_yaml_syntax
    validate_schema
    validate_field_values
    validate_test_commands
    generate_summary
    echo "✅ Configuration validation PASSED"
    echo ""
    echo "Next steps:"
    echo "1. Add this config to your repository: .github/development-pipeline-config.yml"
    echo "2. Create PIPELINE_TOKEN secret with GitHub Personal Access Token"
    echo "3. Test the pipeline with a test issue"
}
# Help text
if [ "$1" = "--help" ] || [ "$1" = "-h" ]; then
    echo "Configuration Validation Script"
    echo ""
    echo "Usage: $0 <config-file>"
    echo ""
    echo "Validates repository configuration against the pipeline schema."
    echo ""
    echo "Examples:"
    echo "  $0 .github/development-pipeline-config.yml"
    echo "  $0 configs/my-repo.yml"
    echo ""
    echo "Requirements:"
    echo "  - yq (YAML processor)"
    echo "  - python3 (for schema validation)"
    exit 0
fi
main

================
File: scripts/validate-implementation.sh
================
#!/bin/bash
# scripts/validate-implementation.sh
set -e
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
CONFIG_FILE="${1:-.github/development-pipeline-config.yml}"
BRANCH_NAME="${2}"
load_config() {
    if [ -f "$CONFIG_FILE" ]; then
        CONFIG_SOURCE="$CONFIG_FILE"
    else
        CONFIG_SOURCE="$SCRIPT_DIR/../configs/default.yml"
    fi
    # Check if yq is available
    if ! command -v yq >/dev/null 2>&1; then
        echo "❌ yq is required but not installed. Please install yq to use this script."
        exit 1
    fi
    # Load test commands from config (using portable approach)
    TEST_COMMANDS=()
    while IFS= read -r line; do
        TEST_COMMANDS+=("$line")
    done < <(yq eval '.validation.implementation_test_commands[]' "$CONFIG_SOURCE")
}
validate_branch_exists() {
    if ! git rev-parse --verify "$BRANCH_NAME" >/dev/null 2>&1; then
        echo "❌ Branch does not exist: $BRANCH_NAME"
        exit 1
    fi
    echo "✅ Branch exists: $BRANCH_NAME"
}
validate_branch_state() {
    # Switch to the branch
    git checkout "$BRANCH_NAME" >/dev/null 2>&1
    # Get base branch from config
    local base_branch=$(yq eval '.base_branch' "$CONFIG_SOURCE")
    # Check if branch is ahead of base
    local commits_ahead=$(git rev-list --count HEAD ^"$base_branch" 2>/dev/null || echo "0")
    if [ "$commits_ahead" -eq 0 ]; then
        echo "❌ Branch has no commits ahead of base branch ($base_branch)"
        exit 1
    fi
    echo "✅ Branch state valid ($commits_ahead commits ahead of $base_branch)"
}
validate_test_commands() {
    echo "🧪 Running implementation validation tests..."
    for cmd in "${TEST_COMMANDS[@]}"; do
        echo "   Running: $cmd"
        if ! eval "$cmd" >/dev/null 2>&1; then
            echo "❌ Test command failed: $cmd"
            exit 1
        fi
        echo "   ✅ Passed: $cmd"
    done
    echo "✅ All test commands passed"
}
validate_no_merge_conflicts() {
    # Check if branch can merge cleanly with base
    local base_branch=$(yq eval '.base_branch' "$CONFIG_SOURCE")
    if ! git merge-tree "$(git merge-base HEAD "$base_branch")" HEAD "$base_branch" >/dev/null 2>&1; then
        echo "❌ Branch has merge conflicts with $base_branch"
        exit 1
    fi
    echo "✅ No merge conflicts detected"
}
validate_decision_record_updated() {
    local issue_number=$(echo "$BRANCH_NAME" | grep -o 'issue-[0-9]*' | cut -d'-' -f2 2>/dev/null || echo "")
    if [ -z "$issue_number" ]; then
        echo "⚠️  Could not extract issue number from branch name: $BRANCH_NAME"
        echo "   Expected format: feature/issue-123-description"
        return 0
    fi
    local decision_file="thoughts/shared/decisions/pipeline-issue-$issue_number.md"
    if [ ! -f "$decision_file" ]; then
        echo "❌ Decision record not found: $decision_file"
        exit 1
    fi
    # Check if decision record mentions implementation
    if ! grep -q "Implementation Phase" "$decision_file"; then
        echo "❌ Decision record not updated with implementation details"
        exit 1
    fi
    echo "✅ Decision record updated: $decision_file"
}
main() {
    echo "⚙️ Validating implementation on branch: $BRANCH_NAME"
    echo "📝 Using config: $CONFIG_SOURCE"
    load_config
    validate_branch_exists
    validate_branch_state
    validate_test_commands
    validate_no_merge_conflicts
    validate_decision_record_updated
    echo ""
    echo "✅ Implementation validation PASSED"
    echo "📊 Implementation statistics:"
    local base_branch=$(yq eval '.base_branch' "$CONFIG_SOURCE")
    echo "   - Commits: $(git rev-list --count HEAD ^"$base_branch" 2>/dev/null || echo "0")"
    echo "   - Files changed: $(git diff --name-only "$base_branch" 2>/dev/null | wc -l || echo "0")"
    echo "   - Test commands run: ${#TEST_COMMANDS[@]}"
}
if [ "$1" = "--help" ] || [ "$1" = "-h" ]; then
    echo "Usage: $0 [config_file] [branch_name]"
    echo ""
    echo "Examples:"
    echo "  $0 feature/issue-123-my-feature"
    echo "  $0 .github/dev-config.yml feature/issue-123-my-feature"
    exit 0
fi
main "$@"

================
File: scripts/validate-plan.sh
================
#!/bin/bash
# scripts/validate-plan.sh
set -e
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
# Parse arguments - if only one argument and it ends in .md, treat as plan doc
if [ $# -eq 1 ] && [[ "$1" == *.md ]]; then
    CONFIG_FILE=".github/development-pipeline-config.yml"
    PLAN_DOC="$1"
elif [ $# -eq 2 ]; then
    CONFIG_FILE="$1"
    PLAN_DOC="$2"
else
    CONFIG_FILE="${1:-.github/development-pipeline-config.yml}"
    PLAN_DOC="${2}"
fi
load_config() {
    if [ -f "$CONFIG_FILE" ]; then
        CONFIG_SOURCE="$CONFIG_FILE"
    else
        CONFIG_SOURCE="$SCRIPT_DIR/../configs/default.yml"
    fi
    # Check if yq is available
    if ! command -v yq >/dev/null 2>&1; then
        echo "❌ yq is required but not installed. Please install yq to use this script."
        exit 1
    fi
    # Load required sections from config (using portable approach)
    REQUIRED_SECTIONS=()
    while IFS= read -r line; do
        REQUIRED_SECTIONS+=("$line")
    done < <(yq eval '.validation.plan_required_sections[]' "$CONFIG_SOURCE")
}
validate_file_exists() {
    if [ ! -f "$PLAN_DOC" ]; then
        echo "❌ Implementation plan not found: $PLAN_DOC"
        exit 1
    fi
    echo "✅ Implementation plan exists: $PLAN_DOC"
}
validate_yaml_frontmatter() {
    if ! head -20 "$PLAN_DOC" | grep -q "^---"; then
        echo "❌ Missing YAML frontmatter"
        exit 1
    fi
    local required_fields=("date" "researcher" "topic" "status")
    for field in "${required_fields[@]}"; do
        if ! grep -q "^$field:" "$PLAN_DOC"; then
            echo "❌ Missing required frontmatter field: $field"
            exit 1
        fi
    done
    echo "✅ YAML frontmatter valid"
}
validate_required_sections() {
    for section in "${REQUIRED_SECTIONS[@]}"; do
        if ! grep -q "$section" "$PLAN_DOC"; then
            echo "❌ Missing required section: $section"
            exit 1
        fi
    done
    echo "✅ All required sections present"
}
validate_success_criteria() {
    # Check for both automated and manual verification sections
    if ! grep -A 10 "#### Automated Verification:" "$PLAN_DOC" | grep -q "make \|npm \|test"; then
        echo "❌ Automated verification section missing or invalid"
        echo "   Expected: Commands like 'make test', 'npm run lint', etc."
        exit 1
    fi
    if ! grep -q "#### Manual Verification:" "$PLAN_DOC"; then
        echo "❌ Manual verification section missing"
        exit 1
    fi
    echo "✅ Success criteria properly formatted"
}
validate_no_open_questions() {
    # Check for unresolved questions or TODOs (skip YAML frontmatter)
    local issues=("TODO" "FIXME" "XXX" "TBD")
    # Skip YAML frontmatter when checking for issues
    local content_without_frontmatter
    if head -20 "$PLAN_DOC" | grep -q "^---"; then
        # Find the end of frontmatter and get content after it
        content_without_frontmatter=$(awk '/^---$/{if(++c==2) f=1; next} f' "$PLAN_DOC")
    else
        content_without_frontmatter=$(cat "$PLAN_DOC")
    fi
    for issue in "${issues[@]}"; do
        if echo "$content_without_frontmatter" | grep -q "$issue"; then
            echo "❌ Found unresolved question or TODO: $issue"
            echo "   Implementation plan must resolve all questions before proceeding"
            exit 1
        fi
    done
    # Check for question marks pattern (???) - using simpler approach
    if echo "$content_without_frontmatter" | grep -q "???"; then
        echo "❌ Found unresolved question or TODO: ???"
        echo "   Implementation plan must resolve all questions before proceeding"
        exit 1
    fi
    echo "✅ No unresolved questions found"
}
validate_phase_structure() {
    # Count phases in the plan
    local phase_count=$(grep -c "^## Phase [0-9]" "$PLAN_DOC" || echo "0")
    if [ "$phase_count" -eq 0 ]; then
        echo "❌ No implementation phases found"
        echo "   Expected: ## Phase 1, ## Phase 2, etc."
        exit 1
    fi
    echo "✅ Implementation phases found ($phase_count phases)"
}
main() {
    echo "📋 Validating implementation plan: $PLAN_DOC"
    echo "📝 Using config: $CONFIG_SOURCE"
    load_config
    validate_file_exists
    validate_yaml_frontmatter
    validate_required_sections
    validate_success_criteria
    validate_no_open_questions
    validate_phase_structure
    echo ""
    echo "✅ Plan validation PASSED"
    echo "📊 Plan statistics:"
    echo "   - Phases: $(grep -c "^## Phase [0-9]" "$PLAN_DOC")"
    echo "   - Success criteria: $(grep -c "#### .*Verification:" "$PLAN_DOC")"
    echo "   - Word count: $(wc -w < "$PLAN_DOC")"
}
if [ "$1" = "--help" ] || [ "$1" = "-h" ]; then
    echo "Usage: $0 [config_file] [plan_document]"
    echo ""
    echo "Examples:"
    echo "  $0 thoughts/shared/plans/my-implementation-plan.md"
    echo "  $0 .github/dev-config.yml thoughts/shared/plans/my-plan.md"
    exit 0
fi
main "$@"

================
File: scripts/validate-pr.sh
================
#!/bin/bash
# scripts/validate-pr.sh
set -e
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
CONFIG_FILE="${1:-.github/development-pipeline-config.yml}"
PR_NUMBER="${2}"
load_config() {
    if [ -f "$CONFIG_FILE" ]; then
        CONFIG_SOURCE="$CONFIG_FILE"
    else
        CONFIG_SOURCE="$SCRIPT_DIR/../configs/default.yml"
    fi
    # Check if gh is available
    if ! command -v gh >/dev/null 2>&1; then
        echo "❌ GitHub CLI (gh) is required but not installed. Please install gh to use this script."
        exit 1
    fi
}
validate_pr_exists() {
    if ! gh pr view "$PR_NUMBER" >/dev/null 2>&1; then
        echo "❌ PR does not exist: #$PR_NUMBER"
        exit 1
    fi
    echo "✅ PR exists: #$PR_NUMBER"
}
validate_pr_description() {
    local pr_body=$(gh pr view "$PR_NUMBER" --json body --jq '.body')
    # Check for required sections in PR description
    local required_sections=("Summary" "Related Documents" "Testing" "Changes Made")
    for section in "${required_sections[@]}"; do
        if ! echo "$pr_body" | grep -q "$section"; then
            echo "❌ PR description missing section: $section"
            exit 1
        fi
    done
    echo "✅ PR description properly structured"
}
validate_document_links() {
    local pr_body=$(gh pr view "$PR_NUMBER" --json body --jq '.body')
    # Check for links to research and plan documents
    if ! echo "$pr_body" | grep -q "research.*\.md"; then
        echo "❌ PR description missing link to research document"
        exit 1
    fi
    if ! echo "$pr_body" | grep -q "plans.*\.md"; then
        echo "❌ PR description missing link to implementation plan"
        exit 1
    fi
    echo "✅ Document links present in PR description"
}
validate_reviewers_assigned() {
    local reviewers=$(gh pr view "$PR_NUMBER" --json reviewRequests --jq '.reviewRequests | length')
    if [ "$reviewers" -eq 0 ]; then
        echo "❌ No reviewers assigned to PR"
        exit 1
    fi
    echo "✅ Reviewers assigned ($reviewers reviewers)"
}
validate_status_checks() {
    # Check if all required status checks are passing
    local status_checks=$(gh pr view "$PR_NUMBER" --json statusCheckRollup --jq '.statusCheckRollup[]? | select(.conclusion == "FAILURE") | .name' 2>/dev/null || echo "")
    if [ -n "$status_checks" ]; then
        echo "❌ Failing status checks:"
        echo "$status_checks"
        exit 1
    fi
    echo "✅ All status checks passing"
}
validate_labels() {
    local labels=$(gh pr view "$PR_NUMBER" --json labels --jq '.labels | length')
    if [ "$labels" -eq 0 ]; then
        echo "⚠️  No labels assigned to PR (recommended but not required)"
    else
        echo "✅ Labels assigned ($labels labels)"
    fi
}
validate_branch_naming() {
    local head_branch=$(gh pr view "$PR_NUMBER" --json headRefName --jq '.headRefName')
    # Check if branch follows naming convention
    if ! echo "$head_branch" | grep -q "^feature/\|^hotfix/\|^bugfix/"; then
        echo "⚠️  Branch name doesn't follow convention: $head_branch"
        echo "   Expected: feature/, hotfix/, or bugfix/ prefix"
    else
        echo "✅ Branch naming follows convention: $head_branch"
    fi
}
main() {
    echo "🔄 Validating PR: #$PR_NUMBER"
    echo "📝 Using config: $CONFIG_SOURCE"
    load_config
    validate_pr_exists
    validate_pr_description
    validate_document_links
    validate_reviewers_assigned
    validate_status_checks
    validate_labels
    validate_branch_naming
    echo ""
    echo "✅ PR validation PASSED"
    echo "📊 PR statistics:"
    echo "   - Reviewers: $(gh pr view "$PR_NUMBER" --json reviewRequests --jq '.reviewRequests | length')"
    echo "   - Files changed: $(gh pr view "$PR_NUMBER" --json files --jq '.files | length')"
    echo "   - Labels: $(gh pr view "$PR_NUMBER" --json labels --jq '.labels | length')"
    echo "   - Status checks: $(gh pr view "$PR_NUMBER" --json statusCheckRollup --jq '.statusCheckRollup | length' 2>/dev/null || echo "0")"
}
if [ "$1" = "--help" ] || [ "$1" = "-h" ]; then
    echo "Usage: $0 [config_file] [pr_number]"
    echo ""
    echo "Examples:"
    echo "  $0 123"
    echo "  $0 .github/dev-config.yml 123"
    exit 0
fi
main "$@"

================
File: scripts/validate-research-multi.sh
================
#!/bin/bash
# scripts/validate-research-multi.sh - Multi-repo aware version
set -e
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
CONFIG_JSON="${1}"  # Configuration passed as JSON from workflow
RESEARCH_DOC="${2}"
# Parse configuration from JSON or file
parse_config() {
    if [ -z "$CONFIG_JSON" ]; then
        echo "❌ No configuration provided"
        exit 1
    fi
    # Check if it's a file path or JSON
    if [ -f "$CONFIG_JSON" ]; then
        # It's a file path - load and convert to JSON
        if ! command -v yq >/dev/null 2>&1; then
            echo "❌ yq is required but not installed. Please install yq to use this script."
            exit 1
        fi
        CONFIG_JSON=$(yq eval -o=json '.' "$CONFIG_JSON")
    fi
    # Extract validation settings using jq
    if ! command -v jq >/dev/null 2>&1; then
        echo "❌ jq is required but not installed. Please install jq to use this script."
        exit 1
    fi
    MIN_REFS=$(echo "$CONFIG_JSON" | jq -r '.validation.research_min_refs // 3')
    THOUGHTS_DIR=$(echo "$CONFIG_JSON" | jq -r '.thoughts_directory // "thoughts/"')
    REPO_NAME=$(echo "$CONFIG_JSON" | jq -r '.repo_name // "unknown"')
    # Get required sections if specified
    REQUIRED_SECTIONS_JSON=$(echo "$CONFIG_JSON" | jq -r '.validation.research_required_sections // null')
    # Set default required sections if none specified
    if [ "$REQUIRED_SECTIONS_JSON" = "null" ]; then
        REQUIRED_SECTIONS=(
            "## Research Question"
            "## Summary"
            "## Detailed Findings"
            "## Code References"
            "## Architecture Insights"
        )
    else
        # Parse custom required sections from JSON
        mapfile -t REQUIRED_SECTIONS < <(echo "$REQUIRED_SECTIONS_JSON" | jq -r '.[]')
    fi
    echo "📝 Using multi-repo configuration:"
    echo "   Repository: $REPO_NAME"
    echo "   Min file references: $MIN_REFS"
    echo "   Thoughts directory: $THOUGHTS_DIR"
    echo "   Required sections: ${#REQUIRED_SECTIONS[@]} sections"
}
validate_repo_specific_patterns() {
    local doc="$1"
    echo "🔍 Applying repository-specific validation rules..."
    case "$REPO_NAME" in
        "platform-api")
            # Platform API specific validations
            echo "   Checking platform-api specific requirements..."
            if ! grep -q -i "api\|endpoint\|service" "$doc"; then
                echo "⚠️  Platform API research should mention API/service considerations"
            fi
            if ! grep -q -i "security\|auth\|permission" "$doc"; then
                echo "⚠️  Platform API research should consider security implications"
            fi
            if ! grep -q -i "performance\|scale\|load" "$doc"; then
                echo "⚠️  Platform API research should consider performance implications"
            fi
            ;;
        "curatefor.me")
            # curatefor.me specific validations
            echo "   Checking curatefor.me specific requirements..."
            if grep -q -i "hld\|daemon" "$doc" && ! grep -q -i "humanlayer" "$doc"; then
                echo "⚠️  HLD-related research should mention humanlayer context"
            fi
            if grep -q -i "curation\|content" "$doc" && ! grep -q -i "user.*experience\|workflow" "$doc"; then
                echo "⚠️  Content curation research should consider user workflows"
            fi
            ;;
        *)
            echo "   Using generic validation rules for $REPO_NAME"
            ;;
    esac
    echo "✅ Repository-specific validation complete"
}
validate_thoughts_directory_structure() {
    local doc="$1"
    # Ensure document is in correct thoughts directory structure
    EXPECTED_PATH_PREFIX="$THOUGHTS_DIR"
    if [[ ! "$doc" =~ ^$EXPECTED_PATH_PREFIX ]]; then
        echo "❌ Research document not in expected directory structure"
        echo "   Expected: $EXPECTED_PATH_PREFIX*"
        echo "   Actual: $doc"
        exit 1
    fi
    echo "✅ Document in correct directory structure"
}
validate_file_references() {
    local doc="$1"
    # Count file references with pattern: `filename.ext:line`
    local file_refs=$(grep -c '`[^`]*\.[a-z]*:' "$doc" || echo "0")
    if [ "$file_refs" -lt "$MIN_REFS" ]; then
        echo "❌ Insufficient file references ($file_refs found, need $MIN_REFS)"
        echo "   Pattern expected: \`filename.ext:line\`"
        echo "   Repository '$REPO_NAME' requires minimum $MIN_REFS references"
        exit 1
    fi
    echo "✅ File references sufficient ($file_refs found, need $MIN_REFS)"
}
validate_required_sections() {
    local doc="$1"
    echo "🔍 Checking required sections..."
    for section in "${REQUIRED_SECTIONS[@]}"; do
        if ! grep -q "$section" "$doc"; then
            echo "❌ Missing required section: $section"
            exit 1
        fi
        echo "   ✅ Found: $section"
    done
    echo "✅ All required sections present (${#REQUIRED_SECTIONS[@]} sections)"
}
validate_research_quality() {
    local doc="$1"
    echo "🔍 Checking research quality indicators..."
    # Check for substantive content
    local word_count=$(wc -w < "$doc")
    if [ "$word_count" -lt 200 ]; then
        echo "⚠️  Research document seems short ($word_count words). Consider adding more detail."
    else
        echo "✅ Document has substantial content ($word_count words)"
    fi
    # Check for code analysis depth
    local code_analysis_lines=$(grep -c -i "implementation\|pattern\|architecture\|design" "$doc" || echo "0")
    if [ "$code_analysis_lines" -lt 3 ]; then
        echo "⚠️  Limited code analysis found. Consider deeper architectural insights."
    else
        echo "✅ Good code analysis depth ($code_analysis_lines relevant lines)"
    fi
}
main() {
    echo "🔍 Multi-repository research validation"
    echo "📄 Document: $RESEARCH_DOC"
    parse_config
    # Standard validations
    validate_file_exists "$RESEARCH_DOC"
    validate_yaml_frontmatter "$RESEARCH_DOC"
    validate_thoughts_directory_structure "$RESEARCH_DOC"
    validate_required_sections "$RESEARCH_DOC"
    validate_file_references "$RESEARCH_DOC"
    validate_no_placeholders "$RESEARCH_DOC"
    # Quality and repository-specific validations
    validate_research_quality "$RESEARCH_DOC"
    validate_repo_specific_patterns "$RESEARCH_DOC"
    echo ""
    echo "✅ Multi-repository research validation PASSED"
    echo "📊 Document statistics:"
    echo "   - Repository: $REPO_NAME"
    echo "   - File references: $(grep -c '`[^`]*\.[a-z]*:' "$RESEARCH_DOC")"
    echo "   - Word count: $(wc -w < "$RESEARCH_DOC")"
    echo "   - Validation level: $MIN_REFS min refs"
    echo "   - Thoughts directory: $THOUGHTS_DIR"
}
# Include the standard validation functions
validate_file_exists() {
    if [ ! -f "$1" ]; then
        echo "❌ Research document not found: $1"
        exit 1
    fi
    echo "✅ Research document exists: $1"
}
validate_yaml_frontmatter() {
    if ! head -20 "$1" | grep -q "^---"; then
        echo "❌ Missing YAML frontmatter"
        exit 1
    fi
    local required_fields=("date" "researcher" "topic" "status")
    for field in "${required_fields[@]}"; do
        if ! grep -q "^$field:" "$1"; then
            echo "❌ Missing required frontmatter field: $field"
            exit 1
        fi
    done
    echo "✅ YAML frontmatter valid"
}
validate_no_placeholders() {
    local placeholders=("TODO" "FIXME" "XXX" "TBD")
    for placeholder in "${placeholders[@]}"; do
        if grep -q "$placeholder" "$1"; then
            echo "❌ Found placeholder text: $placeholder"
            echo "   Research document appears incomplete"
            exit 1
        fi
    done
    # Check for question marks pattern (???)
    if grep -q "???" "$1"; then
        echo "❌ Found unresolved questions: ???"
        echo "   Research document appears incomplete"
        exit 1
    fi
    echo "✅ No placeholder text found"
}
# Help and usage
if [ "$1" = "--help" ] || [ "$1" = "-h" ]; then
    echo "Usage: $0 [config_json_or_file] [research_document]"
    echo ""
    echo "Multi-repository aware research validation script"
    echo ""
    echo "Configuration can be provided as:"
    echo "  - JSON string: '{\"repo_name\":\"curatefor.me\",\"validation\":{\"research_min_refs\":3}}'"
    echo "  - File path: 'configs/curatefor.me.yml'"
    echo ""
    echo "Examples:"
    echo "  $0 configs/curatefor.me.yml research.md"
    echo "  $0 '{\"repo_name\":\"platform-api\",\"validation\":{\"research_min_refs\":5}}' research.md"
    echo ""
    echo "Environment requirements:"
    echo "  - jq (for JSON processing)"
    echo "  - yq (for YAML processing, if using config files)"
    exit 0
fi
if [ $# -lt 2 ]; then
    echo "❌ Insufficient arguments"
    echo "Usage: $0 [config_json_or_file] [research_document]"
    echo "Use --help for more information"
    exit 1
fi
main "$@"

================
File: scripts/validate-research.sh
================
#!/bin/bash
# scripts/validate-research.sh - Multi-repo aware version
set -e
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
# Parse arguments - supports both JSON config and file config
if [ $# -eq 2 ] && [[ "$1" == *"{"* ]]; then
    # New JSON format from workflow
    CONFIG_JSON="${1}"
    RESEARCH_DOC="${2}"
    CONFIG_MODE="json"
elif [ $# -eq 1 ] && [[ "$1" == *.md ]]; then
    # Single argument - research doc only
    CONFIG_FILE=".github/development-pipeline-config.yml"
    RESEARCH_DOC="$1"
    CONFIG_MODE="file"
elif [ $# -eq 2 ]; then
    # Two arguments - config file and research doc
    CONFIG_FILE="$1"
    RESEARCH_DOC="$2"
    CONFIG_MODE="file"
else
    CONFIG_FILE="${1:-.github/development-pipeline-config.yml}"
    RESEARCH_DOC="${2}"
    CONFIG_MODE="file"
fi
# Parse configuration from JSON or file
parse_config() {
    if [ "$CONFIG_MODE" = "json" ]; then
        if [ -z "$CONFIG_JSON" ]; then
            echo "❌ No configuration provided"
            exit 1
        fi
        # Extract validation settings from JSON
        MIN_REFS=$(echo "$CONFIG_JSON" | jq -r '.validation.research_min_refs // 3')
        THOUGHTS_DIR=$(echo "$CONFIG_JSON" | jq -r '.thoughts_directory // "thoughts/"')
        REPO_NAME=$(echo "$CONFIG_JSON" | jq -r '.repo_name // "unknown"')
        # Extract required sections array
        REQUIRED_SECTIONS_JSON=$(echo "$CONFIG_JSON" | jq -r '.validation.plan_required_sections[]? // empty')
        REQUIRED_SECTIONS=()
        while IFS= read -r line; do
            [ -n "$line" ] && REQUIRED_SECTIONS+=("$line")
        done <<< "$REQUIRED_SECTIONS_JSON"
    else
        # File-based configuration (legacy mode)
        if [ -f "$CONFIG_FILE" ]; then
            CONFIG_SOURCE="$CONFIG_FILE"
        else
            CONFIG_SOURCE="$SCRIPT_DIR/../configs/default.yml"
        fi
        # Check if yq is available
        if ! command -v yq >/dev/null 2>&1; then
            echo "❌ yq is required but not installed. Please install yq to use this script."
            exit 1
        fi
        MIN_REFS=$(yq eval '.validation.research_min_refs // 3' "$CONFIG_SOURCE")
        THOUGHTS_DIR=$(yq eval '.thoughts_directory // "thoughts/"' "$CONFIG_SOURCE")
        REPO_NAME=$(yq eval '.repo_name // "unknown"' "$CONFIG_SOURCE")
        # Get required sections array
        REQUIRED_SECTIONS=()
        while IFS= read -r line; do
            REQUIRED_SECTIONS+=("$line")
        done < <(yq eval '.validation.plan_required_sections[]? // empty' "$CONFIG_SOURCE")
    fi
    # If no custom required sections, use defaults for research
    if [ ${#REQUIRED_SECTIONS[@]} -eq 0 ]; then
        REQUIRED_SECTIONS=(
            "## Research Question"
            "## Summary"
            "## Detailed Findings"
            "## Code References"
            "## Architecture Insights"
        )
    fi
    echo "📝 Using configuration:"
    echo "   Repository: $REPO_NAME"
    echo "   Min file references: $MIN_REFS"
    echo "   Thoughts directory: $THOUGHTS_DIR"
    echo "   Required sections: ${#REQUIRED_SECTIONS[@]} sections"
    echo "   Config mode: $CONFIG_MODE"
}
validate_file_exists() {
    if [ ! -f "$RESEARCH_DOC" ]; then
        echo "❌ Research document not found: $RESEARCH_DOC"
        exit 1
    fi
    echo "✅ Research document exists: $RESEARCH_DOC"
}
validate_yaml_frontmatter() {
    if ! head -20 "$RESEARCH_DOC" | grep -q "^---"; then
        echo "❌ Missing YAML frontmatter"
        exit 1
    fi
    # Check required frontmatter fields (flexible to support different formats)
    local required_fields=("date" "researcher")
    local optional_fields=("topic" "status" "issue_title" "research_type")
    for field in "${required_fields[@]}"; do
        if ! grep -q "^$field:" "$RESEARCH_DOC"; then
            echo "❌ Missing required frontmatter field: $field"
            exit 1
        fi
    done
    # Check that at least one topic-like field exists
    if ! grep -q "^topic:\|^issue_title:\|^research_type:" "$RESEARCH_DOC"; then
        echo "❌ Missing topic information (need one of: topic, issue_title, research_type)"
        exit 1
    fi
    echo "✅ YAML frontmatter valid"
}
validate_required_sections() {
    local required_sections=(
        "## Research Question"
        "## Summary" 
        "## Detailed Findings"
        "## Code References"
        "## Architecture Insights"
    )
    for section in "${required_sections[@]}"; do
        if ! grep -q "$section" "$RESEARCH_DOC"; then
            echo "❌ Missing required section: $section"
            exit 1
        fi
    done
    echo "✅ All required sections present"
}
validate_file_references() {
    # Count file references with pattern: `filename.ext:line`
    local file_refs
    file_refs=$(grep -c '`[^`]*\.[a-z]*:' "$RESEARCH_DOC" 2>/dev/null || echo "0")
    file_refs=$(echo "$file_refs" | head -1 | tr -d '\n\r ')  # Get first line, remove whitespace
    if [ "$file_refs" -lt "$MIN_REFS" ]; then
        echo "❌ Insufficient file references ($file_refs found, need $MIN_REFS)"
        echo "   Pattern expected: \`filename.ext:line\`"
        exit 1
    fi
    echo "✅ File references sufficient ($file_refs found, need $MIN_REFS)"
}
validate_repo_specific_patterns() {
    local doc="$1"
    case "$REPO_NAME" in
        "platform-api")
            echo "🔍 Applying platform-api specific validations..."
            # Platform API specific validations
            if ! grep -q "API" "$doc"; then
                echo "⚠️  Platform API research should mention API considerations"
            fi
            if ! grep -q -i "security\|auth\|permission" "$doc"; then
                echo "⚠️  Platform API research should consider security implications"
            fi
            if ! grep -q -i "performance\|scalability" "$doc"; then
                echo "⚠️  Platform API research should consider performance impact"
            fi
            ;;
        "curatefor.me")
            echo "🔍 Applying curatefor.me specific validations..."
            # curatefor.me specific validations
            if grep -q "hld\|daemon" "$doc" && ! grep -q "humanlayer" "$doc"; then
                echo "⚠️  HLD-related research should mention humanlayer context"
            fi
            if grep -q -i "user.*interface\|frontend" "$doc" && ! grep -q -i "ux\|user.*experience" "$doc"; then
                echo "⚠️  Frontend research should consider user experience"
            fi
            ;;
        *)
            echo "ℹ️  Using generic validation rules for $REPO_NAME"
            ;;
    esac
}
validate_thoughts_directory_structure() {
    local doc="$1"
    # Ensure document is in correct thoughts directory structure
    EXPECTED_PATH_PREFIX="$THOUGHTS_DIR"
    if [[ ! "$doc" =~ ^$EXPECTED_PATH_PREFIX ]]; then
        echo "❌ Research document not in expected directory structure"
        echo "   Expected: $EXPECTED_PATH_PREFIX*"
        echo "   Actual: $doc"
        exit 1
    fi
    echo "✅ Document in correct directory structure"
}
validate_required_sections() {
    local doc="$1"
    for section in "${REQUIRED_SECTIONS[@]}"; do
        if ! grep -q "$section" "$doc"; then
            echo "❌ Missing required section: $section"
            exit 1
        fi
    done
    echo "✅ All required sections present (${#REQUIRED_SECTIONS[@]} sections)"
}
validate_no_placeholders() {
    # Check for common placeholder text (skip YAML frontmatter)
    local placeholders=("TODO" "FIXME" "XXX" "\\.\\.\\.")
    # Skip YAML frontmatter when checking for placeholders
    local content_without_frontmatter
    if head -20 "$RESEARCH_DOC" | grep -q "^---"; then
        # Find the end of frontmatter and get content after it
        content_without_frontmatter=$(awk '/^---$/{if(++c==2) f=1; next} f' "$RESEARCH_DOC")
    else
        content_without_frontmatter=$(cat "$RESEARCH_DOC")
    fi
    for placeholder in "${placeholders[@]}"; do
        if echo "$content_without_frontmatter" | grep -q "$placeholder"; then
            echo "❌ Found placeholder text: $placeholder"
            echo "   Research document appears incomplete"
            exit 1
        fi
    done
    # Check for bracket placeholders like [TODO] or [PLACEHOLDER] but not valid markdown links
    if echo "$content_without_frontmatter" | grep -q "\\[TODO\\]\\|\\[FIXME\\]\\|\\[PLACEHOLDER\\]\\|\\[XXX\\]"; then
        echo "❌ Found bracket placeholder text"
        echo "   Research document appears incomplete"
        exit 1
    fi
    echo "✅ No placeholder text found"
}
main() {
    echo "🔍 Multi-repo research validation"
    echo "📄 Document: $RESEARCH_DOC"
    parse_config
    # Standard validations
    validate_file_exists
    validate_yaml_frontmatter
    validate_thoughts_directory_structure "$RESEARCH_DOC"
    validate_required_sections "$RESEARCH_DOC"
    validate_file_references
    validate_no_placeholders
    # Repository-specific validations
    validate_repo_specific_patterns "$RESEARCH_DOC"
    echo ""
    echo "✅ Multi-repo research validation PASSED"
    echo "📊 Document statistics:"
    echo "   - Repository: $REPO_NAME"
    echo "   - File references: $(grep -c '`[^`]*\.[a-z]*:' "$RESEARCH_DOC")"
    echo "   - Word count: $(wc -w < "$RESEARCH_DOC")"
    echo "   - Validation level: $MIN_REFS min refs"
}
# Usage help
if [ "$1" = "--help" ] || [ "$1" = "-h" ]; then
    echo "Usage: $0 [config_json] [research_document]"
    echo ""
    echo "Examples:"
    echo "  $0 '{\"repo_name\":\"curatefor.me\",\"validation\":{\"research_min_refs\":3}}' research.md"
    echo "  $0 thoughts/shared/research/2025-08-17_my-research.md"
    echo "  $0 .github/dev-config.yml thoughts/shared/research/2025-08-17_my-research.md"
    exit 0
fi
main "$@"

================
File: templates/atriumn-pipeline.yml
================
name: Atriumn Pipeline (Issue Comment)
on:
  issue_comment:
    types: [created]
permissions:
  contents: write
  issues: write
  pull-requests: write
  actions: read
  id-token: write
jobs:
  run-research:
    if: contains(github.event.comment.body, '/atriumn-research')
    uses: atriumn/atriumn-issue-driven-development/.github/workflows/development-pipeline.yml@main
    secrets: inherit
    with:
      repo_name: ${{ github.repository }}
      issue_number: ${{ github.event.issue.number }}
      feature_ref: feature/issue-${{ github.event.issue.number }}
      action: run
      phase: research
      task_description: ${{ github.event.issue.title }} — ${{ github.event.issue.body }}
      trigger_comment: /atriumn-research
  run-plan:
    if: contains(github.event.comment.body, '/atriumn-plan')
    uses: atriumn/atriumn-issue-driven-development/.github/workflows/development-pipeline.yml@main
    secrets: inherit
    with:
      repo_name: ${{ github.repository }}
      issue_number: ${{ github.event.issue.number }}
      feature_ref: feature/issue-${{ github.event.issue.number }}
      action: run
      phase: plan
      task_description: ${{ github.event.issue.title }} — ${{ github.event.issue.body }}
      trigger_comment: /atriumn-plan
  run-implement:
    if: contains(github.event.comment.body, '/atriumn-implement')
    uses: atriumn/atriumn-issue-driven-development/.github/workflows/development-pipeline.yml@main
    secrets: inherit
    with:
      repo_name: ${{ github.repository }}
      issue_number: ${{ github.event.issue.number }}
      feature_ref: feature/issue-${{ github.event.issue.number }}
      action: run
      phase: implement
      task_description: ${{ github.event.issue.title }} — ${{ github.event.issue.body }}
      trigger_comment: /atriumn-implement
  run-validate:
    if: contains(github.event.comment.body, '/atriumn-validate')
    uses: atriumn/atriumn-issue-driven-development/.github/workflows/development-pipeline.yml@main
    secrets: inherit
    with:
      repo_name: ${{ github.repository }}
      issue_number: ${{ github.event.issue.number }}
      feature_ref: feature/issue-${{ github.event.issue.number }}
      action: run
      phase: validate
      task_description: ${{ github.event.issue.title }} — ${{ github.event.issue.body }}
      trigger_comment: /atriumn-validate

================
File: templates/decision-record-template.md
================
# Pipeline Decision Record - Issue #{issue_number}: {issue_title}

## Issue Context
- **Issue**: #{issue_number}
- **Title**: {issue_title}
- **Branch**: {branch_name}
- **Started**: {start_date}
- **Repository**: {repo_name}

## Current Status
- **Phase**: {current_phase}
- **Completion**: {completion_percentage}%
- **Next Action**: {next_action}

---

## Research Phase
- **Status**: {research_status}
- **Started**: {research_start_date}
- **Completed**: {research_completion_date}

### Key Findings
- {research_finding_1}
- {research_finding_2}
- {research_finding_3}

### Architecture Decisions
- **Decision**: {architecture_decision}
- **Rationale**: {decision_rationale}
- **Impact**: {decision_impact}

---

## Planning Phase
- **Status**: {planning_status}
- **Started**: {planning_start_date}
- **Completed**: {planning_completion_date}

### Implementation Approach
- **Strategy**: {implementation_strategy}
- **Timeline**: {estimated_timeline}
- **Phases**: {number_of_phases}

### Scope Decisions
- **Include**: {scope_include}
- **Exclude**: {scope_exclude}
- **Future Work**: {future_work}

---

## Implementation Phase
- **Status**: {implementation_status}
- **Started**: {implementation_start_date}
- **Completion**: {implementation_completion}%

### Phase Progress
- **Phase 1**: {phase_1_status} - {phase_1_description}
- **Phase 2**: {phase_2_status} - {phase_2_description}
- **Phase 3**: {phase_3_status} - {phase_3_description}

### Files Modified
- {modified_file_1}
- {modified_file_2}
- {modified_file_3}

### Implementation Decisions
- **Decision**: {implementation_decision}
- **Context**: {implementation_context}
- **Impact**: {implementation_impact}

---

## PR Phase
- **Status**: {pr_status}
- **PR Number**: #{pr_number}
- **Created**: {pr_creation_date}

### PR Details
- **Reviewers**: {pr_reviewers}
- **Status Checks**: {status_checks_status}
- **Merge Status**: {merge_status}

---

## Lessons Learned
- {lesson_1}
- {lesson_2}
- {lesson_3}

## Future Reference
- {future_reference_1}
- {future_reference_2}

================
File: templates/enhanced-repo-workflow-template.yml
================
# Copy this to your repo: .github/workflows/development-pipeline-enhanced.yml
# Enhanced Development Pipeline with Branch Safety & Context Preservation
name: Enhanced Development Pipeline
on:
  issue_comment:
    types: [created]
  pull_request:
    types: [opened, closed]
env:
  GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
jobs:
  # Load configuration for all jobs
  load-config:
    runs-on: ubuntu-latest
    outputs:
      base_branch: ${{ steps.config.outputs.base_branch }}
      thoughts_dir: ${{ steps.config.outputs.thoughts_dir }}
      config_loaded: ${{ steps.config.outputs.config_loaded }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      - name: Checkout shared workflows
        uses: actions/checkout@v4
        with:
          repository: atriumn/atriumn-shared-workflows
          path: .github/shared-workflows
      - name: Install dependencies
        run: |
          sudo wget -qO /usr/local/bin/yq https://github.com/mikefarah/yq/releases/latest/download/yq_linux_amd64
          sudo chmod +x /usr/local/bin/yq
          sudo apt-get update && sudo apt-get install -y gh
      - name: Load pipeline configuration
        id: config
        run: |
          # Load from repo-specific config or use defaults
          if [ -f ".github/development-pipeline-config.yml" ]; then
            CONFIG_FILE=".github/development-pipeline-config.yml"
          else
            CONFIG_FILE=".github/shared-workflows/configs/default.yml"
          fi
          BASE_BRANCH=$(yq eval '.base_branch' "$CONFIG_FILE")
          THOUGHTS_DIR=$(yq eval '.thoughts_directory' "$CONFIG_FILE")
          echo "base_branch=$BASE_BRANCH" >> $GITHUB_OUTPUT
          echo "thoughts_dir=$THOUGHTS_DIR" >> $GITHUB_OUTPUT
          echo "config_loaded=true" >> $GITHUB_OUTPUT
          echo "✅ Configuration loaded: base=$BASE_BRANCH, thoughts=$THOUGHTS_DIR"
  # Task 1: Enhanced Branch Management - Branch Safety Checks
  validate-branch-continuity:
    if: github.event_name == 'issue_comment'
    needs: load-config
    runs-on: ubuntu-latest
    outputs:
      branch_name: ${{ steps.branch-check.outputs.expected_branch }}
      branch_valid: ${{ steps.branch-state.outputs.branch_valid }}
      commits_ahead: ${{ steps.branch-state.outputs.commits_ahead }}
      commits_behind: ${{ steps.branch-state.outputs.commits_behind }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - name: Ensure Pipeline Branch Exists
        id: branch-check
        run: |
          # Get expected branch from issue labels
          EXPECTED_BRANCH=$(gh issue view ${{ github.event.issue.number }} --json labels --jq '.labels[] | select(.name | startswith("branch:")) | .name | sub("branch:"; "")')
          if [ -z "$EXPECTED_BRANCH" ]; then
            echo "❌ No pipeline branch found in issue labels"
            echo "This indicates the pipeline was not properly started."
            gh issue comment ${{ github.event.issue.number }} --body "
            ❌ **Pipeline Branch Error**
            No pipeline branch found for this issue. This usually means:
            - Pipeline was not started with '@claude run development pipeline'
            - Issue labels were modified incorrectly
            - Pipeline initialization failed
            **To Fix**: Start the pipeline with '@claude run development pipeline'
            "
            exit 1
          fi
          echo "expected_branch=$EXPECTED_BRANCH" >> $GITHUB_OUTPUT
          echo "✅ Expected pipeline branch: $EXPECTED_BRANCH"
      - name: Validate Branch Exists in Repository
        run: |
          BRANCH_NAME="${{ steps.branch-check.outputs.expected_branch }}"
          # Check if branch exists in remote
          if ! git ls-remote --heads origin "$BRANCH_NAME" | grep -q "$BRANCH_NAME"; then
            echo "❌ Pipeline branch does not exist in repository: $BRANCH_NAME"
            gh issue comment ${{ github.event.issue.number }} --body "
            ❌ **Missing Pipeline Branch**
            Expected branch \`$BRANCH_NAME\` does not exist in repository.
            **Possible Causes:**
            - Branch was deleted manually
            - Pipeline initialization failed
            - Branch name mismatch
            **To Fix**: Restart the pipeline to recreate the branch.
            "
            exit 1
          fi
          echo "✅ Pipeline branch exists: $BRANCH_NAME"
      - name: Check Branch State
        id: branch-state
        run: |
          BRANCH_NAME="${{ steps.branch-check.outputs.expected_branch }}"
          BASE_BRANCH="${{ needs.load-config.outputs.base_branch }}"
          # Get branch info
          git fetch origin "$BRANCH_NAME"
          git checkout "$BRANCH_NAME"
          # Check if branch is ahead of base
          COMMITS_AHEAD=$(git rev-list --count HEAD ^origin/$BASE_BRANCH)
          COMMITS_BEHIND=$(git rev-list --count origin/$BASE_BRANCH ^HEAD)
          echo "commits_ahead=$COMMITS_AHEAD" >> $GITHUB_OUTPUT
          echo "commits_behind=$COMMITS_BEHIND" >> $GITHUB_OUTPUT
          echo "branch_valid=true" >> $GITHUB_OUTPUT
          if [ "$COMMITS_AHEAD" -eq 0 ]; then
            echo "⚠️ Branch has no commits ahead of $BASE_BRANCH"
          fi
          if [ "$COMMITS_BEHIND" -gt 0 ]; then
            echo "⚠️ Branch is $COMMITS_BEHIND commits behind $BASE_BRANCH"
          fi
          echo "✅ Branch state: +$COMMITS_AHEAD/-$COMMITS_BEHIND vs $BASE_BRANCH"
  # Task 2: Context Validation Between Phases
  validate-context-continuity:
    if: github.event_name == 'issue_comment'
    needs: [load-config, validate-branch-continuity]
    runs-on: ubuntu-latest
    outputs:
      decision_record_path: ${{ steps.context.outputs.decision_record_path }}
      research_doc_path: ${{ steps.context.outputs.research_doc_path }}
      plan_doc_path: ${{ steps.context.outputs.plan_doc_path }}
      current_phase: ${{ steps.context.outputs.current_phase }}
      context_valid: ${{ steps.validate-phase-context.outputs.context_valid }}
    steps:
      - name: Checkout target repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - name: Get pipeline branch
        id: branch
        run: |
          BRANCH_NAME="${{ needs.validate-branch-continuity.outputs.branch_name }}"
          echo "name=$BRANCH_NAME" >> $GITHUB_OUTPUT
          git checkout "$BRANCH_NAME"
          git pull origin "$BRANCH_NAME"
      - name: Validate Decision Record Context
        id: context
        run: |
          THOUGHTS_DIR="${{ needs.load-config.outputs.thoughts_dir }}"
          DECISION_RECORD="$THOUGHTS_DIR/shared/decisions/pipeline-issue-${{ github.event.issue.number }}.md"
          # Check decision record exists
          if [ ! -f "$DECISION_RECORD" ]; then
            echo "❌ Decision record missing: $DECISION_RECORD"
            gh issue comment ${{ github.event.issue.number }} --body "
            ❌ **Context Error: Missing Decision Record**
            The pipeline decision record is missing: \`$DECISION_RECORD\`
            This indicates a serious pipeline integrity issue. The decision record is required for maintaining context between phases.
            **Action Required**: Restart the pipeline from the beginning.
            "
            exit 1
          fi
          echo "decision_record_path=$DECISION_RECORD" >> $GITHUB_OUTPUT
          echo "✅ Decision record exists: $DECISION_RECORD"
          # Determine current phase based on decision record content
          if grep -q "## Implementation Phase (Complete" "$DECISION_RECORD"; then
            CURRENT_PHASE="pr-creation"
          elif grep -q "## Planning Phase (Complete" "$DECISION_RECORD"; then
            CURRENT_PHASE="implementation"
          elif grep -q "## Research Phase (Complete" "$DECISION_RECORD"; then
            CURRENT_PHASE="planning"
          else
            CURRENT_PHASE="research"
          fi
          echo "current_phase=$CURRENT_PHASE" >> $GITHUB_OUTPUT
          echo "✅ Current phase: $CURRENT_PHASE"
      - name: Validate Phase-Specific Context
        id: validate-phase-context
        run: |
          THOUGHTS_DIR="${{ needs.load-config.outputs.thoughts_dir }}"
          CURRENT_PHASE="${{ steps.context.outputs.current_phase }}"
          case "$CURRENT_PHASE" in
            "planning")
              # Planning phase needs research document
              RESEARCH_DOC=$(find "$THOUGHTS_DIR/shared/research" -name "*issue-${{ github.event.issue.number }}*" -type f | head -1)
              if [ -z "$RESEARCH_DOC" ]; then
                echo "❌ Planning phase requires research document"
                gh issue comment ${{ github.event.issue.number }} --body "
                ❌ **Context Error: Missing Research**
                Planning phase requires a completed research document, but none was found.
                **Expected Path**: \`$THOUGHTS_DIR/shared/research/*issue-${{ github.event.issue.number }}*.md\`
                **Action Required**: Complete the research phase first.
                "
                exit 1
              fi
              echo "research_doc_path=$RESEARCH_DOC" >> $GITHUB_OUTPUT
              echo "✅ Research document found: $RESEARCH_DOC"
              ;;
            "implementation")
              # Implementation phase needs both research and plan
              RESEARCH_DOC=$(find "$THOUGHTS_DIR/shared/research" -name "*issue-${{ github.event.issue.number }}*" -type f | head -1)
              PLAN_DOC=$(find "$THOUGHTS_DIR/shared/plans" -name "*issue-${{ github.event.issue.number }}*" -type f | head -1)
              if [ -z "$RESEARCH_DOC" ] || [ -z "$PLAN_DOC" ]; then
                echo "❌ Implementation phase requires both research and plan documents"
                gh issue comment ${{ github.event.issue.number }} --body "
                ❌ **Context Error: Missing Prerequisites**
                Implementation phase requires both research and plan documents:
                - Research: $([ -n "$RESEARCH_DOC" ] && echo "✅ Found" || echo "❌ Missing")
                - Plan: $([ -n "$PLAN_DOC" ] && echo "✅ Found" || echo "❌ Missing")
                **Action Required**: Complete research and planning phases first.
                "
                exit 1
              fi
              echo "research_doc_path=$RESEARCH_DOC" >> $GITHUB_OUTPUT
              echo "plan_doc_path=$PLAN_DOC" >> $GITHUB_OUTPUT
              echo "✅ All required documents found for implementation"
              ;;
            "pr-creation")
              # PR creation needs all documents
              RESEARCH_DOC=$(find "$THOUGHTS_DIR/shared/research" -name "*issue-${{ github.event.issue.number }}*" -type f | head -1)
              PLAN_DOC=$(find "$THOUGHTS_DIR/shared/plans" -name "*issue-${{ github.event.issue.number }}*" -type f | head -1)
              if [ -z "$RESEARCH_DOC" ] || [ -z "$PLAN_DOC" ]; then
                echo "❌ PR creation requires all pipeline documents"
                exit 1
              fi
              echo "research_doc_path=$RESEARCH_DOC" >> $GITHUB_OUTPUT
              echo "plan_doc_path=$PLAN_DOC" >> $GITHUB_OUTPUT
              echo "✅ All required documents found for PR creation"
              ;;
          esac
          echo "context_valid=true" >> $GITHUB_OUTPUT
      - name: Validate Document Integrity
        run: |
          DECISION_RECORD="${{ steps.context.outputs.decision_record_path }}"
          # Check decision record has required structure
          REQUIRED_SECTIONS=("## Issue Context" "## Current Status")
          for section in "${REQUIRED_SECTIONS[@]}"; do
            if ! grep -q "$section" "$DECISION_RECORD"; then
              echo "❌ Decision record missing section: $section"
              gh issue comment ${{ github.event.issue.number }} --body "
              ❌ **Context Error: Malformed Decision Record**
              Decision record is missing required section: \`$section\`
              This indicates the decision record was corrupted or manually edited incorrectly.
              **Action Required**: Review and fix the decision record, or restart the pipeline.
              "
              exit 1
            fi
          done
          echo "✅ Decision record structure validated"
  # Parse comment triggers with enhanced context
  parse-comment:
    if: github.event_name == 'issue_comment'
    needs: [load-config, validate-branch-continuity, validate-context-continuity]
    runs-on: ubuntu-latest
    outputs:
      trigger: ${{ steps.parse.outputs.trigger }}
      comment_body: ${{ steps.parse.outputs.comment_body }}
      issue_number: ${{ steps.parse.outputs.issue_number }}
      action_valid: ${{ steps.parse.outputs.action_valid }}
    steps:
      - name: Parse comment for triggers
        id: parse
        run: |
          COMMENT_BODY="${{ github.event.comment.body }}"
          ISSUE_NUMBER="${{ github.event.issue.number }}"
          echo "comment_body=$COMMENT_BODY" >> $GITHUB_OUTPUT
          echo "issue_number=$ISSUE_NUMBER" >> $GITHUB_OUTPUT
          # Enhanced trigger detection with context validation
          if echo "$COMMENT_BODY" | grep -q "✅ Research Phase Complete"; then
            echo "trigger=validate-research" >> $GITHUB_OUTPUT
          elif echo "$COMMENT_BODY" | grep -q "✅ Planning Phase Complete"; then
            echo "trigger=validate-plan" >> $GITHUB_OUTPUT
          elif echo "$COMMENT_BODY" | grep -q "✅ Implementation Phase Complete"; then
            echo "trigger=validate-implementation" >> $GITHUB_OUTPUT
          elif echo "$COMMENT_BODY" | grep -q "@claude run development pipeline"; then
            echo "trigger=start-pipeline" >> $GITHUB_OUTPUT
          elif echo "$COMMENT_BODY" | grep -q "@claude retry research"; then
            echo "trigger=retry-research" >> $GITHUB_OUTPUT
          elif echo "$COMMENT_BODY" | grep -q "@claude retry planning"; then
            echo "trigger=retry-planning" >> $GITHUB_OUTPUT
          elif echo "$COMMENT_BODY" | grep -q "@claude retry implementation"; then
            echo "trigger=retry-implementation" >> $GITHUB_OUTPUT
          elif echo "$COMMENT_BODY" | grep -q "@claude restart pipeline"; then
            echo "trigger=restart-pipeline" >> $GITHUB_OUTPUT
          elif echo "$COMMENT_BODY" | grep -q "continue implementing remaining items"; then
            echo "trigger=continue-partial-implementation" >> $GITHUB_OUTPUT
          elif echo "$COMMENT_BODY" | grep -q "accept current implementation"; then
            echo "trigger=accept-partial-implementation" >> $GITHUB_OUTPUT
          elif echo "$COMMENT_BODY" | grep -q "modify implementation:"; then
            echo "trigger=modify-implementation-scope" >> $GITHUB_OUTPUT
          else
            echo "trigger=none" >> $GITHUB_OUTPUT
          fi
          echo "action_valid=true" >> $GITHUB_OUTPUT
  # Enhanced start pipeline with decision record creation
  start-pipeline:
    if: needs.parse-comment.outputs.trigger == 'start-pipeline'
    needs: [load-config, parse-comment]
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - name: Create pipeline branch and decision record
        run: |
          ISSUE_NUMBER="${{ needs.parse-comment.outputs.issue_number }}"
          ISSUE_TITLE=$(gh issue view $ISSUE_NUMBER --json title --jq '.title')
          ISSUE_BODY=$(gh issue view $ISSUE_NUMBER --json body --jq '.body')
          TITLE_SLUG=$(echo "$ISSUE_TITLE" | sed 's/[^a-zA-Z0-9]/-/g' | tr '[:upper:]' '[:lower:]' | sed 's/--*/-/g' | sed 's/^-\|-$//g')
          BRANCH_NAME="feature/issue-$ISSUE_NUMBER-$TITLE_SLUG"
          THOUGHTS_DIR="${{ needs.load-config.outputs.thoughts_dir }}"
          # Create branch if it doesn't exist
          if ! git rev-parse --verify "origin/$BRANCH_NAME" >/dev/null 2>&1; then
            git checkout -b "$BRANCH_NAME"
            # Create directory structure
            mkdir -p "$THOUGHTS_DIR/shared/research"
            mkdir -p "$THOUGHTS_DIR/shared/plans"
            mkdir -p "$THOUGHTS_DIR/shared/decisions"
            # Create initial decision record
            DECISION_FILE="$THOUGHTS_DIR/shared/decisions/pipeline-issue-$ISSUE_NUMBER.md"
            cat > "$DECISION_FILE" << EOF
          # Development Pipeline Decision Record - Issue #$ISSUE_NUMBER
          ## Issue Context
          - **Issue Title**: $ISSUE_TITLE
          - **Issue Number**: #$ISSUE_NUMBER
          - **Pipeline Branch**: \`$BRANCH_NAME\`
          - **Base Branch**: \`${{ needs.load-config.outputs.base_branch }}\`
          - **Started**: $(date -Iseconds)
          - **Requestor**: @${{ github.event.comment.user.login }}
          ## Original Issue Description
          $ISSUE_BODY
          ## Current Status
          - **Phase**: Research (Starting)
          - **Branch State**: Clean, created from ${{ needs.load-config.outputs.base_branch }}
          - **Context Validation**: ✅ Passed
          - **Decision Record**: ✅ Created
          ## Research Phase (Starting)
          - **Status**: Awaiting research document creation
          - **Expected Path**: \`$THOUGHTS_DIR/shared/research/research-issue-$ISSUE_NUMBER.md\`
          - **Validation Trigger**: Comment "✅ Research Phase Complete" when done
          - **Started**: $(date -Iseconds)
          ## Pipeline Progress
          - [❌] Research Phase
          - [❌] Planning Phase  
          - [❌] Implementation Phase
          - [❌] PR Creation
          ---
          *This decision record tracks the complete development pipeline lifecycle for issue #$ISSUE_NUMBER*
          EOF
            git add .
            git commit -m "Pipeline: Initialize development pipeline for issue #$ISSUE_NUMBER
          - Created pipeline branch: $BRANCH_NAME
          - Initialized decision record
          - Set up directory structure
          - Ready for research phase"
            git push origin "$BRANCH_NAME"
            echo "✅ Created branch: $BRANCH_NAME"
          fi
          # Add branch label
          gh label create "branch:$BRANCH_NAME" --color "0366d6" --description "Pipeline branch" || true
          gh issue edit $ISSUE_NUMBER --add-label "branch:$BRANCH_NAME"
          gh issue comment $ISSUE_NUMBER --body "
          🚀 **Development Pipeline Started**
          **Branch**: \`$BRANCH_NAME\` (created from \`${{ needs.load-config.outputs.base_branch }}\`)
          **Decision Record**: \`$THOUGHTS_DIR/shared/decisions/pipeline-issue-$ISSUE_NUMBER.md\`
          **Next Step**: Please begin research following the methodology
          **Research Requirements:**
          - Create research document: \`$THOUGHTS_DIR/shared/research/research-issue-$ISSUE_NUMBER.md\`
          - Use YAML frontmatter with metadata
          - Include code references with file:line format
          - Follow research methodology guidelines
          - Comment \`✅ Research Phase Complete\` when done
          **Context Preservation:**
          - All pipeline context is tracked in the decision record
          - Branch state and progress are automatically monitored
          - Error recovery mechanisms are active
          "
  # Enhanced validation with context checks
  validate-research:
    if: needs.parse-comment.outputs.trigger == 'validate-research'
    needs: [load-config, validate-branch-continuity, validate-context-continuity, parse-comment]
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - name: Checkout shared workflows
        uses: actions/checkout@v4
        with:
          repository: atriumn/atriumn-shared-workflows
          path: .github/shared-workflows
      - name: Install dependencies
        run: |
          sudo wget -qO /usr/local/bin/yq https://github.com/mikefarah/yq/releases/latest/download/yq_linux_amd64
          sudo chmod +x /usr/local/bin/yq
      - name: Switch to pipeline branch
        run: |
          BRANCH_NAME="${{ needs.validate-branch-continuity.outputs.branch_name }}"
          git checkout "$BRANCH_NAME"
          git pull origin "$BRANCH_NAME"
      - name: Validate research with context
        run: |
          ISSUE_NUMBER="${{ needs.parse-comment.outputs.issue_number }}"
          THOUGHTS_DIR="${{ needs.load-config.outputs.thoughts_dir }}"
          # Find research document
          RESEARCH_DOC=$(find "$THOUGHTS_DIR/shared/research" -name "*issue-$ISSUE_NUMBER*" -type f | head -1)
          if [ -z "$RESEARCH_DOC" ]; then
            gh issue comment $ISSUE_NUMBER --body "❌ **Research Validation Failed**: No research document found for issue #$ISSUE_NUMBER"
            exit 1
          fi
          # Run validation with enhanced context
          if ./.github/shared-workflows/scripts/validate-research.sh "" "$RESEARCH_DOC"; then
            # Update decision record
            DECISION_RECORD="$THOUGHTS_DIR/shared/decisions/pipeline-issue-$ISSUE_NUMBER.md"
            cat >> "$DECISION_RECORD" << EOF
          ## Research Phase (Complete ✅)
          - **Status**: Validation passed
          - **Document**: \`$RESEARCH_DOC\`
          - **Validated**: $(date -Iseconds)
          - **Validator**: GitHub Actions
          - **Next Phase**: Planning
          ## Planning Phase (Starting)
          - **Status**: Awaiting implementation plan
          - **Expected Path**: \`$THOUGHTS_DIR/shared/plans/plan-issue-$ISSUE_NUMBER.md\`
          - **Validation Trigger**: Comment "✅ Planning Phase Complete" when done
          - **Started**: $(date -Iseconds)
          ## Pipeline Progress
          - [✅] Research Phase
          - [❌] Planning Phase  
          - [❌] Implementation Phase
          - [❌] PR Creation
          EOF
            git add "$DECISION_RECORD"
            git commit -m "Pipeline: Research phase complete for issue #$ISSUE_NUMBER"
            git push origin "$BRANCH_NAME"
            gh issue comment $ISSUE_NUMBER --body "
            ✅ **Research Validation Passed**
            **Document**: \`$RESEARCH_DOC\`
            **Decision Record Updated**: \`$DECISION_RECORD\`
            **Next Step**: Create implementation plan
            - Expected path: \`$THOUGHTS_DIR/shared/plans/plan-issue-$ISSUE_NUMBER.md\`
            - Comment \`✅ Planning Phase Complete\` when done
            "
          else
            gh issue comment $ISSUE_NUMBER --body "❌ **Research Validation Failed**: Please review document structure and content"
            exit 1
          fi
  # Task 4: Partial Completion Detection
  detect-partial-completion:
    if: needs.parse-comment.outputs.trigger == 'validate-implementation'
    needs: [load-config, validate-branch-continuity, validate-context-continuity, parse-comment]
    runs-on: ubuntu-latest
    outputs:
      completion_percentage: ${{ steps.analyze.outputs.completion_percentage }}
      completed_items: ${{ steps.analyze.outputs.completed_items }}
      remaining_items: ${{ steps.analyze.outputs.remaining_items }}
      total_phases: ${{ steps.analyze.outputs.total_phases }}
      completed_phases: ${{ steps.analyze.outputs.completed_phases }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - name: Switch to pipeline branch
        run: |
          BRANCH_NAME="${{ needs.validate-branch-continuity.outputs.branch_name }}"
          git checkout "$BRANCH_NAME"
          git pull origin "$BRANCH_NAME"
      - name: Analyze Implementation Completion
        id: analyze
        run: |
          THOUGHTS_DIR="${{ needs.load-config.outputs.thoughts_dir }}"
          PLAN_DOC="${{ needs.validate-context-continuity.outputs.plan_doc_path }}"
          BASE_BRANCH="${{ needs.load-config.outputs.base_branch }}"
          if [ -z "$PLAN_DOC" ] || [ ! -f "$PLAN_DOC" ]; then
            echo "No plan document found - cannot analyze completion"
            exit 1
          fi
          # Extract phases from plan
          TOTAL_PHASES=$(grep -c "^## Phase [0-9]" "$PLAN_DOC")
          echo "total_phases=$TOTAL_PHASES" >> $GITHUB_OUTPUT
          # Check which phases are complete based on commits and files
          COMPLETED_PHASES=0
          COMPLETED_ITEMS=""
          REMAINING_ITEMS=""
          echo "📊 Analyzing $TOTAL_PHASES phases for completion..."
          # Analyze each phase
          for i in $(seq 1 $TOTAL_PHASES); do
            echo "🔍 Analyzing Phase $i..."
            # Get phase description
            PHASE_SECTION=$(sed -n "/^## Phase $i:/,/^## Phase $((i+1)):/p" "$PLAN_DOC" | head -n -1)
            if [ -z "$PHASE_SECTION" ]; then
              PHASE_SECTION=$(sed -n "/^## Phase $i:/,\$p" "$PLAN_DOC")
            fi
            PHASE_TITLE=$(echo "$PHASE_SECTION" | head -1 | sed "s/^## Phase $i: *//")
            # Look for files that should be modified in this phase
            PHASE_FILES=$(echo "$PHASE_SECTION" | grep -o '\`[^`]*\.[a-z]*\`' | tr -d '`' | sort -u || echo "")
            PHASE_COMPLETE=true
            MISSING_FILES=""
            if [ -n "$PHASE_FILES" ]; then
              for file in $PHASE_FILES; do
                if [ -f "$file" ]; then
                  # Check if file was modified in this branch
                  if git diff --name-only "origin/$BASE_BRANCH" | grep -q "^$file$"; then
                    echo "  ✅ $file - modified"
                  else
                    echo "  ⚠️  $file - exists but not modified in this branch"
                    # Consider it incomplete if not modified
                    PHASE_COMPLETE=false
                    MISSING_FILES="$MISSING_FILES$file "
                  fi
                else
                  echo "  ❌ $file - missing"
                  PHASE_COMPLETE=false
                  MISSING_FILES="$MISSING_FILES$file "
                fi
              done
            else
              # No specific files mentioned, check for any changes related to phase
              PHASE_KEYWORDS=$(echo "$PHASE_SECTION" | tr '[:upper:]' '[:lower:]' | grep -o '\b[a-z]\{4,\}\b' | head -5 | tr '\n' '|' | sed 's/|$//')
              if [ -n "$PHASE_KEYWORDS" ]; then
                # Check if any commits mention phase-related keywords
                PHASE_COMMITS=$(git log --oneline "origin/$BASE_BRANCH".."HEAD" | grep -iE "$PHASE_KEYWORDS" || echo "")
                if [ -z "$PHASE_COMMITS" ]; then
                  PHASE_COMPLETE=false
                  echo "  ❌ No commits found related to: $PHASE_KEYWORDS"
                else
                  echo "  ✅ Found commits related to phase keywords"
                fi
              else
                echo "  ⚠️  Cannot determine completion - no files or keywords identified"
                PHASE_COMPLETE=false
              fi
            fi
            if [ "$PHASE_COMPLETE" = true ]; then
              COMPLETED_PHASES=$((COMPLETED_PHASES + 1))
              COMPLETED_ITEMS="$COMPLETED_ITEMS- ✅ Phase $i: $PHASE_TITLE\n"
              echo "  ✅ Phase $i complete"
            else
              REMAINING_ITEMS="$REMAINING_ITEMS- ❌ Phase $i: $PHASE_TITLE"
              if [ -n "$MISSING_FILES" ]; then
                REMAINING_ITEMS="$REMAINING_ITEMS (missing: $MISSING_FILES)"
              fi
              REMAINING_ITEMS="$REMAINING_ITEMS\n"
              echo "  ❌ Phase $i incomplete"
            fi
          done
          # Calculate completion percentage
          if [ $TOTAL_PHASES -gt 0 ]; then
            COMPLETION_PCT=$((COMPLETED_PHASES * 100 / TOTAL_PHASES))
          else
            COMPLETION_PCT=0
          fi
          echo "completion_percentage=$COMPLETION_PCT" >> $GITHUB_OUTPUT
          echo "completed_phases=$COMPLETED_PHASES" >> $GITHUB_OUTPUT
          echo "completed_items=$COMPLETED_ITEMS" >> $GITHUB_OUTPUT
          echo "remaining_items=$REMAINING_ITEMS" >> $GITHUB_OUTPUT
          echo "📊 Analysis complete: $COMPLETED_PHASES/$TOTAL_PHASES phases ($COMPLETION_PCT%)"
  # Handle partial completion scenarios
  handle-partial-completion:
    needs: [load-config, validate-branch-continuity, validate-context-continuity, parse-comment, detect-partial-completion]
    if: needs.detect-partial-completion.outputs.completion_percentage != '100' && needs.detect-partial-completion.outputs.completion_percentage != '0'
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - name: Switch to pipeline branch
        run: |
          BRANCH_NAME="${{ needs.validate-branch-continuity.outputs.branch_name }}"
          git checkout "$BRANCH_NAME"
          git pull origin "$BRANCH_NAME"
      - name: Update Decision Record with Partial Status
        run: |
          THOUGHTS_DIR="${{ needs.load-config.outputs.thoughts_dir }}"
          DECISION_FILE="$THOUGHTS_DIR/shared/decisions/pipeline-issue-${{ needs.parse-comment.outputs.issue_number }}.md"
          COMPLETION="${{ needs.detect-partial-completion.outputs.completion_percentage }}"
          cat >> "$DECISION_FILE" << EOF
          ### Implementation Status Update - Partial Completion
          - **Date**: $(date -Iseconds)
          - **Completion**: ${COMPLETION}%
          - **Status**: Partial implementation detected
          - **Completed Phases**: ${{ needs.detect-partial-completion.outputs.completed_phases }}/${{ needs.detect-partial-completion.outputs.total_phases }}
          #### Completed Items:
          ${{ needs.detect-partial-completion.outputs.completed_items }}
          #### Remaining Items:
          ${{ needs.detect-partial-completion.outputs.remaining_items }}
          #### Next Action Required:
          Human decision needed on how to proceed with remaining $((100 - ${COMPLETION}))% of implementation.
          EOF
          git add "$DECISION_FILE"
          git commit -m "Pipeline: Implementation partial completion status for issue #${{ needs.parse-comment.outputs.issue_number }}"
          git push origin "$BRANCH_NAME"
      - name: Request Human Decision on Partial Completion
        run: |
          COMPLETION="${{ needs.detect-partial-completion.outputs.completion_percentage }}"
          REMAINING=$((100 - COMPLETION))
          BRANCH_NAME="${{ needs.validate-branch-continuity.outputs.branch_name }}"
          gh issue comment ${{ needs.parse-comment.outputs.issue_number }} --body "
          ⚠️ **Partial Implementation Detected**
          **Completion Status**: ${COMPLETION}% complete
          **Remaining Work**: ${REMAINING}% of planned implementation
          **Completed Phases**: ${{ needs.detect-partial-completion.outputs.completed_phases }}/${{ needs.detect-partial-completion.outputs.total_phases }}
          **Completed Items:**
          ${{ needs.detect-partial-completion.outputs.completed_items }}
          **Remaining Items:**
          ${{ needs.detect-partial-completion.outputs.remaining_items }}
          **Decision Required**: 
          @${{ github.event.issue.user.login }} How would you like to proceed?
          **Options:**
          - 🔄 **Continue**: Comment \`continue implementing remaining items\`
          - ✂️ **Reduce Scope**: Comment \`accept current implementation\` (will create PR with current state)
          - 🔀 **Modify Plan**: Comment \`modify implementation: [specific changes]\`
          - ❌ **Stop**: Comment \`cancel implementation\`
          **Current Branch**: \`$BRANCH_NAME\`
          **Decision Record**: \`${{ needs.load-config.outputs.thoughts_dir }}/shared/decisions/pipeline-issue-${{ needs.parse-comment.outputs.issue_number }}.md\`
          "
  # Validate complete implementation
  validate-complete-implementation:
    needs: [load-config, validate-branch-continuity, validate-context-continuity, parse-comment, detect-partial-completion]
    if: needs.detect-partial-completion.outputs.completion_percentage == '100'
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - name: Checkout shared workflows
        uses: actions/checkout@v4
        with:
          repository: atriumn/atriumn-shared-workflows
          path: .github/shared-workflows
      - name: Switch to pipeline branch
        run: |
          BRANCH_NAME="${{ needs.validate-branch-continuity.outputs.branch_name }}"
          git checkout "$BRANCH_NAME"
          git pull origin "$BRANCH_NAME"
      - name: Run implementation validation
        run: |
          ISSUE_NUMBER="${{ needs.parse-comment.outputs.issue_number }}"
          BRANCH_NAME="${{ needs.validate-branch-continuity.outputs.branch_name }}"
          # Run full implementation validation
          if ./.github/shared-workflows/scripts/validate-implementation.sh "$BRANCH_NAME"; then
            # Update decision record
            THOUGHTS_DIR="${{ needs.load-config.outputs.thoughts_dir }}"
            DECISION_RECORD="$THOUGHTS_DIR/shared/decisions/pipeline-issue-$ISSUE_NUMBER.md"
            cat >> "$DECISION_RECORD" << EOF
          ## Implementation Phase (Complete ✅)
          - **Status**: Full implementation validation passed
          - **Completion**: 100%
          - **Phases Complete**: ${{ needs.detect-partial-completion.outputs.total_phases }}/${{ needs.detect-partial-completion.outputs.total_phases }}
          - **Validated**: $(date -Iseconds)
          - **Validator**: GitHub Actions
          - **Next Phase**: PR Creation
          ## PR Creation Phase (Ready)
          - **Status**: Ready for pull request creation
          - **Branch**: \`$BRANCH_NAME\`
          - **Target**: \`${{ needs.load-config.outputs.base_branch }}\`
          ## Pipeline Progress
          - [✅] Research Phase
          - [✅] Planning Phase  
          - [✅] Implementation Phase
          - [❌] PR Creation
          EOF
            git add "$DECISION_RECORD"
            git commit -m "Pipeline: Implementation phase complete for issue #$ISSUE_NUMBER"
            git push origin "$BRANCH_NAME"
            gh issue comment $ISSUE_NUMBER --body "
            ✅ **Implementation Validation Passed**
            **Branch**: \`$BRANCH_NAME\`
            **Completion**: 100% - All planned phases implemented
            **Decision Record Updated**: \`$DECISION_RECORD\`
            **Next Step**: Create pull request for review
            All implementation phases have been completed successfully. Ready to create pull request.
            "
          else
            gh issue comment $ISSUE_NUMBER --body "❌ **Implementation Validation Failed**: Please review implementation and fix issues"
            exit 1
          fi
  # Task 3: Error Recovery - Retry mechanisms
  retry-research:
    if: needs.parse-comment.outputs.trigger == 'retry-research'
    needs: [load-config, validate-branch-continuity, validate-context-continuity, parse-comment]
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - name: Clear Previous Research Attempt
        run: |
          BRANCH_NAME="${{ needs.validate-branch-continuity.outputs.branch_name }}"
          THOUGHTS_DIR="${{ needs.load-config.outputs.thoughts_dir }}"
          ISSUE_NUMBER="${{ needs.parse-comment.outputs.issue_number }}"
          git checkout "$BRANCH_NAME"
          git pull origin "$BRANCH_NAME"
          # Remove failed research document if it exists
          find "$THOUGHTS_DIR/shared/research" -name "*issue-$ISSUE_NUMBER*" -delete || true
          # Reset decision record to pre-research state
          DECISION_RECORD="$THOUGHTS_DIR/shared/decisions/pipeline-issue-$ISSUE_NUMBER.md"
          # Remove everything after the initial research phase start
          sed -i '/## Research Phase/,$d' "$DECISION_RECORD"
          cat >> "$DECISION_RECORD" << EOF
          ## Research Phase (Retry)
          - **Status**: Retrying after validation failure
          - **Started**: $(date -Iseconds)
          - **Trigger**: Retry requested by @${{ github.event.comment.user.login }}
          - **Previous Attempt**: Failed validation
          EOF
          git add .
          git commit -m "Pipeline: Reset for research retry on issue #$ISSUE_NUMBER"
          git push origin "$BRANCH_NAME"
      - name: Trigger Research Retry
        run: |
          ISSUE_NUMBER="${{ needs.parse-comment.outputs.issue_number }}"
          THOUGHTS_DIR="${{ needs.load-config.outputs.thoughts_dir }}"
          gh issue comment $ISSUE_NUMBER --body "
          🔄 **Research Phase Retry**
          Previous research attempt failed validation. Retrying with clean state.
          **This is a RETRY** - Previous attempt failed validation. Please ensure:
          - All required sections are present and complete
          - Minimum 3 file references with \`filename.ext:line\` format
          - Valid YAML frontmatter with all required fields
          - No placeholder text (TODO, FIXME, etc.)
          **Expected Path**: \`$THOUGHTS_DIR/shared/research/research-issue-$ISSUE_NUMBER.md\`
          **Validation Trigger**: Comment \`✅ Research Phase Complete\` when done
          ---
          @claude Please research the codebase for this issue following our research methodology.
          **Research Requirements (Retry):**
          - Read and understand the issue requirements thoroughly
          - Search through the codebase to understand current implementation
          - Identify relevant files, patterns, and architectural decisions
          - Document findings with specific file references
          - Provide architectural insights and recommendations
          "
  # Restart entire pipeline
  restart-pipeline:
    if: needs.parse-comment.outputs.trigger == 'restart-pipeline'
    needs: [load-config, parse-comment]
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - name: Clean Up Previous Pipeline Attempt
        run: |
          ISSUE_NUMBER="${{ needs.parse-comment.outputs.issue_number }}"
          # Get current branch if it exists
          CURRENT_BRANCH=$(gh issue view $ISSUE_NUMBER --json labels --jq '.labels[] | select(.name | startswith("branch:")) | .name | sub("branch:"; "")' || echo "")
          if [ -n "$CURRENT_BRANCH" ]; then
            echo "🧹 Cleaning up previous branch: $CURRENT_BRANCH"
            # Delete branch and remove label
            git push origin --delete "$CURRENT_BRANCH" || true
            gh issue edit $ISSUE_NUMBER --remove-label "branch:$CURRENT_BRANCH" || true
          fi
          # Remove any pipeline-related labels
          gh issue edit $ISSUE_NUMBER --remove-label "pipeline-paused" || true
      - name: Restart Pipeline Fresh
        run: |
          ISSUE_NUMBER="${{ needs.parse-comment.outputs.issue_number }}"
          BASE_BRANCH="${{ needs.load-config.outputs.base_branch }}"
          gh issue comment $ISSUE_NUMBER --body "
          🔄 **Pipeline Restart**
          Previous pipeline attempt has been cleaned up. Starting fresh.
          **Cleanup Actions:**
          - Previous branch deleted
          - Pipeline labels removed
          - Clean slate for new attempt
          ---
          🚀 **Development Pipeline Starting**
          @claude run development pipeline:
          - Base branch: $BASE_BRANCH
          - Context preservation: ✅ Active
          - Error recovery: ✅ Active
          - Branch safety: ✅ Active
          **Note**: This is a fresh start. All previous pipeline artifacts have been removed.
          "
  # Continue partial implementation
  continue-partial-implementation:
    if: needs.parse-comment.outputs.trigger == 'continue-partial-implementation'
    needs: [load-config, validate-branch-continuity, validate-context-continuity, parse-comment]
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - name: Update Decision Record with Continue Decision
        run: |
          BRANCH_NAME="${{ needs.validate-branch-continuity.outputs.branch_name }}"
          THOUGHTS_DIR="${{ needs.load-config.outputs.thoughts_dir }}"
          DECISION_FILE="$THOUGHTS_DIR/shared/decisions/pipeline-issue-${{ needs.parse-comment.outputs.issue_number }}.md"
          git checkout "$BRANCH_NAME"
          git pull origin "$BRANCH_NAME"
          # Get completion status from latest partial completion check
          PARTIAL_INFO=$(grep -A 10 "Implementation Status Update - Partial Completion" "$DECISION_FILE" | tail -10)
          cat >> "$DECISION_FILE" << EOF
          ### Implementation Decision - Continue with Remaining Items
          - **Date**: $(date -Iseconds)
          - **Decision**: Continue implementing remaining items
          - **Decided By**: @${{ github.event.comment.user.login }}
          - **Context**: Partial implementation detected, continuing with remaining work
          #### Implementation Instructions for Claude:
          - **Current State**: Working code exists for completed phases
          - **Task**: Complete remaining items only, do not modify completed work
          - **Branch**: Continue on existing branch \`$BRANCH_NAME\`
          - **Validation**: Ensure all existing tests continue passing
          EOF
          git add "$DECISION_FILE"
          git commit -m "Pipeline: Decision to continue remaining implementation for issue #${{ needs.parse-comment.outputs.issue_number }}"
          git push origin "$BRANCH_NAME"
      - name: Trigger Continued Implementation
        run: |
          BRANCH_NAME="${{ needs.validate-branch-continuity.outputs.branch_name }}"
          ISSUE_NUMBER="${{ needs.parse-comment.outputs.issue_number }}"
          THOUGHTS_DIR="${{ needs.load-config.outputs.thoughts_dir }}"
          gh issue comment $ISSUE_NUMBER --body "
          ✅ **Decision Recorded: Continue Implementation**
          Proceeding with remaining implementation items.
          ---
          @claude Please continue implementation with the remaining items.
          **Read complete pipeline context:**
          \`$THOUGHTS_DIR/shared/decisions/pipeline-issue-$ISSUE_NUMBER.md\`
          **Critical Context from Decision Record:**
          - Current branch: \`$BRANCH_NAME\`
          - Some phases are ✅ COMPLETE - DO NOT MODIFY
          - Remaining phases need ❌ COMPLETION 
          **Your Task:**
          1. Read the decision record to understand what's complete vs. incomplete
          2. Use existing branch: \`git checkout $BRANCH_NAME && git pull origin $BRANCH_NAME\`
          3. Implement ONLY the remaining items identified in the decision record
          4. Do NOT modify or refactor completed items
          5. Ensure all existing tests continue passing
          6. Update decision record with progress
          **Important**: The decision record contains the authoritative status of what's been completed. Use it to guide your work.
          "
  # Task 5: Decision Record Size Management
  manage-decision-record:
    if: github.event_name == 'issue_comment' && needs.validate-context-continuity.outputs.context_valid == 'true'
    needs: [load-config, validate-context-continuity]
    runs-on: ubuntu-latest
    steps:
      - name: Check Decision Record Size
        id: size-check
        run: |
          THOUGHTS_DIR="${{ needs.load-config.outputs.thoughts_dir }}"
          DECISION_FILE="$THOUGHTS_DIR/shared/decisions/pipeline-issue-${{ github.event.issue.number }}.md"
          if [ -f "$DECISION_FILE" ]; then
            LINE_COUNT=$(wc -l < "$DECISION_FILE")
            WORD_COUNT=$(wc -w < "$DECISION_FILE")
            echo "lines=$LINE_COUNT" >> $GITHUB_OUTPUT
            echo "words=$WORD_COUNT" >> $GITHUB_OUTPUT
            # Set thresholds for management
            if [ "$LINE_COUNT" -gt 200 ]; then
              echo "needs_management=true" >> $GITHUB_OUTPUT
              echo "management_type=summarize" >> $GITHUB_OUTPUT
            elif [ "$LINE_COUNT" -gt 150 ]; then
              echo "needs_management=true" >> $GITHUB_OUTPUT
              echo "management_type=compress" >> $GITHUB_OUTPUT
            else
              echo "needs_management=false" >> $GITHUB_OUTPUT
            fi
            echo "📊 Decision record: $LINE_COUNT lines, $WORD_COUNT words"
          fi
      - name: Compress Decision Record
        if: steps.size-check.outputs.needs_management == 'true' && steps.size-check.outputs.management_type == 'compress'
        run: |
          THOUGHTS_DIR="${{ needs.load-config.outputs.thoughts_dir }}"
          DECISION_FILE="$THOUGHTS_DIR/shared/decisions/pipeline-issue-${{ github.event.issue.number }}.md"
          echo "🗜️ Compressing decision record to improve readability..."
          # Create backup
          cp "$DECISION_FILE" "$DECISION_FILE.backup"
          # Compress completed phases into collapsible sections
          python3 << 'EOF'
          import re
          import os
          decision_file = os.environ['DECISION_FILE']
          with open(decision_file, 'r') as f:
              content = f.read()
          # Compress completed phases into collapsible sections
          def compress_section(match):
              section_name = match.group(1)
              section_content = match.group(2)
              if "(Complete ✅)" in section_name:
                  # Extract key info for summary
                  summary_lines = []
                  for line in section_content.split('\n'):
                      if line.strip().startswith('- **') and any(key in line for key in ['Status', 'Validated', 'Document']):
                          summary_lines.append(line.strip())
                  summary = '\n'.join(summary_lines[:3])  # Keep top 3 key facts
                  return f"""{section_name}
          <details>
          <summary>📋 Phase Summary (click to expand)</summary>
          {summary}
          <details>
          <summary>📝 Full Details</summary>
          {section_content}
          </details>
          </details>
          """
              return match.group(0)
          # Apply compression to completed phases
          compressed = re.sub(r'(## \w+ Phase.*?Complete ✅\).*?)\n(.*?)(?=\n## |$)', compress_section, content, flags=re.DOTALL)
          with open(decision_file, 'w') as f:
              f.write(compressed)
          print("✅ Decision record compressed successfully")
          EOF
  # Task 3: Enhanced Error Recovery - Global failure handler
  handle-validation-failure:
    if: failure()
    runs-on: ubuntu-latest
    steps:
      - name: Analyze Failure Context
        id: analyze
        run: |
          echo "failure_type=validation" >> $GITHUB_OUTPUT
          echo "retry_possible=true" >> $GITHUB_OUTPUT
      - name: Provide Recovery Guidance
        run: |
          gh issue comment ${{ github.event.issue.number }} --body "
          ❌ **Pipeline Failure Detected**
          **Failure Type**: ${{ steps.analyze.outputs.failure_type }}
          **Recovery Possible**: ${{ steps.analyze.outputs.retry_possible }}
          **Common Recovery Actions:**
          ### If Research Failed:
          - Review research document structure and content
          - Ensure sufficient file references (minimum 3)
          - Check YAML frontmatter is complete
          - Retry: Comment \`@claude retry research\`
          ### If Planning Failed:
          - Review implementation plan for completeness
          - Ensure success criteria are properly formatted
          - Check for unresolved TODO/FIXME items
          - Retry: Comment \`@claude retry planning\`
          ### If Implementation Failed:
          - Check that all tests pass locally
          - Verify code quality standards
          - Ensure no merge conflicts
          - Retry: Comment \`@claude retry implementation\`
          ### If Branch Issues:
          - Pipeline may need to be restarted entirely
          - Comment \`@claude restart pipeline\` to begin fresh
          **Pipeline Status**: ⏸️ Paused pending resolution
          **Debug Information:**
          - Issue: #${{ github.event.issue.number }}
          - Workflow: ${{ github.workflow }}
          - Run: ${{ github.run_id }}
          - Time: $(date -Iseconds)
          "
  # Enhanced PR validation
  validate-pr:
    if: github.event_name == 'pull_request' && github.event.action == 'opened'
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      - name: Checkout shared workflows
        uses: actions/checkout@v4
        with:
          repository: atriumn/atriumn-shared-workflows
          path: .github/shared-workflows
      - name: Install dependencies
        run: |
          sudo apt-get update && sudo apt-get install -y gh
      - name: Validate PR with enhanced context
        run: |
          PR_NUMBER="${{ github.event.pull_request.number }}"
          PR_BRANCH="${{ github.event.pull_request.head.ref }}"
          # Check if this is a pipeline branch
          if echo "$PR_BRANCH" | grep -q "^feature/issue-[0-9]\+-"; then
            ISSUE_NUMBER=$(echo "$PR_BRANCH" | sed -n 's/^feature\/issue-\([0-9]\+\)-.*/\1/p')
            # Enhanced PR validation for pipeline PRs
            if ./.github/shared-workflows/scripts/validate-pr.sh "$PR_NUMBER"; then
              gh pr comment $PR_NUMBER --body "
              ✅ **Pipeline PR Validation Passed**
              **Source Issue**: #$ISSUE_NUMBER
              **Pipeline Branch**: \`$PR_BRANCH\`
              **Validation**: All pipeline requirements met
              **Status**: Ready for review
              This PR was created through the development pipeline and has passed all validation checks.
              "
            else
              gh pr comment $PR_NUMBER --body "❌ **Pipeline PR Validation Failed** - Please review requirements"
              exit 1
            fi
          else
            # Standard PR validation
            if ./.github/shared-workflows/scripts/validate-pr.sh "$PR_NUMBER"; then
              gh pr comment $PR_NUMBER --body "✅ **PR Validation Passed** - Ready for review"
            else
              gh pr comment $PR_NUMBER --body "❌ **PR Validation Failed** - Please review requirements"
              exit 1
            fi
          fi

================
File: templates/repo-workflow-template.yml
================
# Copy this to your repo: .github/workflows/development-pipeline.yml
name: Development Pipeline
on:
  issue_comment:
    types: [created]
jobs:
  # Only trigger on pipeline-related comments in issues
  check-trigger:
    if: |
      github.event.issue &&
      (contains(github.event.comment.body, '@atriumn-pipeline start') ||
       contains(github.event.comment.body, '✅ Research Phase Complete') ||
       contains(github.event.comment.body, 'approve research') ||
       contains(github.event.comment.body, '✅ Planning Phase Complete') ||
       contains(github.event.comment.body, 'approve plan') ||
       contains(github.event.comment.body, '✅ Implementation Phase Complete') ||
       contains(github.event.comment.body, 'approve implementation'))
    runs-on: ubuntu-latest
    outputs:
      should_trigger: ${{ steps.check.outputs.should_trigger }}
    steps:
      - name: Check if should trigger pipeline
        id: check
        run: |
          echo "should_trigger=true" >> $GITHUB_OUTPUT
          echo "Triggering shared development pipeline"
  development-pipeline:
    needs: check-trigger
    if: needs.check-trigger.outputs.should_trigger == 'true'
    uses: atriumn/atriumn-shared-workflows/.github/workflows/development-pipeline.yml@main
    with:
      repo_name: ${{ github.repository }}
      issue_number: ${{ github.event.issue.number }}
    secrets:
      REPO_TOKEN: ${{ secrets.PIPELINE_TOKEN }}
# Configuration Instructions:
# 
# 1. Copy this file to your repository at: .github/workflows/development-pipeline.yml
# 
# 2. Update the repository name if different from github.repository
# 
# 3. Create a repository secret named PIPELINE_TOKEN:
#    - Go to Settings > Secrets and variables > Actions
#    - Create new repository secret: PIPELINE_TOKEN
#    - Value: Your GitHub Personal Access Token with repo permissions
# 
# 4. Optional: Create repo-specific config at .github/development-pipeline-config.yml:
#    repo_name: "your-repo-name"
#    base_branch: "main"  # or "develop", "master"
#    thoughts_directory: "thoughts/"  # or "docs/", "documentation/"
#    
#    validation:
#      research_min_refs: 3
#      implementation_test_commands:
#        - "npm test"
#        - "npm run lint"
#        - "npm run typecheck"
#    
#    team:
#      default_reviewers: ["@your-username"]
#    
#    notifications:
#      slack_channel: "#dev-team"
# 
# 5. Test the pipeline:
#    - Create an issue in your repository
#    - Comment: "@atriumn-pipeline start"
#    - The pipeline will create a branch and guide Claude through the process
# 
# 6. Pipeline modes:
#    - Human validation (default): "@atriumn-pipeline start"
#    - Fully automated: "@atriumn-pipeline start\nHuman validation: false"
#    - Custom base branch: "@atriumn-pipeline start\nBase branch: develop"

================
File: test/test-validation-scripts.sh
================
#!/bin/bash
# test/test-validation-scripts.sh
set -e
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
TEST_DATA_DIR="$SCRIPT_DIR/test-data"
SCRIPTS_DIR="$SCRIPT_DIR/../scripts"
# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color
# Test result tracking
TESTS_RUN=0
TESTS_PASSED=0
TESTS_FAILED=0
print_header() {
    echo -e "${BLUE}========================================${NC}"
    echo -e "${BLUE}   Validation Scripts Test Suite${NC}"
    echo -e "${BLUE}========================================${NC}"
    echo
}
print_test_result() {
    local test_name="$1"
    local result="$2"
    local details="$3"
    TESTS_RUN=$((TESTS_RUN + 1))
    if [ "$result" = "PASS" ]; then
        echo -e "${GREEN}✅ PASS${NC}: $test_name"
        TESTS_PASSED=$((TESTS_PASSED + 1))
    else
        echo -e "${RED}❌ FAIL${NC}: $test_name"
        if [ -n "$details" ]; then
            echo -e "   ${YELLOW}Details: $details${NC}"
        fi
        TESTS_FAILED=$((TESTS_FAILED + 1))
    fi
}
# Create test data
setup_test_data() {
    echo -e "${BLUE}Setting up test data...${NC}"
    rm -rf "$TEST_DATA_DIR"
    mkdir -p "$TEST_DATA_DIR"
    # Create valid research document
    cat > "$TEST_DATA_DIR/valid-research.md" << 'EOF'
---
date: 2025-08-17T14:30:00-05:00
researcher: test-user
topic: "Test Research Document"
status: complete
tags: [research, testing]
---
# Research: Test Topic
**Date**: 2025-08-17T14:30:00-05:00
**Researcher**: test-user
## Research Question
How do we test validation scripts effectively?
## Summary
This is a comprehensive test research document that validates all required sections and patterns. The research covers validation script testing methodologies and includes proper file references as required by the validation framework.
## Detailed Findings
### Key Requirements Analysis
- Validation scripts must check for proper YAML frontmatter
- File references are critical for traceability
- All required sections must be present
### Technical Implementation
Found relevant implementation details in the following files:
- Configuration system defined in `configs/schema.yml:45`
- Main validation logic in `scripts/validate-research.sh:75` 
- Helper functions located in `lib/utils.ts:123`
## Code References
- `scripts/validate-research.sh:26` - Configuration loading logic
- `configs/default.yml:12` - Default validation settings
- `test/test-data/sample.md:89` - Example valid document structure
- `templates/decision-record-template.md:34` - Template structure reference
## Architecture Insights
The validation system follows a modular architecture where each validation script can be configured independently through YAML configuration files. This allows for repository-specific customization while maintaining consistent validation standards across all projects.
## Historical Context (from thoughts/)
None available for this test document.
## Related Research
This test document validates the research validation framework itself.
## Open Questions
None - all validation requirements have been addressed in this test document.
EOF
    # Create invalid research document (missing sections)
    cat > "$TEST_DATA_DIR/invalid-research-missing-sections.md" << 'EOF'
---
date: 2025-08-17T14:30:00-05:00
researcher: test-user
topic: "Incomplete Research"
status: incomplete
---
# Incomplete Research
This research is missing required sections and file references.
## Summary
This document is intentionally incomplete for testing validation failure cases.
EOF
    # Create invalid research document (missing frontmatter)
    cat > "$TEST_DATA_DIR/invalid-research-no-frontmatter.md" << 'EOF'
# Research Without Frontmatter
This document has no YAML frontmatter.
## Research Question
How to test missing frontmatter?
## Summary  
This should fail validation.
## Detailed Findings
No findings because no frontmatter.
## Code References
- `some/file.js:123` - Some reference
## Architecture Insights
None.
EOF
    # Create invalid research with insufficient file references
    cat > "$TEST_DATA_DIR/invalid-research-no-refs.md" << 'EOF'
---
date: 2025-08-17T14:30:00-05:00
researcher: test-user
topic: "Research Without File References"
status: complete
---
# Research: No File References
## Research Question
How to test insufficient file references?
## Summary
This document has all required sections but insufficient file references.
## Detailed Findings
Some findings without proper file references.
## Code References
This section exists but has no actual file references.
## Architecture Insights
Some insights without file references.
EOF
    # Create valid implementation plan
    cat > "$TEST_DATA_DIR/valid-plan.md" << 'EOF'
---
date: 2025-08-17T15:00:00-05:00
researcher: test-user
topic: "Test Implementation Plan"
status: complete
---
# Test Feature Implementation Plan
## Overview
Implementation plan for testing validation scripts.
## Implementation Approach
Systematic testing approach using shell scripts and test data.
## Phase 1: Setup
Create test infrastructure and sample documents.
#### Automated Verification:
- [ ] Test scripts run successfully: `./test/test-validation-scripts.sh`
- [ ] All validation scripts are executable: `find scripts/ -name "*.sh" -executable`
- [ ] Configuration files are valid: `make validate-config`
#### Manual Verification:
- [ ] Test output is readable and informative
- [ ] Error messages are clear and actionable
- [ ] Test coverage includes all validation scenarios
## Phase 2: Validation
Run comprehensive validation tests.
#### Automated Verification:
- [ ] Research validation tests pass: `make test-research-validation`
- [ ] Plan validation tests pass: `make test-plan-validation`
- [ ] All tests complete without errors: `make test-all`
#### Manual Verification:
- [ ] Test results are accurate
- [ ] Edge cases are properly handled
- [ ] Documentation is up to date
EOF
    # Create invalid plan (missing sections)
    cat > "$TEST_DATA_DIR/invalid-plan-missing-sections.md" << 'EOF'
---
date: 2025-08-17T15:00:00-05:00
researcher: test-user
topic: "Incomplete Plan"
status: incomplete
---
# Incomplete Implementation Plan
This plan is missing required sections.
## Overview
Some overview text.
EOF
    echo -e "${GREEN}✅ Test data setup complete${NC}"
}
# Test research validation script
test_research_validation() {
    echo
    echo -e "${BLUE}Testing Research Validation Script${NC}"
    echo "======================================"
    # Test 1: Valid research document should pass
    if "$SCRIPTS_DIR/validate-research.sh" "$TEST_DATA_DIR/valid-research.md" >/dev/null 2>&1; then
        print_test_result "Valid research document" "PASS"
    else
        print_test_result "Valid research document" "FAIL" "Valid document failed validation"
    fi
    # Test 2: Invalid document (missing sections) should fail
    if ! "$SCRIPTS_DIR/validate-research.sh" "$TEST_DATA_DIR/invalid-research-missing-sections.md" >/dev/null 2>&1; then
        print_test_result "Invalid research (missing sections)" "PASS"
    else
        print_test_result "Invalid research (missing sections)" "FAIL" "Invalid document passed validation"
    fi
    # Test 3: Invalid document (no frontmatter) should fail
    if ! "$SCRIPTS_DIR/validate-research.sh" "$TEST_DATA_DIR/invalid-research-no-frontmatter.md" >/dev/null 2>&1; then
        print_test_result "Invalid research (no frontmatter)" "PASS"
    else
        print_test_result "Invalid research (no frontmatter)" "FAIL" "Document without frontmatter passed validation"
    fi
    # Test 4: Invalid document (insufficient file references) should fail
    if ! "$SCRIPTS_DIR/validate-research.sh" "$TEST_DATA_DIR/invalid-research-no-refs.md" >/dev/null 2>&1; then
        print_test_result "Invalid research (insufficient file refs)" "PASS"
    else
        print_test_result "Invalid research (insufficient file refs)" "FAIL" "Document with insufficient file references passed validation"
    fi
    # Test 5: Non-existent file should fail
    if ! "$SCRIPTS_DIR/validate-research.sh" "$TEST_DATA_DIR/non-existent.md" >/dev/null 2>&1; then
        print_test_result "Non-existent research file" "PASS"
    else
        print_test_result "Non-existent research file" "FAIL" "Non-existent file somehow passed validation"
    fi
}
# Test plan validation script
test_plan_validation() {
    echo
    echo -e "${BLUE}Testing Plan Validation Script${NC}"
    echo "================================"
    # Test 1: Valid plan document should pass
    if "$SCRIPTS_DIR/validate-plan.sh" "$TEST_DATA_DIR/valid-plan.md" >/dev/null 2>&1; then
        print_test_result "Valid plan document" "PASS"
    else
        print_test_result "Valid plan document" "FAIL" "Valid plan failed validation"
    fi
    # Test 2: Invalid plan (missing sections) should fail
    if ! "$SCRIPTS_DIR/validate-plan.sh" "$TEST_DATA_DIR/invalid-plan-missing-sections.md" >/dev/null 2>&1; then
        print_test_result "Invalid plan (missing sections)" "PASS"
    else
        print_test_result "Invalid plan (missing sections)" "FAIL" "Invalid plan passed validation"
    fi
    # Test 3: Non-existent file should fail
    if ! "$SCRIPTS_DIR/validate-plan.sh" "$TEST_DATA_DIR/non-existent-plan.md" >/dev/null 2>&1; then
        print_test_result "Non-existent plan file" "PASS"
    else
        print_test_result "Non-existent plan file" "FAIL" "Non-existent file somehow passed validation"
    fi
}
# Test script help functionality
test_help_functionality() {
    echo
    echo -e "${BLUE}Testing Help Functionality${NC}"
    echo "============================"
    # Test research script help
    if "$SCRIPTS_DIR/validate-research.sh" --help >/dev/null 2>&1; then
        print_test_result "Research script help flag" "PASS"
    else
        print_test_result "Research script help flag" "FAIL" "Help flag failed"
    fi
    # Test plan script help
    if "$SCRIPTS_DIR/validate-plan.sh" --help >/dev/null 2>&1; then
        print_test_result "Plan script help flag" "PASS"
    else
        print_test_result "Plan script help flag" "FAIL" "Help flag failed"
    fi
    # Test implementation script help
    if "$SCRIPTS_DIR/validate-implementation.sh" --help >/dev/null 2>&1; then
        print_test_result "Implementation script help flag" "PASS"
    else
        print_test_result "Implementation script help flag" "FAIL" "Help flag failed"
    fi
    # Test PR script help
    if "$SCRIPTS_DIR/validate-pr.sh" --help >/dev/null 2>&1; then
        print_test_result "PR script help flag" "PASS"
    else
        print_test_result "PR script help flag" "FAIL" "Help flag failed"
    fi
}
# Test script executable permissions
test_script_permissions() {
    echo
    echo -e "${BLUE}Testing Script Permissions${NC}"
    echo "==========================="
    local scripts=("validate-research.sh" "validate-plan.sh" "validate-implementation.sh" "validate-pr.sh")
    for script in "${scripts[@]}"; do
        if [ -x "$SCRIPTS_DIR/$script" ]; then
            print_test_result "Script executable: $script" "PASS"
        else
            print_test_result "Script executable: $script" "FAIL" "Script is not executable"
        fi
    done
}
# Test configuration file access
test_config_access() {
    echo
    echo -e "${BLUE}Testing Configuration Access${NC}"
    echo "=============================="
    # Test default config exists
    if [ -f "$SCRIPT_DIR/../configs/default.yml" ]; then
        print_test_result "Default config file exists" "PASS"
    else
        print_test_result "Default config file exists" "FAIL" "Default config not found"
    fi
    # Test schema file exists
    if [ -f "$SCRIPT_DIR/../configs/schema.yml" ]; then
        print_test_result "Schema config file exists" "PASS"
    else
        print_test_result "Schema config file exists" "FAIL" "Schema config not found"
    fi
    # Test if yq is available (required for validation scripts)
    if command -v yq >/dev/null 2>&1; then
        print_test_result "yq command available" "PASS"
    else
        print_test_result "yq command available" "FAIL" "yq is required but not installed"
    fi
}
# Cleanup test data
cleanup() {
    echo
    echo -e "${BLUE}Cleaning up test data...${NC}"
    rm -rf "$TEST_DATA_DIR"
    echo -e "${GREEN}✅ Cleanup complete${NC}"
}
# Print final results
print_summary() {
    echo
    echo -e "${BLUE}========================================${NC}"
    echo -e "${BLUE}           Test Results Summary${NC}"
    echo -e "${BLUE}========================================${NC}"
    echo -e "Total tests run: ${BLUE}$TESTS_RUN${NC}"
    echo -e "Tests passed: ${GREEN}$TESTS_PASSED${NC}"
    echo -e "Tests failed: ${RED}$TESTS_FAILED${NC}"
    if [ $TESTS_FAILED -eq 0 ]; then
        echo
        echo -e "${GREEN}🎉 All tests passed! Validation scripts are working correctly.${NC}"
        exit 0
    else
        echo
        echo -e "${RED}❌ Some tests failed. Please review the validation scripts.${NC}"
        exit 1
    fi
}
# Main test execution
main() {
    print_header
    # Check prerequisites
    if ! command -v yq >/dev/null 2>&1; then
        echo -e "${YELLOW}⚠️  Warning: yq is not installed. Some tests may fail.${NC}"
        echo -e "   Install yq with: brew install yq (macOS) or see https://github.com/mikefarah/yq"
        echo
    fi
    setup_test_data
    test_script_permissions
    test_config_access
    test_help_functionality
    test_research_validation
    test_plan_validation
    cleanup
    print_summary
}
# Run tests
main "$@"

================
File: .gitignore
================
# Shell script temporary files
*.tmp
*.temp
*.log

# Script execution artifacts
*.pid
*.lock
*.out
*.err

# OS generated files
.DS_Store
.DS_Store?
._*
.Spotlight-V100
.Trashes
ehthumbs.db
Thumbs.db

# IDE files
.vscode/
.idea/
*.swp
*.swo
*~

# Environment files
.env
.env.local
.env.*.local

# Runtime data
pids
*.pid
*.seed
*.pid.lock

# Coverage directory used by tools like istanbul
coverage/
*.lcov

# nyc test coverage
.nyc_output

# Dependency directories
node_modules/

# Optional npm cache directory
.npm

# Optional REPL history
.node_repl_history

# Output of 'npm pack'
*.tgz

# Yarn Integrity file
.yarn-integrity

# parcel-bundler cache (https://parceljs.org/)
.cache
.parcel-cache

# Build outputs
dist/
build/
out/

# Runtime configuration
config.local.yml
config.*.local.yml

# Logs
logs
*.log
npm-debug.log*
yarn-debug.log*
yarn-error.log*

# Temporary folders
tmp/
temp/

================
File: Makefile
================
# Makefile for Shared Workflows Validation Scripts
# 
# This Makefile provides convenient commands for testing and validating
# the development pipeline components.

.PHONY: help test test-validation test-research test-plan test-implementation test-pr validate-config install-deps check-deps clean

# Default target
help:
	@echo "Shared Workflows Validation Commands"
	@echo "====================================="
	@echo ""
	@echo "Testing Commands:"
	@echo "  make test                 - Run all validation script tests"
	@echo "  make test-validation      - Run validation script test suite"
	@echo ""
	@echo "Individual Validation Commands:"
	@echo "  make test-research DOC=<file>    - Test research document validation"
	@echo "  make test-plan DOC=<file>        - Test implementation plan validation"
	@echo "  make test-implementation BRANCH=<name> - Test implementation validation"
	@echo "  make test-pr PR=<number>         - Test PR validation"
	@echo ""
	@echo "Configuration Commands:"
	@echo "  make validate-config      - Validate configuration files"
	@echo "  make check-deps          - Check required dependencies"
	@echo "  make install-deps        - Install required dependencies (macOS)"
	@echo ""
	@echo "Utility Commands:"
	@echo "  make clean               - Clean up test artifacts"
	@echo "  make help                - Show this help message"
	@echo ""
	@echo "Examples:"
	@echo "  make test-research DOC=thoughts/shared/research/my-research.md"
	@echo "  make test-plan DOC=thoughts/shared/plans/my-plan.md" 
	@echo "  make test-implementation BRANCH=feature/issue-123-my-feature"
	@echo "  make test-pr PR=456"

# Check if required dependencies are installed
check-deps:
	@echo "🔍 Checking dependencies..."
	@command -v yq >/dev/null 2>&1 || (echo "❌ yq is required but not installed. Run 'make install-deps' or see https://github.com/mikefarah/yq" && exit 1)
	@command -v gh >/dev/null 2>&1 || (echo "❌ GitHub CLI (gh) is required but not installed. See https://cli.github.com/" && exit 1)
	@echo "✅ All dependencies are installed"

# Install dependencies on macOS (requires Homebrew)
install-deps:
	@echo "📦 Installing dependencies..."
	@if command -v brew >/dev/null 2>&1; then \
		brew install yq gh; \
		echo "✅ Dependencies installed successfully"; \
	else \
		echo "❌ Homebrew not found. Please install manually:"; \
		echo "   - yq: https://github.com/mikefarah/yq"; \
		echo "   - gh: https://cli.github.com/"; \
		exit 1; \
	fi

# Run all tests
test: test-validation

# Run validation script test suite
test-validation: check-deps
	@echo "🧪 Running validation script test suite..."
	@./test/test-validation-scripts.sh

# Test research document validation
test-research:
	@if [ -z "$(DOC)" ]; then \
		echo "❌ Usage: make test-research DOC=<document_path>"; \
		echo "   Example: make test-research DOC=thoughts/shared/research/my-research.md"; \
		exit 1; \
	fi
	@echo "🔍 Validating research document: $(DOC)"
	@./scripts/validate-research.sh "$(DOC)"

# Test implementation plan validation
test-plan:
	@if [ -z "$(DOC)" ]; then \
		echo "❌ Usage: make test-plan DOC=<document_path>"; \
		echo "   Example: make test-plan DOC=thoughts/shared/plans/my-plan.md"; \
		exit 1; \
	fi
	@echo "📋 Validating implementation plan: $(DOC)"
	@./scripts/validate-plan.sh "$(DOC)"

# Test implementation validation
test-implementation:
	@if [ -z "$(BRANCH)" ]; then \
		echo "❌ Usage: make test-implementation BRANCH=<branch_name>"; \
		echo "   Example: make test-implementation BRANCH=feature/issue-123-my-feature"; \
		exit 1; \
	fi
	@echo "⚙️ Validating implementation on branch: $(BRANCH)"
	@./scripts/validate-implementation.sh "$(BRANCH)"

# Test PR validation
test-pr:
	@if [ -z "$(PR)" ]; then \
		echo "❌ Usage: make test-pr PR=<pr_number>"; \
		echo "   Example: make test-pr PR=456"; \
		exit 1; \
	fi
	@echo "🔄 Validating PR: #$(PR)"
	@./scripts/validate-pr.sh "$(PR)"

# Validate configuration files
validate-config: check-deps
	@echo "⚙️ Validating configuration files..."
	@echo "📝 Checking default.yml..."
	@yq eval '.' configs/default.yml >/dev/null && echo "✅ configs/default.yml is valid"
	@echo "📝 Checking schema.yml..."
	@yq eval '.' configs/schema.yml >/dev/null && echo "✅ configs/schema.yml is valid"
	@if [ -f configs/curatefor.me.yml ]; then \
		echo "📝 Checking curatefor.me.yml..."; \
		yq eval '.' configs/curatefor.me.yml >/dev/null && echo "✅ configs/curatefor.me.yml is valid"; \
	fi
	@echo "✅ All configuration files are valid"

# Clean up test artifacts
clean:
	@echo "🧹 Cleaning up test artifacts..."
	@rm -rf test/test-data
	@echo "✅ Cleanup complete"

# Development helpers
.PHONY: make-scripts-executable lint-scripts

# Ensure all scripts are executable
make-scripts-executable:
	@echo "🔧 Making all scripts executable..."
	@chmod +x scripts/*.sh
	@chmod +x test/*.sh
	@echo "✅ All scripts are now executable"

# Lint shell scripts (requires shellcheck)
lint-scripts:
	@if command -v shellcheck >/dev/null 2>&1; then \
		echo "🔍 Linting shell scripts..."; \
		shellcheck scripts/*.sh test/*.sh; \
		echo "✅ Script linting complete"; \
	else \
		echo "⚠️  shellcheck not installed. Install with: brew install shellcheck"; \
	fi

================
File: README.md
================
# Atriumn Shared Workflows

Centralized GitHub Actions workflows for consistent development processes across all Atriumn repositories.

## Available Workflows

### Development Pipeline (`development-pipeline.yml`)

A comprehensive workflow that automates the research, planning, and implementation phases for GitHub issues.

**Features:**
- ✅ Automated issue research and analysis
- ✅ Human validation checkpoints
- ✅ Decision record generation
- ✅ Cross-repository compatibility
- ✅ Scalable architecture

## Quick Start

### 1. Prerequisites

Your repository needs:
- A GitHub Personal Access Token (PAT) with repository permissions
- The PAT stored as a repository secret named `PIPELINE_TOKEN`

### 2. Copy the Template

1. Copy `template-development-pipeline.yml` from this repository
2. Save it to your repo at `.github/workflows/development-pipeline.yml`
3. Customize the `repo_name` field (line ~50) to match your repository

### 3. Configure Secrets

Add a repository secret:
- **Name:** `PIPELINE_TOKEN`
- **Value:** Your GitHub Personal Access Token

### 4. Usage

Trigger the pipeline by:
- **Comment on an issue:** `@claude run development pipeline`
- **Manual trigger:** Use the "Actions" tab and run "Development Pipeline"

## Architecture

### Separation of Concerns

The architecture separates **orchestration** (shared workflows) from **file operations** (local workflows):

```
┌─────────────────────────────────────┐
│        Shared Workflow              │
│  (atriumn-shared-workflows)         │
│                                     │
│  • Issue research & analysis        │
│  • Planning & validation            │
│  • Human approval workflow          │
│  • Generate structured outputs      │
│                                     │
│  ❌ NO direct file writes           │
└─────────────────────────────────────┘
                    │
                    │ outputs
                    ▼
┌─────────────────────────────────────┐
│        Local Workflow               │
│    (your repository)                │
│                                     │
│  • Branch creation                  │
│  • Decision record files            │
│  • Git commits & pushes             │
│  • Repository-specific operations   │
│                                     │
│  ✅ All file writes happen here     │
└─────────────────────────────────────┘
```

### Why This Design?

GitHub's security model prevents shared workflows from writing to external repositories. This architecture:
- ✅ Centralizes business logic in shared workflows
- ✅ Respects GitHub's security boundaries
- ✅ Scales to any number of repositories
- ✅ Maintains consistency across projects

## Workflow Outputs

The shared workflow provides these outputs for local processing:

| Output | Description |
|--------|-------------|
| `pipeline_status` | `success` or `failed` |
| `branch_name` | Generated feature branch name |
| `pipeline_id` | Unique pipeline identifier |
| `issue_title` | GitHub issue title |
| `research_summary` | Detailed research findings |
| `next_actions` | Recommended implementation steps |
| `decision_record_content` | Complete decision record in Markdown |

## Customization

### Directory Structure

The template assumes your decision records go in:
```
thoughts/shared/decisions/pipeline-issue-{number}.md
```

To customize, edit the `mkdir -p` and file paths in your local workflow.

### Branch Naming

Default branch naming: `feature/issue-{number}-pipeline`

Customize by passing `branch_name` input or modifying the generation logic.

### Human Validation

Set `human_validation: false` to skip approval steps, or leave as `true` for human oversight.

## Advanced Configuration

### Multiple Workflows

You can create multiple specialized workflows:
- `development-pipeline.yml` - Full feature development
- `hotfix-pipeline.yml` - Emergency fixes
- `documentation-pipeline.yml` - Documentation updates

Each can use the same shared workflow with different parameters.

### Custom Issue Triggers

Extend the trigger conditions in your local workflow:
```yaml
if: |
  contains(github.event.comment.body, '@claude run development pipeline') ||
  contains(github.event.comment.body, 'your-custom-trigger') ||
  github.event_name == 'workflow_dispatch'
```

## Troubleshooting

### Common Issues

**"Bad credentials" errors:**
- Verify `PIPELINE_TOKEN` is correctly set
- Ensure the PAT has repository permissions
- Check token expiration

**"Repository not found" errors:**
- Update `repo_name` in your local workflow
- Use format: `organization/repository`

**"Workflow is not reusable" errors:**
- Ensure you're referencing the correct branch (`@main`)
- Verify the shared workflow has `workflow_call` trigger

### Debug Mode

Enable debug output by setting workflow inputs:
```yaml
with:
  test_mode: true
```

## Contributing

1. Fork this repository
2. Create a feature branch
3. Test changes across multiple repositories
4. Submit a pull request

## Support

For issues or questions:
1. Check existing GitHub issues
2. Create a new issue with:
   - Repository details
   - Workflow logs
   - Steps to reproduce

---

## Legacy Documentation

The sections below contain documentation for the previous implementation phases and validation scripts that are still available in this repository.

### Phase 1: Validation Scripts

Standalone scripts for validating development pipeline phases:
- `validate-research.sh` - Research document validation
- `validate-plan.sh` - Implementation plan validation  
- `validate-implementation.sh` - Code quality and testing
- `validate-pr.sh` - Pull request validation

### Phase 2: GitHub Actions Integration ✅ COMPLETE

Full GitHub Actions workflow implementation with automated validation gates and human approval modes.

**Key Features:**
- Complete research → planning → implementation → PR workflow
- Automated validation using Phase 1 scripts
- Human approval checkpoints with configurable bypass
- Cross-repository compatibility with shared workflow architecture
- Decision record management throughout pipeline
- Branch creation and management
- Comprehensive error handling and retry mechanisms

**Components:**
- `.github/workflows/development-pipeline.yml` - Main shared workflow
- `templates/repo-workflow-template.yml` - Repository adoption template
- `.github/workflows/test-development-pipeline.yml` - Testing framework

**Usage:**
1. Copy `templates/repo-workflow-template.yml` to your repo
2. Configure `PIPELINE_TOKEN` secret with repository access
3. Comment `@claude run development pipeline` on any issue
4. Pipeline automatically guides Claude through the complete process

### Phase 3: Branch Safety & Context Preservation

Enhanced error handling and recovery mechanisms.

### Phase 4: Multi-Repo Configuration & Testing

Advanced configuration management and testing framework.

For complete legacy documentation, see the git history of this README.

================
File: template-development-pipeline.yml
================
# Template Development Pipeline Workflow
# 
# Copy this file to your repository at:
# .github/workflows/development-pipeline.yml
#
# Then customize the following:
# 1. Update repo_name in the workflow call (line ~50)
# 2. Ensure you have PIPELINE_TOKEN secret configured
# 3. Adjust directory structure if needed (e.g., thoughts/shared/decisions)
name: Development Pipeline
on:
  issue_comment:
    types: [created]
  workflow_dispatch:
    inputs:
      issue_number:
        description: 'Issue number'
        required: true
        type: string
      branch_name:
        description: 'Branch name'
        required: false
        type: string
      pipeline_id:
        description: 'Pipeline ID'
        required: false
        type: string
      human_validation:
        description: 'Enable human validation'
        required: false
        type: boolean
        default: true
      test_mode:
        description: 'Run in test mode'
        required: false
        type: boolean
        default: false
jobs:
  check-trigger:
    if: |
      contains(github.event.comment.body, '@claude run development pipeline') ||
      contains(github.event.comment.body, 'Research Phase Complete') ||
      contains(github.event.comment.body, 'approve research') ||
      contains(github.event.comment.body, 'Planning Phase Complete') ||
      contains(github.event.comment.body, 'approve plan') ||
      contains(github.event.comment.body, 'Implementation Phase Complete') ||
      contains(github.event.comment.body, 'approve implementation') ||
      github.event_name == 'workflow_dispatch'
    runs-on: ubuntu-latest
    steps:
      - run: echo "Triggering development pipeline"
  create-branch:
    needs: check-trigger
    runs-on: ubuntu-latest
    outputs:
      branch_name: ${{ steps.setup.outputs.branch_name }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.PIPELINE_TOKEN }}
          fetch-depth: 0
      - name: Create feature branch
        id: setup
        run: |
          # Extract issue number from workflow input or comment
          if [ -n "${{ inputs.issue_number }}" ]; then
            ISSUE_NUMBER="${{ inputs.issue_number }}"
          else
            # Extract from comment trigger (you may need to adjust this regex)
            ISSUE_NUMBER=$(echo "${{ github.event.issue.number }}")
          fi
          # Generate branch name if not provided
          if [ -z "${{ inputs.branch_name }}" ]; then
            BRANCH_NAME="feature/issue-${ISSUE_NUMBER}-pipeline"
          else
            BRANCH_NAME="${{ inputs.branch_name }}"
          fi
          echo "Creating branch: $BRANCH_NAME"
          # Check if branch already exists
          if git show-ref --verify --quiet refs/remotes/origin/$BRANCH_NAME; then
            echo "Branch already exists, switching to it"
            git checkout $BRANCH_NAME
          else
            echo "Creating new branch"
            git checkout -b $BRANCH_NAME
            git push -u origin $BRANCH_NAME
          fi
          echo "branch_name=$BRANCH_NAME" >> $GITHUB_OUTPUT
          echo "✅ Branch ready: $BRANCH_NAME"
  run-shared-pipeline:
    needs: [check-trigger, create-branch]
    if: success()
    uses: atriumn/atriumn-shared-workflows/.github/workflows/development-pipeline.yml@main
    with:
      # CUSTOMIZE THIS: Update to your repository name
      repo_name: "YOUR_ORG/YOUR_REPO"
      issue_number: ${{ inputs.issue_number || github.event.issue.number }}
      branch_name: ${{ needs.create-branch.outputs.branch_name }}
      pipeline_id: ${{ inputs.pipeline_id || format('pipeline-{0}-{1}', github.run_id, inputs.issue_number || github.event.issue.number) }}
      human_validation: ${{ inputs.human_validation || true }}
      test_mode: ${{ inputs.test_mode || false }}
    secrets:
      REPO_TOKEN: ${{ secrets.PIPELINE_TOKEN }}
  process-pipeline-outputs:
    needs: [create-branch, run-shared-pipeline]
    if: needs.run-shared-pipeline.outputs.pipeline_status == 'success'
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.PIPELINE_TOKEN }}
          ref: ${{ needs.create-branch.outputs.branch_name }}
          fetch-depth: 0
      - name: Process pipeline outputs
        run: |
          echo "🔧 Processing pipeline outputs..."
          # Create directory for decision records
          # CUSTOMIZE THIS: Adjust directory structure for your repo
          mkdir -p thoughts/shared/decisions
          # Extract issue number for file naming
          ISSUE_NUMBER="${{ inputs.issue_number || github.event.issue.number }}"
          # Create decision record file
          cat > "thoughts/shared/decisions/pipeline-issue-${ISSUE_NUMBER}.md" << 'EOF'
          ${{ needs.run-shared-pipeline.outputs.decision_record_content }}
          EOF
          echo "✅ Decision record created"
      - name: Commit and push changes
        run: |
          # Configure git
          git config user.name "Pipeline Bot"
          git config user.email "pipeline@atriumn.com"
          # Add and commit files
          ISSUE_NUMBER="${{ inputs.issue_number || github.event.issue.number }}"
          git add thoughts/shared/decisions/pipeline-issue-${ISSUE_NUMBER}.md
          if git diff --staged --quiet; then
            echo "No changes to commit"
          else
            git commit -m "Add pipeline decision record for issue #${ISSUE_NUMBER}
Pipeline ID: ${{ needs.run-shared-pipeline.outputs.pipeline_id }}
Branch: ${{ needs.run-shared-pipeline.outputs.branch_name }}
Research Summary:
${{ needs.run-shared-pipeline.outputs.research_summary }}
🤖 Generated with [Claude Code](https://claude.ai/code)
Co-Authored-By: Claude <noreply@anthropic.com>"
            git push origin ${{ needs.create-branch.outputs.branch_name }}
            echo "✅ Changes committed and pushed"
          fi
  pipeline-summary:
    needs: [create-branch, run-shared-pipeline, process-pipeline-outputs]
    if: always()
    runs-on: ubuntu-latest
    steps:
      - name: Pipeline summary
        run: |
          echo "📊 Development Pipeline Summary"
          echo "================================"
          echo "Repository: ${{ github.repository }}"
          echo "Issue: #${{ inputs.issue_number || github.event.issue.number }}"
          echo "Branch: ${{ needs.create-branch.outputs.branch_name }}"
          echo "Pipeline ID: ${{ needs.run-shared-pipeline.outputs.pipeline_id }}"
          echo "Status: ${{ needs.run-shared-pipeline.outputs.pipeline_status }}"
          echo ""
          echo "Research Summary:"
          echo "${{ needs.run-shared-pipeline.outputs.research_summary }}"
          echo ""
          echo "Next Actions:"
          echo "${{ needs.run-shared-pipeline.outputs.next_actions }}"
